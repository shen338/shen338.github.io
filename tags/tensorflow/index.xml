<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Tensorflow on Tong Shen</title>
    <link>/tags/tensorflow/</link>
    <description>Recent content in Tensorflow on Tong Shen</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Thu, 19 Jul 2018 21:00:00 -0500</lastBuildDate>
    
	<atom:link href="/tags/tensorflow/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>CapsNet and dynamic routing</title>
      <link>/post/capsnet-and-dynamic-routing/</link>
      <pubDate>Thu, 19 Jul 2018 21:00:00 -0500</pubDate>
      
      <guid>/post/capsnet-and-dynamic-routing/</guid>
      <description>Introduction In late 2017, Geoffrey Hinton, one of the biggest names in deep learning community, finally published his work about capsule theory. Hintion has worked on this for years, like Transforming Auto-Encoders. This should be a big step for us to understand human brain.
CapsNet consists of many capsules. Rather than output a scalar, capsules output a vector. The length of the vector will represent the probability of certain entity and the vector itself will represent the property of this entity.</description>
    </item>
    
    <item>
      <title>NLP basics - Word2vec: Skip-gram, CBOW, GloVe</title>
      <link>/post/nlp-basics---word2vec/</link>
      <pubDate>Fri, 01 Jun 2018 21:00:00 -0500</pubDate>
      
      <guid>/post/nlp-basics---word2vec/</guid>
      <description>Table of Content  Overview Basic Algorithm  Skip-Gram  Intuition Implementation details  CBOW Co-occurrence Models GloVe Evaluation Metrics  Limitation Reference Materials  Overview First of all, we need to figure out how do we represent the meaning of a word. A common solution is using WordNet: a resource containing lists of synonym sets and hypernyms (&amp;ldquo;is a&amp;rdquo; relationships).
But this method is not so good. It is great as a resource but missing nuance.</description>
    </item>
    
    <item>
      <title>Going Deeper in Batch Normalization</title>
      <link>/post/going-deeper-into-batch-normalization/</link>
      <pubDate>Thu, 17 May 2018 21:00:00 -0500</pubDate>
      
      <guid>/post/going-deeper-into-batch-normalization/</guid>
      <description>Table of contents/optimization  Interpretation and Advantage of Batch Norm Algorithm and implementation Improvements and Alternatives  Batch norm fused with Convolution Layer normalization Instance Normalization Group Normalization Other normalization techniques*  Reference Materials:  This article will thoroughly explain batch normalization in a simple way. I wrote this article after getting failed an interview because of detailed batchnorm related question. I will start with why we need it, how it works, then how to fuse it into conv layer, and finally how to implement it in tensorflow.</description>
    </item>
    
    <item>
      <title>Amazing GAN - Wasserstein GAN</title>
      <link>/post/amazing-gan---wasserstein-gan/</link>
      <pubDate>Sat, 17 Feb 2018 21:00:00 -0600</pubDate>
      
      <guid>/post/amazing-gan---wasserstein-gan/</guid>
      <description>Table of Content  KL divergence and JS divergence Generative Adversarial Networks  Global optimal loss  Problem with Vanilla GANs  Gradient Vanishing Mode Collapse  Improved Training of GANs Wasserstein GAN  Earth Mover distance Comparasion between EM distance and KL/JS divergence Lipschitz continuity Modified Algorithm Wasserstein GAN with gradient penalty  Disadvantages of gradient clipping in WGAN Gradient Penalty   Reference Materials   KL divergence and JS divergence Before diving into details, let first review two very important metrics to quantify the similarity of two probability distributions: Kullback-Leibler Divergence and Jensen-Shannon Divergence.</description>
    </item>
    
    <item>
      <title>Neural Network based Object Recognition: Fast/Faster/Mask R-CNN, SSD, YOLO and RetinaNet</title>
      <link>/post/neural-network-based-object-recognition-fast-faster-mask-r-cnn-ssd-and-yolo/</link>
      <pubDate>Mon, 01 Jan 2018 21:00:00 -0600</pubDate>
      
      <guid>/post/neural-network-based-object-recognition-fast-faster-mask-r-cnn-ssd-and-yolo/</guid>
      <description>Table of Content  History  AlexNet ResNet Inception Network Series Deformable Parts Model Overfeat  Metrics: mAP  Precision and Recall Average precision Mean Average Precision  R-CNN Families  RCNN  Selective Search Bounding Box Regression Non Maximum Suppression Workflow Speed Bottleneck  Fast-RCNN  ROI pooling Workflow  Faster-RCNN  Region Proposal Network Workflow  Mask-RCNN  ROI Align Key Point Detection   Single Shot Detector  Single Shot MultiBox Detector  Architecture Implementation Details  YOLOv1  Architecture Loss function Workflow Limitation  YOLOv2  Improvements  YOLOv3 Feature Pyramid Network (FPN)  Architecture Feature Pyramid Networks for RPN Feature Pyramid Networks for Faster/Mask R-CNN Experimental Result  Focal Loss and RetinaNet  Class Imbalance RetinaNet Implementation Details Results   Summary Reference Materials  It&amp;rsquo;s well known that deep learning has been a real game changer in machine learning, especially in computer vision.</description>
    </item>
    
  </channel>
</rss>