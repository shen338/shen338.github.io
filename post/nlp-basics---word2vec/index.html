<!DOCTYPE html>
<html lang="en-us">
<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="theme" content="hugo-academic">
  <meta name="generator" content="Hugo 0.42" />
  <meta name="author" content="Tong Shen">

  
  
  
  
    
      
    
  
  <meta name="description" content="Table of Content  Overview Basic Algorithm  Skip-Gram  Intuition Implementation details  CBOW Co-occurrence Models GloVe Evaluation Metrics  Limitation Reference Materials  Overview First of all, we need to figure out how do we represent the meaning of a word. A common solution is using WordNet: a resource containing lists of synonym sets and hypernyms (&ldquo;is a&rdquo; relationships).
But this method is not so good. It is great as a resource but missing nuance.">

  
  <link rel="alternate" hreflang="en-us" href="/post/nlp-basics---word2vec/">

  


  

  
  
  <meta name="theme-color" content="#0095eb">
  
  
  
  
    
  
  
    
    
      
        <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">
      
    
  
  
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha512-6MXa8B6uaO18Hid6blRMetEIoPqHf7Ux1tnyIQdpt9qI5OACx7C+O3IVTr98vwGnlcg0LOLa02i9Y1HpVhlfiw==" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha512-SfTiTlX6kk+qitfevl/7LibUOeJWlt9rbyDn92a1DqWOw9vWG2MFoays0sgObmWazO5BQPiFucnnEAjpAB+/Sw==" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">
  
  
  
  
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Lato:400,700%7cMerriweather%7cRoboto&#43;Mono%7cPermanent&#43;Marker%7cPrata">
  
  <link rel="stylesheet" href="/styles.css">
  

  

  
  <link rel="alternate" href="/index.xml" type="application/rss+xml" title="Tong Shen">
  <link rel="feed" href="/index.xml" type="application/rss+xml" title="Tong Shen">
  

  <link rel="manifest" href="/site.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="/post/nlp-basics---word2vec/">

  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="og:site_name" content="Tong Shen">
  <meta property="og:url" content="/post/nlp-basics---word2vec/">
  <meta property="og:title" content="NLP basics - Word2vec: Skip-gram, CBOW, GloVe | Tong Shen">
  <meta property="og:description" content="Table of Content  Overview Basic Algorithm  Skip-Gram  Intuition Implementation details  CBOW Co-occurrence Models GloVe Evaluation Metrics  Limitation Reference Materials  Overview First of all, we need to figure out how do we represent the meaning of a word. A common solution is using WordNet: a resource containing lists of synonym sets and hypernyms (&ldquo;is a&rdquo; relationships).
But this method is not so good. It is great as a resource but missing nuance.">
  <meta property="og:locale" content="en-us">
  
  <meta property="article:published_time" content="2018-06-01T21:00:00-05:00">
  
  <meta property="article:modified_time" content="2018-06-01T21:00:00-05:00">
  

  
  

  <title>NLP basics - Word2vec: Skip-gram, CBOW, GloVe | Tong Shen</title>

</head>
<body id="top" data-spy="scroll" data-target="#toc" data-offset="71" >

<nav class="navbar navbar-default navbar-fixed-top" id="navbar-main">
  <div class="container">

    
    <div class="navbar-header">
      
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse"
              data-target=".navbar-collapse" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      
      <a class="navbar-brand" href="/">Tong Shen</a>
    </div>

    
    <div class="collapse navbar-collapse">

      
      
      <ul class="nav navbar-nav navbar-right">
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#about">
            
            <span>Home</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#myself">
            
            <span>CV</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#projects">
            
            <span>Projects</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#posts">
            
            <span>Posts</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#publications_selected">
            
            <span>Publications</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#contact">
            
            <span>Contact</span>
            
          </a>
        </li>

        
        
      

      
      </ul>

    </div>
  </div>
</nav>


<article class="article" itemscope itemtype="http://schema.org/Article">

  


  <div class="article-container">
    <h1 itemprop="name">NLP basics - Word2vec: Skip-gram, CBOW, GloVe</h1>

    

<div class="article-metadata">

  <span class="article-date">
    
    <time datetime="2018-06-01 21:00:00 -0500 CDT" itemprop="datePublished dateModified">
      Jun 1, 2018
    </time>
  </span>
  <span itemscope itemprop="author publisher" itemtype="http://schema.org/Person">
    <meta itemprop="name" content="Tong Shen">
  </span>

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    10 min read
  </span>
  

  
  

  

  
  
<div class="share-box" aria-hidden="true">
  <ul class="share">
    <li>
      <a class="twitter"
         href="https://twitter.com/intent/tweet?text=NLP%20basics%20-%20Word2vec%3a%20Skip-gram%2c%20CBOW%2c%20GloVe&amp;url=%2fpost%2fnlp-basics---word2vec%2f"
         target="_blank" rel="noopener">
        <i class="fa fa-twitter"></i>
      </a>
    </li>
    <li>
      <a class="facebook"
         href="https://www.facebook.com/sharer.php?u=%2fpost%2fnlp-basics---word2vec%2f"
         target="_blank" rel="noopener">
        <i class="fa fa-facebook"></i>
      </a>
    </li>
    <li>
      <a class="linkedin"
         href="https://www.linkedin.com/shareArticle?mini=true&amp;url=%2fpost%2fnlp-basics---word2vec%2f&amp;title=NLP%20basics%20-%20Word2vec%3a%20Skip-gram%2c%20CBOW%2c%20GloVe"
         target="_blank" rel="noopener">
        <i class="fa fa-linkedin"></i>
      </a>
    </li>
    <li>
      <a class="weibo"
         href="http://service.weibo.com/share/share.php?url=%2fpost%2fnlp-basics---word2vec%2f&amp;title=NLP%20basics%20-%20Word2vec%3a%20Skip-gram%2c%20CBOW%2c%20GloVe"
         target="_blank" rel="noopener">
        <i class="fa fa-weibo"></i>
      </a>
    </li>
    <li>
      <a class="email"
         href="mailto:?subject=NLP%20basics%20-%20Word2vec%3a%20Skip-gram%2c%20CBOW%2c%20GloVe&amp;body=%2fpost%2fnlp-basics---word2vec%2f">
        <i class="fa fa-envelope"></i>
      </a>
    </li>
  </ul>
</div>


  

</div>


    <div class="article-style" itemprop="articleBody">
      

<h2 id="table-of-content">Table of Content</h2>

<ul>
<li><a href="#overview">Overview</a></li>
<li><a href="#basic-algorithm">Basic Algorithm</a>

<ul>
<li><a href="#skip-gram">Skip-Gram</a>

<ul>
<li><a href="#intuition">Intuition</a></li>
<li><a href="#implementation-details">Implementation details</a></li>
</ul></li>
<li><a href="#cbow">CBOW</a></li>
<li><a href="#co-occurrence-models">Co-occurrence Models</a></li>
<li><a href="#glove">GloVe</a></li>
<li><a href="#evaluation-metrics">Evaluation Metrics</a></li>
</ul></li>
<li><a href="#limitation">Limitation</a></li>
<li><a href="#reference-materials">Reference Materials</a></li>
</ul>

<h1 id="overview">Overview</h1>

<p>First of all, we need to figure out how do we represent the meaning of a word. A common solution is using WordNet: a    resource containing lists   of synonym  sets and hypernyms (&ldquo;is a&rdquo; relationships).</p>

<p><img src="/img/word2vec/WordNet.png" width=700></p>

<p>But this method is not so good. It is great as a resource but   missing nuance. Also it misses the new meaning of a word.</p>

<p>Another way is representing words   as discrete symbols. For example, like &ldquo;hotel&rdquo; and &ldquo;motel&rdquo;. We can represent them as $hotel = [0, 0, 0, 1]$ while $motel = [0, 0, 1, 0]$. In this way, word vector dimension is equal to the number of words in vocabulary. But apparently, this method is not perfect. The word &ldquo;hotel&rdquo; and &ldquo;motel&rdquo; is quite similar in meaning, but the word vectors of them are orthogonal, which is hard to quantify their similarity.</p>

<p>So, here comes the word2vec models. The core idea is: A word’s meaning is   given   by the words that frequently appear close-by. This is one of the most successful ideas in modern NLP. We will build a dense word vector for each word (dimension far less than the number of words in vocabulary)   so  that    the words   that    appear  in  similar contexts have similar word vectors.</p>

<p><a href="https://arxiv.org/pdf/1310.4546.pdf" target="_blank">Word2vec (Mikolov et   al. 2013)</a> is  a   framework   for learning
word vectors. The idea is as follows:</p>

<ol>
<li>We   have    a   large   corpus  of  text</li>
<li>Every    word    in  a   fixed   vocabulary  is  represented by  a   vector</li>
<li>Go   through each    position    t in    the text,   which   has a   center  word c and  context (“outside”) words   o</li>
<li>Use  the similarity  of  the word    vectors for c   and o to    calculate   the probability of  o given c   (or vice    versa)</li>
<li>Keep adjusting   the word    vectors to  maximize    this    probability</li>
</ol>

<p>Here is an example of calculating the   probability of c given o:</p>

<p><img src="/img/word2vec/wordwindow.png" width=800></p>

<p>So the total objective function is:</p>

<p>$$L(\theta) = \prod_{t=1}^{T} \prod_{j  \in [-m, 0) \bigcup (0, m]} P(w_{t+j}|w_t;\theta)  $$</p>

<p>Where the $T$ is corpus size, and m is the half window size. Our goal would be minimizing the negative log-likelihood:</p>

<p>$$J(\theta) = -\frac{1}{T} \sum_{t=1}^{T} \sum_{j  \in [-m, 0) \bigcup (0, m] }  P(w_{t+j}|w_t;\theta)  $$</p>

<p>So, the question is: how to calculate $P(w_{t+j}|w_t;\theta)$, the probability of  o given c. Here we will use the most simple one: vector inner product and softmax function to present the probability:</p>

<p>$$P(o|c) = \frac{exp(u_o^Tv_c)}{\sum_{w \in V} exp(u_w^Tv_c)}$$</p>

<p>Hmm&hellip;Softmax again. Neural network then comes to play.</p>

<h1 id="basic-algorithm">Basic Algorithm</h1>

<h2 id="skip-gram">Skip-Gram</h2>

<h3 id="intuition">Intuition</h3>

<p>The <a href="https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf" target="_blank">skip-gram algorithm</a> is surprisingly simple and elegant. We’re going to train a simple neural network with a single hidden layer to minimizing the negative log-likelihood listed above. The input word is the <strong>center word</strong> of windows on the text corpus, while the output word is the <strong>surrounding words</strong>. And every word have two word vectors, $v$ for center and $u$ for context. But we are not actually using the neural work afterwards, instead just using the weight we trained in the  hidden layer. This weight matrix is actually the word vectors we want.</p>

<p>First, say we have a vacabulary of 10,000 words. We’re going to represent an input word like &ldquo;hotel&rdquo; as a one-hot vector. So the one hot vector will be 10000 dimensional. The output of the network is a single vector (also 10,000 dimensional), predicting the probability that certain word is appearing nearby the input word.</p>

<p>The architecture of skip-gram network is simple:</p>

<p><img src="/img/word2vec/skip-gram.png" width=800></p>

<p>There is no activation function on the hidden layer neurons, but the output neurons use softmax to predict the probability of each word&rsquo;s appearance in each nearby location.</p>

<p>Since the input is a one-hot vector, it just select the matrix row corresponding to the &ldquo;1&rdquo;. So, this is just a simple lookup table on the weight $W$ in the above figure. <strong>The corresponding row in weight W is the word vector of input word</strong>.</p>

<p>The word vector will be sent into the output layer to produce the probability of each word&rsquo;s appearance in each location. And number of prediction head is equal to the window size - 1.</p>

<p>The brilliance of skip-gram is that it makes sure the words frequently appearing in similar context would have similar word vectors, because the network only uses context words as ground truth. So, similar context words will certainly produce similar results.</p>

<h3 id="implementation-details">Implementation details</h3>

<p>The authors proposed some <a href="https://arxiv.org/pdf/1310.4546.pdf" target="_blank">improvements</a> to the original skip-gram model. There are three main modifications:</p>

<ol>
<li>Use Word Pairs and &ldquo;phrases&rdquo;</li>
<li>Subsample frequent words</li>
<li>Negative sampling</li>
</ol>

<p><strong>Use Word Pairs and “phrases”</strong>: The author pointed out that some common word phrases, like &ldquo;New York City&rdquo;, should be represented as a single word with its own word vector. This is quite natural to understand. We often use word phrase as a word and their meanings are different when splitting them apart.</p>

<p>The phrase detection mechanism is quite simple, too. We just need to count the number of times each combination of two words appears in the training text, and then these counts are used in an equation to determine which word combinations to turn into phrases. This equation should be related to words co-occurence and individual occurence.</p>

<p><strong>Subsample frequent words</strong>:</p>

<p>There are two “problems” with common words like “the”:</p>

<ol>
<li>When looking at word pairs, (&ldquo;fox&rdquo;&ldquo;, &ldquo;the&rdquo;) doesn’t tell us much about the meaning of &ldquo;fox&rdquo;. &ldquo;the&rdquo; appears in the context of pretty much every word.</li>
<li>We will have many more samples of (&ldquo;the&rdquo;, …) than we need to learn a good vector for “the”.</li>
</ol>

<p>So, we can subsample frequent words using the follow equation:</p>

<p>$$P(w_i) = (\sqrt{\frac{z(w_i)}{0.001}} + 1) \times \frac{0.001}{z(w_i)}$$</p>

<p>Where $P(w_i)$ is probability of keeping the word and $z(w_i)$ is the frequency of the word. For example, if &ldquo;we&rdquo; appears 100 time in a corpus of 10000 words, $z(&ldquo;we&rdquo;)$ should be 0.01.</p>

<p><strong>Negative sampling</strong>: Although the network only has three layers, but the weight size is huge because of a large vacabulary. And the nature of softmax (gradient on all input) function will result in huge amount of computation in single training step.</p>

<p>Negative sampling addresses this by having each training sample only modify a small percentage of the weights, rather than all of them. We are instead going to randomly select just a small number of “not ground truth” words (let’s say 5) to update their weights. In the paper, the authors suggested 5-20 words for smaller datasets, and you can get away with only 2-5 words for large datasets.</p>

<p>Here a totally random selection is not good. some rare words may have little probability to be selected. So, the sampling prob comes from an equation:</p>

<p>$$P(w_i) = \frac{f(w_i)^{\frac{3}{4}}}{\sum_{j=0}^{n} {f(w_i)^{\frac{3}{4}}}}$$</p>

<p>where $f(w_i)$ is the word appearance count. And the power $\frac{3}{4}$ is an empirical value, which works very well. Recent research result also proved the effectiveness of this value.</p>

<p><a href="https://www.tensorflow.org/tutorials/word2vec" target="_blank">Here</a> is an official tutorial of word2vec on TensorFlow.</p>

<h2 id="cbow">CBOW</h2>

<p>In the other hand, Continuous   Bag of  Words   (CBOW) does the opposite thing compared to skip-gram. It uses the context words to predict the center word. (image <a href="https://zhuanlan.zhihu.com/p/35074402" target="_blank">source</a>)</p>

<p><img src="/img/word2vec/CBOW.jpg" width=800></p>

<p>Note the weight matrix for every input is <strong>shared</strong>. All hidden layer weights are <strong>added</strong> and send into next layer.</p>

<p><strong>Skip-Gram and CBOW difference</strong>: CBOW smoothes over a lot of the distributional information (by treating an entire context as one observation). For the most part, this turns out to be a useful thing for smaller datasets. However, skip-gram treats each context-target pair as a new observation, and this tends to do better when we have larger datasets. Also, this makes the skip-gram model performs better on infrequent words.</p>

<h2 id="co-occurrence-models">Co-occurrence Models</h2>

<p>There are two kinds of co-occurrence models, window based and full document based models. Herem we only talk about window based models: Similar to  word2vec,   use window  around each word, and count the co-occurrence situations.</p>

<p>Let&rsquo;s use the example from <a href="http://web.stanford.edu/class/cs224n/lectures/lecture3.pdf" target="_blank">cs224 lecture</a> to illustrate:</p>

<p>The example corpus contains three sentences: &ldquo;I like    deep    learning.&rdquo;, &ldquo;I like NLP.&rdquo;, &ldquo;I enjoy flying.&rdquo;. The co-occurrence matrix would be:</p>

<p><img src="/img/word2vec/cooccurrence.png" width=800></p>

<p>Problems of the model is obvious. It&rsquo;s bery high dimensional and may need future process. And the matrix is quite sparse, which makes the model less robust. So, further we can use some dimensionality reduction technique to reduce the dimension of our word vectors, like singular  value   decomposition. However, the computational cost of SVD is quite high($O(mn^2)$), which is not suitable for large vacabulary. The <a href="http://web.stanford.edu/class/cs224n/lectures/lecture3.pdf" target="_blank">cs224 lecture</a> lecture summarized the pros and cons of Count   based model vs direct   prediction model:</p>

<p><img src="/img/word2vec/compare.png" width=800></p>

<h2 id="glove">GloVe</h2>

<p>Since count based method and direct prediction method both have pros and cons, how about combine them together to make it stronger? <a href="https://nlp.stanford.edu/projects/glove/" target="_blank">GloVe</a> is a good attempt. The loss function of GloVe model is:</p>

<p>$$J(\theta) = \frac{1}{2} \sum_{i,j=1}^{W} f(P_{ij})(u_i^Tv_j - logP_{ij})^2$$</p>

<p>Where the $u_i$ $v_j$ is the same to skip-gram model, and $P_{ij}$ is the co-occurrent matrix. $F(x)$ is a step-like function:</p>

<p><img src="/img/word2vec/glove-fig-1.png" width=400></p>

<p>The cut-off value $x_{max}$ is not sensitive. In experiment choose $x_{max} = 100$. Instead of direct predict center or context words, the GloVe model trains word vectors to fit the log probability of the co-occurrence matrix. The GloVe model utilizes the overall statistical information, and is also scalable for huge corpus. It can achieve good performance with small corpus. But it need to store the co-occurrence matrix, which requires a large storage space. But it is faster than skip-gram with negative sampling. So, the choice of GloVe or prediction model has a trade-off of speed and storage.</p>

<h2 id="evaluation-metrics">Evaluation Metrics</h2>

<p>For every machine learning model, we need a metrics to evaluate its performance. Word vectors encode the similarity of words, but how to exam their effectiveness on the encoding? Of course we can use real world tasks to do extrinsic evaluation, but it takes a long cycle to update the word vector model.</p>

<p>One popular method utilize syntactic    analogy to evaluate word vectors. We will evaluate  word    vectors by  how well    their   cosine  distance    after   addition    captures    intuitive   semantic    and
syntactic   analogy questions. For example, given man over woman, we want the use the mapping to map king to queen. This mapping accuracy (cosine distance) is the key for evaluation. Here is an illustration:</p>

<p><img src="/img/word2vec/evaluation.png" width=600></p>

<p>Our model should uses the differential vector of &ldquo;nephew&rdquo; and &ldquo;niece&rdquo; to find &ldquo;aunt&rdquo; from &ldquo;uncle&rdquo;. A</p>

<p>Another way is just evaluate the word vector similarity (usually inner produce) of synonyms. Dataset like wordsim353, simlex999 should be helpful.</p>

<p>Also, people developed normal document classification tasks to reversely reflect the quality of word vectors.</p>

<p>But these similarity-based evaluation method is very sensitive to the choice of training data size, domain, source, and vocabulary. And the data set is too small and often does not adequately determine the quality of word vectors. Recently, people tend to learning task-specific word vectors, because it is hard to evaluate word embeddings using the same standard. In my mind, word vectors can be treated as a tool of further NLP process and it&rsquo;s quality can not be evaluated directly.</p>

<p>Here is a great paper on this problem: <a href="http://www.aclweb.org/anthology/D15-1036" target="_blank">Evaluation methods for unsupervised word embeddings</a>.</p>

<h1 id="limitation">Limitation</h1>

<p>The limitation of word embedding model has three folds:</p>

<ol>
<li><strong>For each word, we only have one word vector</strong>. When certain word is a polysemy, like &ldquo;book&rdquo;, this model obviously fails.</li>
<li><strong>Cannnot leverage the benefits of pre-trained models</strong>. This means you probably need to train word vector models for your own dataset rather than using pretrained models like image classification tasks. The pretrained word vectors is a result representing the similarity of words in specific dataset rather than a feature extrator.</li>
<li><strong>No shared representations at sub-word levels</strong>. For example, words like &ldquo;newspaper&rdquo; is assembled by two word, and their meaning is a combination of these two words. Like &ldquo;unhappy&rdquo;, the meaning is determine by &ldquo;happy&rdquo; and its prefix. But word vector model does not take this into account. I believe the word embedding model should be better with shared representation for sub-word.</li>
</ol>

<h1 id="reference-materials">Reference Materials</h1>

<ol>
<li><a href="http://web.stanford.edu/class/cs224n/syllabus.html" target="_blank">Stanford cs224n lectures</a></li>
<li><a href="http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/" target="_blank">Word2Vec Tutorial - The Skip-Gram Model</a></li>
<li><a href="https://arxiv.org/pdf/1310.4546.pdf" target="_blank">Distributed Representations of Words and Phrases and their Compositionality</a></li>
<li><a href="https://arxiv.org/pdf/1301.3781.pdf" target="_blank">Efficient Estimation of Word Representations in Vector Space</a></li>
<li><a href="https://nlp.stanford.edu/projects/glove/" target="_blank">GloVe Model</a></li>
</ol>

    </div>

    


<div class="article-tags">
  
  <a class="btn btn-primary btn-outline" href="/tags/nlp/">NLP</a>
  
  <a class="btn btn-primary btn-outline" href="/tags/word2vec/">word2vec</a>
  
  <a class="btn btn-primary btn-outline" href="/tags/deep-learning/">deep learning</a>
  
  <a class="btn btn-primary btn-outline" href="/tags/tensorflow/">tensorflow</a>
  
</div>




    
    
    <div class="article-widget">
      <div class="hr-light"></div>
      <h3>Related</h3>
      <ul>
        
        <li><a href="/post/going-deeper-into-batch-normalization/">Going Deeper in Batch Normalization</a></li>
        
        <li><a href="/post/amazing-gan---wasserstein-gan/">Amazing GAN - Wasserstein GAN</a></li>
        
      </ul>
    </div>
    

    

    


  </div>
</article>

<footer class="site-footer">
  <div class="container">
    <p class="powered-by">

      &copy; 2018 &middot; 

      Powered by
      
      <a href="https://shen338.github.io/" target="_blank" rel="noopener">Tong Shen</a>. 
	  
	  All rights reserved.

      <span class="pull-right" aria-hidden="true">
        <a href="#" id="back_to_top">
          <span class="button_icon">
            <i class="fa fa-chevron-up fa-2x"></i>
          </span>
        </a>
      </span>

    </p>
  </div>
</footer>


<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <button type="button" class="close btn-large" data-dismiss="modal">&times;</button>
        <h4 class="modal-title">Cite</h4>
      </div>
      <div>
        <pre><code class="modal-body tex"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-primary btn-outline js-copy-cite" href="#" target="_blank">
          <i class="fa fa-copy"></i> Copy
        </a>
        <a class="btn btn-primary btn-outline js-download-cite" href="#" target="_blank">
          <i class="fa fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

    

    
    

    

    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.2.1/jquery.min.js" integrity="sha512-3P8rXCuGJdNZOnUx/03c1jOTnMn3rP63nBip5gOP2qmUh5YAdVAvFZ1E+QLZZbC1rtMrQb+mah3AfYW11RUrWA==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.3/imagesloaded.pkgd.min.js" integrity="sha512-umsR78NN0D23AzgoZ11K7raBD+R6hqKojyBZs1w8WvYlsI+QuKRGBx3LFCwhatzBunCjDuJpDHwxD13sLMbpRA==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha512-iztkobsvnjKfAtTNdHkGVjAYTrrtlC7mGp/54c40wowO7LhURYl3gVzzcEqGl/qKXQltJ2HwMrdLcNUdo+N/RQ==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.4/isotope.pkgd.min.js" integrity="sha512-VDBOIlDbuC4VWxGJNmuFRQ0Li0SKkDpmGyuhAG5LTDLd/dJ/S0WMVxriR2Y+CyPL5gzjpN4f/6iqWVBJlht0tQ==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>
    
    
    <script src="/js/hugo-academic.js"></script>
    

    
    
      
      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js" integrity="sha256-/BfiIkHlHoVihZdc6TFuj7MmJ0TWcWsMXkeDFwhi0zw=" crossorigin="anonymous"></script>
      

      

      

      <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']] } });
    </script>
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML" integrity="sha512-tOav5w1OjvsSJzePRtt2uQPFwBoHt1VZcUq8l8nm5284LEKE9FSJBQryzMBzHxY5P0zRdNqEcpLIRVYFNgu1jw==" crossorigin="anonymous"></script>
    
    

  </body>
</html>

