<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Tong Shen on Tong Shen</title>
    <link>/</link>
    <description>Recent content in Tong Shen on Tong Shen</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Sun, 15 Oct 2017 00:00:00 -0500</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Reinforcement Learning: Markov Decision Process and Model-free Algorithms</title>
      <link>/post/model-free-reinforcement-learning/</link>
      <pubDate>Sat, 01 Dec 2018 21:00:00 -0600</pubDate>
      
      <guid>/post/model-free-reinforcement-learning/</guid>
      <description>

&lt;h1 id=&#34;table-of-contents&#34;&gt;Table of Contents&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#what-is-reinforcement-learning-&#34;&gt;What is reinforcement learning?&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#model&#34;&gt;Model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#reinforcement-learning-objective&#34;&gt;Reinforcement learning objective&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#markov-decision-process&#34;&gt;Markov Decision Process&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#policy-value-iteration&#34;&gt;Policy/Value iteration&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#imitation-learning&#34;&gt;Imitation Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#basics&#34;&gt;Basics&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#policy-gradient&#34;&gt;Policy Gradient&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#actor-critic-algorithm&#34;&gt;Actor-Critic Algorithm&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#temporal-difference-learning&#34;&gt;Temporal Difference Learning&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#sarsa&#34;&gt;Sarsa&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#q-learning&#34;&gt;Q-learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#expected-sarsa&#34;&gt;Expected Sarsa&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#double-q-learning&#34;&gt;Double Q-Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#n-step-return&#34;&gt;N-step Return&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#eligibility-trace-and-td---lambda--&#34;&gt;Eligibility Trace and TD($\lambda$)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#deep-q-network&#34;&gt;Deep Q network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#q-learning-with-continuous-action&#34;&gt;Q-learning with continuous action&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#advanced-policy-gradient&#34;&gt;Advanced policy gradient&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#policy-gradient-theorem&#34;&gt;Policy gradient theorem&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#off-policy-policy-gradient&#34;&gt;Off-policy policy gradient&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#a3c&#34;&gt;A3C&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#dpg&#34;&gt;DPG&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#ddpg&#34;&gt;DDPG&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#td3&#34;&gt;TD3&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#trpo&#34;&gt;TRPO&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#ppo&#34;&gt;PPO&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#acer&#34;&gt;ACER&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#soft-actor-critic&#34;&gt;Soft Actor-critic&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#sum-up&#34;&gt;Sum up&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#reference&#34;&gt;Reference&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;what-is-reinforcement-learning&#34;&gt;What is reinforcement learning?&lt;/h1&gt;

&lt;p&gt;In supervised learning, we saw algorithms that tried to make their outputs
mimic the labels y given in the training set. In that setting, the labels gave
an unambiguous “right answer” for each of the inputs x. In contrast, for
many sequential decision making and control problems, it is very difficult to
provide this type of explicit supervision to a learning algorithm. For example,
if we have just built a four-legged robot and are trying to program it to walk,
then initially we have no idea what the “correct” actions to take are to make
it walk, and so do not know how to provide explicit supervision for a learning
algorithm to try to mimic.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/RL/RL_image.png&#34; width=600&gt;&lt;/p&gt;

&lt;p&gt;A typical RL problem diagram is shown above. The agent &amp;ldquo;mouse&amp;rdquo; is frequently
interacting with environment &amp;ldquo;maze&amp;rdquo; with different actions,
and a reward from outside environment works as a guideline for the agent to achieve the optimum. Different to
supervised learning problem, the reward cannot be directly used as supervision. The RL alogrithms act like the mouse, learn proper actions from observations and rewards to approach the final goal &amp;ldquo;cake&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;To sum up, the goal of reinforcement learning is to find a good strategy for the agent in such environment by
taking experimental trails and optimize the reward.
And a better strategy means a higher &lt;strong&gt;expection&lt;/strong&gt; of overall reward with this strategy.&lt;/p&gt;

&lt;h2 id=&#34;model&#34;&gt;Model&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;/img/RL/RL1.jpg&#34; width=680&gt;&lt;/p&gt;

&lt;p&gt;The agent is acting in an environment (Image &lt;a href=&#34;http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/intro_RL.pdf&#34; target=&#34;_blank&#34;&gt;source&lt;/a&gt;). The environment can have many different conditions/states, called &lt;strong&gt;state&lt;/strong&gt;$(s \in S)$. And the agent can have a lot of different behaviors, called &lt;strong&gt;action&lt;/strong&gt;$(a \in A)$. In the agent&amp;rsquo;s perspective, it observes the environment state as an &lt;strong&gt;observation&lt;/strong&gt;$(o \in O)$. The RL algorithms give agent a guidance to follow in different states and usually it is a probability distribution, $P(a|s)$, called &lt;strong&gt;policy&lt;/strong&gt;. After the agent takes an action, the environment state can change accordingly.&lt;/p&gt;

&lt;p&gt;Also, after taking an action, the agent will receive a reward with state: $(r \in R : S × A)$ as a feedback. The system itself can be
stochastic: the probability of the next state $(s&amp;rsquo;)$ on current state $(s)$ and with action $(a)$ is: $P(s&amp;rsquo;|s, a)$ (System Dynamics).&lt;/p&gt;

&lt;p&gt;Divided by this probability, there are two main categories of RL algorithm:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Model-free algorithms. In model-free algorithm, we do not need to optimize over state dynamics models $P(s&amp;rsquo;|s, a)$, like policy gradient algorithm, directly optimizing policy with gradient descent or deep Q-learning to build neural network to fit expected value of the total reward over all successive steps.&lt;/li&gt;
&lt;li&gt;Model-based algorithms. In this setting, we already know the system dynamics, like $P(s&amp;rsquo;|s, a)$ or we can learn a good model to simulate the environment. It might be hard to learning system dynamics and good strategy at the same time. So, it can be particularly effective if we can hand-engineer a dynamics representation using our knowledge of physics, and fit just a few parameters.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;reinforcement-learning-objective&#34;&gt;Reinforcement learning objective&lt;/h2&gt;

&lt;p&gt;First, we name a trail from time 0 to time step $T$ as a &lt;strong&gt;trajectory&lt;/strong&gt;. So, the probability of the hwole trajectory is:&lt;/p&gt;

&lt;p&gt;$$P(\tau) = P_{\theta}(s_0, a_0, &amp;hellip;, s_T, a_T) = p(s_0) \prod_{t=1}^T \pi_{\theta}(a_t|s_t) p(s_{t+1}|s_t, a_t)$$&lt;/p&gt;

&lt;p&gt;Our objective is to find an optimal $\theta^{\star}$, that maximizes the expection of the trajectory reward:&lt;/p&gt;

&lt;p&gt;$$\theta^{\star} = argmax_{\theta} E_{\tau \sim p(\tau)}[\sum_{t=1}^{T} r(a_t, s_t)]$$&lt;/p&gt;

&lt;p&gt;Note that we still treat this as a Markov process.&lt;/p&gt;

&lt;p&gt;Next step, we will talk about reinforcement learning algorithms. There are a lot of categorical information from CS294 Lecture &lt;a href=&#34;http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-4.pdf&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&#34;markov-decision-process&#34;&gt;Markov Decision Process&lt;/h1&gt;

&lt;p&gt;Markov decision process(MDP) is a simplified sequential decision making process, in which RL problems are usually posed. Here are two main assumptions hold for MDP:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;The whole system is fully observed.&lt;/li&gt;
&lt;li&gt;System state transition is a Markov process. $P(S_t | S_{t-1}, S_{t-2} , &amp;hellip; ,  S_{1}) = P(S_t | S_{t-1})$&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Basically, markov decision process is just a tuple: {$ S, A, P, \gamma, r $}. Here, $\gamma$ is a discount factor, it will discount the reward with time step increases, which can make RL algorithms get the reward as soon as possible.&lt;/p&gt;

&lt;p&gt;The dynamics of an MDP proceeds as follows: We start in some state $S_0$, and get to choose some action $a_0 \in A$ to take in the MDP. As a result of our choice, the state of the MDP randomly transitions to some successor state $s_1$. And from $s_1$, continue this process on and on. So, this process goes like:&lt;/p&gt;

&lt;p&gt;$$s_0 \xrightarrow[]{a_0} s_1 \xrightarrow[]{a_1} s_2 \xrightarrow[]{a_2} &amp;hellip; $$&lt;/p&gt;

&lt;p&gt;In such a sequence, the reward is given as:&lt;/p&gt;

&lt;p&gt;$$R(s_0, a_0) + \gamma R(s_1, a_1)  + \gamma^2 R(s_2, a_2)  + \gamma^3 R(s_3, a_3) &amp;hellip;$$&lt;/p&gt;

&lt;p&gt;For Markov decision process, our goal is to find a good action strategy to maximize the reward expection:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/RL/MDP_expection.jpg&#34; width=680&gt;&lt;/p&gt;

&lt;p&gt;Here, $\pi$ means a union of all policies. And the reward at timestep t is discounted by a factor of $\gamma$
. Thus, to
make this expectation large, we would like to accrue positive rewards as soon
as possible.&lt;/p&gt;

&lt;p&gt;With this, we define the &lt;strong&gt;Value Function&lt;/strong&gt; as:&lt;/p&gt;

&lt;p&gt;$$V_{\pi}(s) = E[R(s_0, a_0) + \gamma R(s_1, a_1)  + \gamma^2 R(s_2, a_2)  + \gamma^3 R(s_3, a_3) &amp;hellip; | s_0, \pi]$$&lt;/p&gt;

&lt;p&gt;$V_{\pi}(s)$ is just a simple expection over all the future reward with a start state $s$.&lt;/p&gt;

&lt;p&gt;Given a stationary policy, we have the &lt;strong&gt;Bellman equation&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;$$V_{\pi}(s) = R(s) + \gamma \sum_{s&amp;rsquo; \in S} P(s&amp;rsquo;|s, a) V_{\pi}(s&amp;rsquo;)$$&lt;/p&gt;

&lt;p&gt;And here, $V_{\pi}(s)$ is equal to current state reward plus the value function of all possible next state with a discount.&lt;/p&gt;

&lt;p&gt;So, the optimal value function should be the value function obtained by optimal policy:&lt;/p&gt;

&lt;p&gt;$$ V^{\star}(s) = max_{\pi} V_{\pi}(s) = R(s) + \gamma max_{a \in A} \sum_{s&amp;rsquo; \in S} P(s&amp;rsquo;|s, a) V^{\star}(s&amp;rsquo;) $$&lt;/p&gt;

&lt;p&gt;The second term
is the maximum over all actions $a$ of the expected future sum of discounted
rewards we’ll get upon after action $a$. It is very easy to understand, just go with the optimal policy and the value function will also be optimal.&lt;/p&gt;

&lt;p&gt;From this equation, the optimal policy will guarantee the largest future reward from certain state. This means that we can use the same policy $\pi^{\star}$ no matter what the initial state of our MDP is.&lt;/p&gt;

&lt;h2 id=&#34;policy-value-iteration&#34;&gt;Policy/Value iteration&lt;/h2&gt;

&lt;p&gt;We now describe two efficient algorithms for solving finite-state MDPs, $|S| &amp;lt; \infty$, $|A| &amp;lt; \infty$.&lt;/p&gt;

&lt;p&gt;The frist one is &lt;strong&gt;value iteration&lt;/strong&gt;:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;For each state s, initialize V (s) := 0.&lt;/li&gt;
&lt;li&gt;Repeat until convergence
{&lt;br /&gt;
For every state, update $V (s) := R(s) + max_{a \in A} \gamma \sum_{s&amp;rsquo; \in S} P(s&amp;rsquo;|a, s) V(s&amp;rsquo;)$&lt;br /&gt;
}&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This algorithm iteratively update estimated value function using Bellman equation.There are two kinds of value iteration, asynchronous or synchronous. The first one will update value function for every step, while the second one will overwrite all value functions after one iteration. After the algorithm converges, we can easily get optimal policies with value functions.&lt;/p&gt;

&lt;p&gt;Another algorithm is &lt;strong&gt;policy iteration&lt;/strong&gt;:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Initialize $\pi$ randomly.&lt;/li&gt;
&lt;li&gt;Repeat until convergence {&lt;br /&gt;
(a) Let $V := V_{\pi}$&lt;br /&gt;
(b) For each state s, let $π(s) := argmax_{a \in A} P(s&amp;rsquo;|s, a) V(s)$&lt;br /&gt;
}&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Thus, the inner-loop repeatedly computes the value function for the current
policy, and then updates the policy using the current value function.&lt;/p&gt;

&lt;p&gt;Note that step (a) can be done via solving Bellman’s equations
as described earlier, which in the case of a fixed policy, is just a set of |S|
linear equations in |S| variables.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Both value iteration and policy iteration are standard algorithms for solving
MDPs, and there isn’t currently universal agreement over which algorithm
is better. For small MDPs, policy iteration is often very fast and
converges with very few iterations. However, for MDPs with large state
spaces, solving for $V_{\pi}$
explicitly would involve solving a large system of linear
equations, and could be difficult. In these problems, value iteration may
be preferred. For this reason, in practice value iteration seems to be used
more often than policy iteration. &amp;ndash; From CS229 by Andrew Ng&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;All above derivation does not learn the state transition dynamics, $P(s|s&amp;rsquo;, a)$. In many realistic problems,
we are not given state transition probabilities and rewards explicitly,
but must instead estimate them from data.&lt;/p&gt;

&lt;p&gt;The most naive approach is just counting:&lt;/p&gt;

&lt;p&gt;$$P(s|s&amp;rsquo;, a) = \frac{times \space took \space we \space action \space a  \space in \space state  \space s \space and \space got \space to \space s_0}{times \space took \space we \space action \space a \space in \space state \space s}$$&lt;/p&gt;

&lt;p&gt;Note that the $0/0$ situation may happen, a little smooth is necessary.&lt;/p&gt;

&lt;p&gt;So, here comes a easy algorithm:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Initialize π randomly.&lt;/li&gt;
&lt;li&gt;Repeat {&lt;br /&gt;
a. Execute $\pi$ in the MDP for some number of trials.&lt;br /&gt;
b. Using the accumulated experience in the MDP, update our estimates
for $P(s|s&amp;rsquo;, a)$&lt;br /&gt;
c. Apply value iteration with the estimated state transition probabilities
and rewards to get a new estimated value function V .&lt;br /&gt;
d. For each state s, let $π(s) := argmax_{a \in A} P(s|s&amp;rsquo;, a) V(s)$&lt;br /&gt;
}&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;MDP provides a mathematical framework for modeling decision making. MDPs are useful for studying optimization problems solved via dynamic programming and reinforcement learning. It can be used in many disciplines, including robotics, automatic control, economics and manufacturing. Next part, we wil dive into reinforcement learning algorithms.&lt;/p&gt;

&lt;h1 id=&#34;imitation-learning&#34;&gt;Imitation Learning&lt;/h1&gt;

&lt;p&gt;In the beginning, we may let the machine mimic human behaviors. In this way, it is easy to give human supervision. Just let the human expert complete the task, and their hehavior should be the label to train algorithms.&lt;/p&gt;

&lt;p&gt;For example, we can easily train a neural network to fit human expert&amp;rsquo;s behaviors, like shown in the following diagram:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/RL/imitation_learning.png&#34; width=800&gt;&lt;/p&gt;

&lt;p&gt;But this method has two main drawbacks:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Since human experts seldom make mistake, the training samples of some extreme cases will be rare. But such extreme situations is critical for a controller. For example, an autonomous vehice should be able to deal with some unexpected obstacles, but such training sample is not efficient to draw from human experts.&lt;/li&gt;
&lt;li&gt;Errors are accumulative. Every state, the controller might make some mistakes, and for next state the controller will make decision based on current state. So, if the controller make some mistakes in some early stages, the whole trajectory should be totally different and the error will be bigger and bigger, as shown in the following figure.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&#34;/img/RL/error_accumulate.jpg&#34; width=600&gt;&lt;/p&gt;

&lt;p&gt;One possible algorithm to mitigate this problem: DAgger (Dataset Aggregation). The main idea is using human expert to correct the wrong behavior rather than using human supervision all the time.&lt;/p&gt;

&lt;p&gt;DAgger:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;train $\pi_{\theta}(a|o)$ with small amount of training data, $D = $ {$a_1, o_1, a_2, o_2, &amp;hellip;, a_N, o_N$}&lt;/li&gt;
&lt;li&gt;Run $\pi_{\theta}(a|o)$ to get extra observations, $D_{\pi}$ = {$o_1, o_2, &amp;hellip;, o_M$}&lt;/li&gt;
&lt;li&gt;Let human expert to label all inappropriate observations.&lt;/li&gt;
&lt;li&gt;Merge the original data and extra data. $D_{new} = D_{\pi} \cup D$&lt;/li&gt;
&lt;li&gt;Iterate to step 1 until converge&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In this way, the algorithm addresses the error cases to make the controller better. But still, our model may fail to fit human experts.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Non-Markov condition. Sometimes, human expert&amp;rsquo;s may not based on current observation, but a combination of several previous observations. RNN may help in such situation, but it will also make our model exponentially complex.&lt;/li&gt;
&lt;li&gt;Multi-modal output. Once hearing the word &amp;ldquo;Multi-modal&amp;rdquo;, the first thing come to my mind is always MoG (mixture of Gaussian). We can also use latent variable models or autoregressive discretization. Latent variable models will inject some other distribution at the input od neural network, and let the output use this additional distribution properly. Autoregressive discretization will discretize one dimension at a time instead of discretize the output at a time to avoid the dimensionality explosion. It is a great way to handle high dimensional output. The first network predict $P(d_1|O)$, while next network predict $P(d_2 | d_1, O)$ and so on.&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;basics&#34;&gt;Basics&lt;/h1&gt;

&lt;h2 id=&#34;policy-gradient&#34;&gt;Policy Gradient&lt;/h2&gt;

&lt;p&gt;The basic idea of policy gradient algorithm is using gradient ascent to optimize policy distribution in order to obtain higher trajectroy reward. Let&amp;rsquo;s start the derivation from RL objective.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/RL/derivation_1.png&#34; width=600&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/RL/derivation_2.png&#34; width=900&gt;&lt;/p&gt;

&lt;p&gt;Since the $\nabla_{\theta} log p(s_{t+1}|s_t, a_t)]$ term is zero because $ log p(s_{t+1}|s_t, a_t)]$ is not related to $\theta$. We have the final result:&lt;/p&gt;

&lt;p&gt;$$ \nabla_{\theta} J(\theta) =  E_{\tau \sim p(\tau)} [\sum_{t=1}^{T} log\pi_{\theta} (a_t | s_t) \sum_{t=1}^{T} r(a_t, s_t)] $$&lt;/p&gt;

&lt;p&gt;For N experimental trails, we can approximate the expection:&lt;/p&gt;

&lt;p&gt;$$\nabla_{\theta} J(\theta) = \frac{1}{N} \sum_{i=1}^N [\sum_{t=1}^{T} log\pi_{\theta} (a_{t, i} | s_{t, i}) \sum_{t=1}^{T} r(a_{t, i}, s_{t, i})]$$&lt;/p&gt;

&lt;p&gt;In this way, we get the REINFORCE algorithm:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Run the policy to obtain a bunch of trajectories: {$\tau_i$}&lt;/li&gt;
&lt;li&gt;Calculate gradient with above formula: $\nabla_{\theta} J(\theta) = \frac{1}{N} \sum_{i=1}^N [\sum_{t=1}^{T} \nabla_{\theta} log\pi_{\theta} (a_{t, i} | s_{t, i}) \sum_{t=1}^{T} r(a_{t, i}, s_{t, i})]$&lt;/li&gt;
&lt;li&gt;Gradient ascent: $ J(\theta) =  J(\theta) + \alpha \nabla_{\theta} J(\theta)$&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;If compare to the maximum likelihood:&lt;/p&gt;

&lt;p&gt;$$\nabla_{\theta} J(\theta) = \frac{1}{N} \sum_{i=1}^N [\sum_{t=1}^{T} \nabla_{\theta} log\pi_{\theta} (a_{t, i} | s_{t, i})]$$&lt;/p&gt;

&lt;p&gt;The policy gradient is just a reward weighted version of maximum likelihood.&lt;/p&gt;

&lt;p&gt;Seems like that&amp;rsquo;s it. But we are not done yet. Algorithms with policy gradient have a big problem, sometimes even make the whole algorithm
break down. First, let&amp;rsquo;s analyze why this problem appears, and then find some strategies for this problem.&lt;/p&gt;

&lt;p&gt;There are two reasons for the high variance:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Policy gradient is evaluated on a whole trajectory. This trajectory contains a lot of times steps. Assuming every time step the agent can have A actions, this will bring $A^T$ complexity to the system. In this way, our samples can act dramatically different. And the policy gradient is just an average of gradients of these diverging trajectories. If we have a large enough sample size N, this problem should be OK. But usually N is a much smaller number than time step T, where this problem is significant.&lt;/li&gt;
&lt;li&gt;The policy update itself can also make policies diverse. A negative reward trajectory may squeeze the distribution probability mass to its left and right, and if we increase reward function to all trajectories, the policy update is different, etc. There are a lot of other situations that increases variance. And most imporvement on policy gradient is on this aspect.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;No surprise, here comes improvements：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Introduce causality. Since policy in time step t cannot affect the reward in time step $t&amp;rsquo;$. So, we can change the formula a little bit: $$\nabla_{\theta} J(\theta) = \frac{1}{N} \sum_{i=1}^N [\sum_{t=1}^{T} \nabla_{\theta} log\pi_{\theta} (a_{t, i} | s_{t, i}) \sum_{t=1}^{T} r(a_{t, i}, s_{t, i})] $$ into $$\nabla_{\theta} J(\theta) = \frac{1}{N} \sum_{i=1}^N \sum_{t=1}^{T}\nabla_{\theta} log\pi_{\theta}(a_{t, i} | s_{t, i})  [\sum_{t&amp;rsquo;=t}^{T} r(a_{t&amp;rsquo;, i}, s_{t&amp;rsquo;, i})]$$
Because $\sum_{t&amp;rsquo;=t}^{T} r(a_{t&amp;rsquo;, i}, s_{t&amp;rsquo;, i})$ always has smaller variance than its original form. This  is also called $\hat{Q}_{i,t}$, means reward to go, all reward after time step t. This function is very useful afterwards.&lt;/li&gt;
&lt;li&gt;Introduce baseline. Actually, policy gradient method want to make the prob of high reward trajectory higher and the prob of low reward trajectory lower. But if both trajectories have positive rewards, the effect on probability distribution will be diluted. We want the prob of low reward trajectory to be lower by make the reward to be negative. How? Using a relative reward.
$$\nabla_{\theta} J(\theta) = \frac{1}{N} \sum_{i=1}^N \sum_{t=1}^{T} log\pi_{\theta} (a_{t, i} | s_{t, i}) [\sum_{t=1}^{T} r(a_{t, i}, s_{t, i}) - b] $$
This $b$can be easily as $\sum_{i=1}^N\sum_{t=1}^{T} r(a_{t, i}, s_{t, i})$, just an average of all trajectory rewards. And this $b$ can some other values to further reduce variance.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;And this is all for policy gradient.&lt;/p&gt;

&lt;h2 id=&#34;actor-critic-algorithm&#34;&gt;Actor-Critic Algorithm&lt;/h2&gt;

&lt;p&gt;Previous, when discussing about casuality in policy gradient, we already have estimated reward to go: $\hat{Q}_{i,t} = \sum_{t&amp;rsquo;=t}^{T} r(a_{t&amp;rsquo;, i}, s_{t&amp;rsquo;, i})$. But if we have the exact reward to go: $Q(s_t, a_t) = E_{\pi_{\theta}}[\sum_{t&amp;rsquo;=t}^{T}r(s_{t&amp;rsquo;}, a_{t&amp;rsquo;})]$, we can further reduce the variance.
And another thing, the value function for state $s_t$ is a pretty good baseline. Based on the definition of value function: $V^{\pi}(s_t) = E_{a_t \sim \pi(a_t | s_t)} [Q^{\pi} (s_t, a_t)]$, it is an expected average of all possible Qs. And we define $A^{\pi} (s_t, a_t) = Q^{\pi} (s_t, a_t) - V^{\pi}(s_t)$ to estimate how better $Q^{\pi} (s_t, a_t)$ is. So, the policy gradient formula becomes:&lt;/p&gt;

&lt;p&gt;$$\nabla_{\theta} J(\theta) = \frac{1}{N} \sum_{i=1}^N \sum_{t=1}^{T} \nabla_{\theta} log\pi_{\theta} (a_{t, i} | s_{t, i}) A^{\pi} (s_{t, i}, a_{t, i})$$&lt;/p&gt;

&lt;p&gt;Here comes the derivation of actor-critic:&lt;/p&gt;

&lt;p&gt;$$Q^{\pi} (s_t, a_t) = r(s_t, a_t) + E_{s_{t+1} \sim P(s_{t+1} | s_t, a_t)}[V^{\pi} (s_{t+1})]$$
$$\approx r(s_t, a_t) + V^{\pi} (s_{t+1})$$&lt;/p&gt;

&lt;p&gt;So, $A^{\pi} (s_t, a_t) = r(s_t, a_t) +  + V^{\pi} (s_{t+1}) - V^{\pi} (s_{t})$&lt;/p&gt;

&lt;p&gt;How about fitting the value function?&lt;/p&gt;

&lt;p&gt;$$V^{\pi} (s_{t}) = \sum_{t&amp;rsquo;=t}^{T} E_{\pi_{\theta}}[r(s_{t&amp;rsquo;}, a_{t&amp;rsquo;}) | s_t] $$&lt;/p&gt;

&lt;p&gt;Here are some evaluation methods. For N experimental trails:&lt;/p&gt;

&lt;p&gt;$$V^{\pi} (s_{t}) = \frac{1}{N} \sum_{i=1}^{N} \sum_{t&amp;rsquo;=t}^{T} r(s_{t&amp;rsquo;, i&amp;rsquo;}, a_{t&amp;rsquo;, i&amp;rsquo;}) $$&lt;/p&gt;

&lt;p&gt;But here, we need a lot of trails comes from {$s_t, a_t$}. We may need to reset the simulator. But we can use a function approximator like a neural network. The function approximator have the assumption that the function is smooth. In such cases, we do not need to have a lot of examples from {$s_t, a_t$}, some samples near {$s_t, a_t$} should also be OK. Although it is not as good as direct evaluation, but good enough.&lt;/p&gt;

&lt;p&gt;So, train neural network with supervision $Y_{s_t}$, the evaluated value:&lt;/p&gt;

&lt;p&gt;$$L_{\phi} = \frac{1}{2} ||V_{fit}(s_t) - Y_{s_t}||^2$$&lt;/p&gt;

&lt;p&gt;Another way to evaluate $V^{\pi} (s_{t})$ need some approximation:&lt;/p&gt;

&lt;p&gt;$$Y_{s_t} = \sum_{t&amp;rsquo;=t}^{T} E_{\pi_{\theta}}[r(s_{t&amp;rsquo;}, a_{t&amp;rsquo;}) | s_t] = r(s_{t}, a_{t}) + V^{\pi} (s_{t+1})$$&lt;/p&gt;

&lt;p&gt;Now, we can use $r(s_{t}, a_{t}) + V_{fit}(s_{t+1})$ as a target value to update the neural network. It&amp;rsquo;s like a bootstrapping strategy reusing learned function again and again. After formulizing this, here comes actor-critic algorithm:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Sample {$s_i, a_i$} from policy $\pi_{\theta}(a_t|s_t)$&lt;/li&gt;
&lt;li&gt;Fit $\hat{V}^{\pi} (s_{i})$ with reward sum&lt;/li&gt;
&lt;li&gt;Evaluate $\hat{A}(s_{i}, a_{i}) = r(s_{i}, a_{i}) + \hat{V}^{\pi} (s_{i}’) - \hat{V}^{\pi} (s_{i})$&lt;/li&gt;
&lt;li&gt;Using policy gradient:  $\nabla_{\theta} J(\theta) = \frac{1}{N} \sum_{i=1}^N \nabla log\pi_{\theta} (a_{i} | s_{i}) \hat{A}(s_{i}, a_{i})$&lt;/li&gt;
&lt;li&gt;Update policy network: $ J(\theta) =  J(\theta) + \alpha \nabla_{\theta} J(\theta)$&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;And a online version:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Take action $a \sim \pi_{\theta}(a|s)$, and get a training sample $(s, a, s&amp;rsquo;, r)$&lt;/li&gt;
&lt;li&gt;Update $\hat{V}^{\pi} (s_{i})$ with target $r + \hat{V}^{\pi} (s&amp;rsquo;_{i})$&lt;/li&gt;
&lt;li&gt;Evaluate $\hat{A}(s, a) = r(s, a) + \hat{V}^{\pi} (s&amp;rsquo;) - \hat{V}^{\pi} (s)$&lt;/li&gt;
&lt;li&gt;Using policy gradient:  $\nabla_{\theta} J(\theta) =  \nabla log\pi_{\theta} (a | s) \hat{A}(s, a)$&lt;/li&gt;
&lt;li&gt;Update policy network: $ J(\theta) =  J(\theta) + \alpha \nabla_{\theta} J(\theta)$&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Another thing to mention, we can add discount factor like the MDPs. It is pretty straight forward to do.&lt;/p&gt;

&lt;p&gt;Now, we have two major algorithms, policy gradient and actor critic:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Policy gradient: $\nabla_{\theta} J(\theta) = \frac{1}{N} \sum_{i=1}^N \sum_{t=1}^{T} log\pi_{\theta} (a_{t, i} | s_{t, i}) [\sum_{t&amp;rsquo;=t}^{T} \gamma^{t&amp;rsquo;-t}r(a_{t&amp;rsquo;, i}, s_{t&amp;rsquo;, i}) - b] $. It is unbiased, but with high variance. This is a Monte Carlo estimation method.&lt;/li&gt;
&lt;li&gt;Actor-critic: $\nabla_{\theta} J(\theta) = \frac{1}{N} \sum_{i=1}^N \sum_{t=1}^{T} log\pi_{\theta} (a_{t, i} | s_{t, i}) [r(s_{i}, a_{i}) + \gamma \hat{V}^{\pi} (s_{i}’) - \hat{V}^{\pi} (s_{i})] $. This method have low variance, since we are not add multiple time steps. But it is not unbiased, because the estimated value function may not be accurate. This is a temporal difference control method, which based on Dynamic Programming, updating result recursively from previous results.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Combining the above two methods, we can build a bridge from TD control method to Monte Carlo method.  $\nabla_{\theta} J(\theta) = \frac{1}{N} \sum_{i=1}^N \sum_{t=1}^{T} log\pi_{\theta} (a_{t, i} | s_{t, i}) [\sum_{t&amp;rsquo;=t}^{T} \gamma^{t&amp;rsquo;-t} r(a_{t&amp;rsquo;, i}, s_{t&amp;rsquo;, i}) - \hat{V}^{\pi} (s_{i})] $
In this way, there is no bias and the variance is smaller than pure policy gradient, but of course, cannot compare to actor-critic.&lt;/p&gt;

&lt;p&gt;To sum up, actor-critic algorithm use function approximator to fit value function and successfully replace large time step reward sum into the sum of current reward and next state reward function. This method deals with the first problem of policy gradient, as mentioned before.
More things to read:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;N-step return to balance bias and variance. $\nabla_{\theta} J(\theta) = \frac{1}{N} \sum_{i=1}^N \sum_{t=1}^{T} log\pi_{\theta} (a_{t, i} | s_{t, i}) [\sum_{t&amp;rsquo;=t}^{t+n} \gamma^{t&amp;rsquo;-t} r(a_{t&amp;rsquo;, i}, s_{t&amp;rsquo;, i}) - \hat{V}^{\pi} (s_{t}) + \gamma^{n} \hat{V}^{\pi} (s_{t+n})] $. N &amp;gt; 1 often works!&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1506.02438&#34; target=&#34;_blank&#34;&gt;General advantage estimator. &lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;temporal-difference-learning&#34;&gt;Temporal Difference Learning&lt;/h1&gt;

&lt;blockquote&gt;
&lt;p&gt;If one had to identify one idea as central and novel to reinforcement learning, it would undoubtedly
be temporal-difference (TD) learning. &amp;ndash; Sutton&amp;rsquo;s RL book&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;At this point, we need to a wrap up before unrolling new stuff. Previous method, like policy gradient, actor-critic, belongs to Monte Carlo (MC) methods, which we sample the trajectory as an episode and make updates based on the rewards. In this way, we have to wait until the whole eposide ends to update our model. While TD learning is a conbination of Monte Carlo method and dynamic programming (DP), and can estimates based on learned estimations, without a final outcome. This bootstrapping fashion makes TD learning usually converge faster than MC methods.&lt;/p&gt;

&lt;p&gt;Actually, the actor critic algorithm also get some essence of TD learning. The update of state value function and advantage is based on previous estimated value function.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s derive TD learning based on MC methods:&lt;/p&gt;

&lt;p&gt;$$V(S_t) = V(S_t) + \alpha [\sum_{t=1}^{T} \gamma ^ t R_{ t} - V(S_t)]$$&lt;/p&gt;

&lt;p&gt;According to Sutton&amp;rsquo;s book, note $\sum_{k=t+1}^{T} \gamma ^ {k-t-1} R_{k}$ as $G_t$. This becomes:&lt;/p&gt;

&lt;p&gt;$$V(S_t) = V(S_t) + \alpha [G_t - V(S_t)]$$&lt;/p&gt;

&lt;p&gt;This is a typical MC value iteration algorithm, updating value function based on MC samples in the end of an eposide. TD learning method only need to wait until next step:&lt;/p&gt;

&lt;p&gt;$$V(S_t) = V(S_t) + \alpha [R_{t+1} + \gamma V(S_{t+1}) - V(S_t)]$$&lt;/p&gt;

&lt;p&gt;This is called one-step TD or TD(0), because it only uses next step estimations. It is more clear written in this way:&lt;/p&gt;

&lt;p&gt;$$V_{\pi} = E_{\pi}[G_t | S_t = s] \\&lt;br /&gt;
V_{\pi} =  E_{\pi}[R_{t+1} + \gamma G_{t+1} | S_t = s] \\&lt;br /&gt;
V_{\pi} =  E_{\pi}[R_{t+1} + \gamma V(S_{t+1}) | S_t = s]$$&lt;/p&gt;

&lt;p&gt;MC method uses the first equation, and TD method uses the third equation. And also, MC error can be written as a sum of TD errors (Image from Sutton&amp;rsquo;s book):&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/RL/TD_error.jpg&#34; width=1000&gt;&lt;/p&gt;

&lt;p&gt;TD learning is a model-free algorithm, does not require system dynamics. And it has an online, fully incremental update fashion, which is very important when single episode is long. In practice, TD methods are found to converge faster than MC methods. MC method is optimizing on the &lt;strong&gt;training dataset&lt;/strong&gt;, while TD method is optimized to the maximum-likelihood model of the &lt;strong&gt;Markov decision process&lt;/strong&gt; and the reward is averaged on different transitions.&lt;/p&gt;

&lt;h2 id=&#34;sarsa&#34;&gt;Sarsa&lt;/h2&gt;

&lt;p&gt;Sarsa is an on-policy TD control algorithm. Rather tahn learn a state-value function, it learns an action-value fucntion. We gonna estimate quality function $Q_{\pi}(s,a)$ for all state and action pair. Literally, it is the same process as fitting $V_{\pi}$.&lt;/p&gt;

&lt;p&gt;The update rule is very similar:&lt;/p&gt;

&lt;p&gt;$$Q(S_t, A_t) = Q(S_t, A_t) + \alpha [R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)]$$&lt;/p&gt;

&lt;p&gt;Update is done for every transition with their learned estimation and reward, like before. With learned $ Q(S_t, A_t)$, it&amp;rsquo;s easy to find a greedy policy to maximize our reward. But during training time, we need some exploration. Greedy policy may just lead us to a local optimum. So, the policy in Sarsa is &amp;ldquo;$\varepsilon$-greedy&amp;rdquo; policy, which have a prob of $\varepsilon$ to choose some other actions.&lt;/p&gt;

&lt;p&gt;Here is Sarsa: For every steps in one eposide:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;At state S, take action A, get R, and S&amp;rsquo;&lt;/li&gt;
&lt;li&gt;Choose A&amp;rsquo; from S&amp;rsquo; using $\varepsilon$-greedy policy&lt;/li&gt;
&lt;li&gt;$Q(S_t, A_t) = Q(S_t, A_t) + \alpha [R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)]$&lt;/li&gt;
&lt;li&gt;S=S&amp;rsquo;, A=A&amp;rsquo;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;After one eposide, start a new one until termination. This is clearly a on-policy algorithm.&lt;/p&gt;

&lt;h2 id=&#34;q-learning&#34;&gt;Q-learning&lt;/h2&gt;

&lt;p&gt;From Sarsa, if we modify the update rule a little bit, we can easily get Q-learning:&lt;/p&gt;

&lt;p&gt;$$Q(S_t, A_t) = Q(S_t, A_t) + \alpha [R_{t+1} + \gamma max_a Q(S_{t+1}, a) - Q(S_t, A_t)]$$&lt;/p&gt;

&lt;p&gt;Instead of choosing action with &amp;ldquo;$\varepsilon$-greedy&amp;rdquo; policy, Q-learning directly choose $max_a Q(S_{t+1}, a)$. Directly optimizing on optim value can make the algorithm even simpler and converge faster. Here is Q-learning:&lt;/p&gt;

&lt;p&gt;For each step of eposide:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Choose A from S using $\varepsilon$-greedy policy&lt;/li&gt;
&lt;li&gt;Take action A,  get R, and S&amp;rsquo;&lt;/li&gt;
&lt;li&gt;$Q(S_t, A_t) = Q(S_t, A_t) + \alpha [R_{t+1} + \gamma max_a Q(S_{t+1}, a) - Q(S_t, A_t)]$&lt;/li&gt;
&lt;li&gt;S=S&amp;rsquo;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Also, after an eposide, start a new one.&lt;/p&gt;

&lt;p&gt;One thing to note, Q-learning is an off-policy algorithm. It follows current $Q(S_t, A_t)$ to choose actions and proceed, but when updating, it directly choose $argmax_a Q(S_{t+1}, a)$ rather than the chosen action. In another words, it update its Q-values using the Q-value of the next state s′ and the &lt;strong&gt;greedy action&lt;/strong&gt; a′. But it&amp;rsquo;s not following the greedy policy. While Sarsa uses &amp;ldquo;$\varepsilon$-greedy&amp;rdquo; policy to choose action and update Q-values, and has only one action selection each time step.&lt;/p&gt;

&lt;p&gt;From Sutton&amp;rsquo;s book, the cliff walking problem indicate the difference between Sarsa and Q-learning very clearly.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/RL/cliffwalking.jpg&#34; width=900&gt;&lt;/p&gt;

&lt;p&gt;The agent starts from S and the goal is G. Falling to the cliff will result in a -100 reward and start from S and every step have a -1 reward to push the agent to optimal. Q-learning can directly learn the shorter path, while Sarsa learns the safer path. Since we are using a &amp;ldquo;$\varepsilon$-greedy&amp;rdquo; policy, Sarsa will consider the probability of falling, while Q-learing always update with a greedy policy, so the update will ignore the worest falling cases and optimize our model to the shortest path. Of course, Sarsa can get a higher reward expection during training.&lt;/p&gt;

&lt;h2 id=&#34;expected-sarsa&#34;&gt;Expected Sarsa&lt;/h2&gt;

&lt;p&gt;From Q-learning, considering instead of the maximum over next state–action pairs, we take expected value over all possible actions.&lt;/p&gt;

&lt;p&gt;$$Q(S_t, A_t) = Q(S_t, A_t) + \alpha [R_{t+1} + \gamma E_{\pi}[Q(S_{t+1}, A_{t+1})] - Q(S_t, A_t)]$$&lt;/p&gt;

&lt;p&gt;Expected Sarsa moves in expectation while Q-learning moves deterministically. Expected Sarsa is more complex computationally than Sarsa but, in return, it eliminates
of Q-learning on this problem. Given the same amount of experience we might expect it to perform slightly better than Sarsa, and indeed it generally does. When policy is purely greedy, not exploratory, Expected Sarsa is exactly Q-learning.&lt;/p&gt;

&lt;h2 id=&#34;double-q-learning&#34;&gt;Double Q-Learning&lt;/h2&gt;

&lt;p&gt;Algorithms we discussed above, like Sarsa, Q-learing, involve a maximization over all actions via policy. This maximization operation can lead to a significant positive bias. It is easy to understand. For example, if the true value of Q(s, a) is zero, and it&amp;rsquo;s a Gaussian with some variance. The sample generated should have a mean of zero, but the expected maximum is definitely above 0. Q-learning will consistently choose the maximum and bring in a big bias. In math:  $$\mathbb{E}_{\epsilon}\left[\max _{a^{\prime}}\left(Q\left(s^{\prime}, a^{\prime}\right)+\epsilon\right)\right] \geq \max _{a^{\prime}} Q\left(s^{\prime}, a^{\prime}\right) $$ Accordingly, the estimated max will generally be greater than true maximum, and this bias will be propagated through Bellman equation.&lt;/p&gt;

&lt;p&gt;From another perspective, we can consider Jensen&amp;rsquo;s inequality in the Bellman backup. We already know the following equation:&lt;/p&gt;

&lt;p&gt;$$Q(\mathbf{s}, \mathbf{a}) \leftarrow r(\mathbf{s}, \mathbf{a})+\gamma  \mathbb{E}_{\mathbf{s}^{\prime} \sim p\left(\mathbf{s}^{\prime} | \mathbf{s}, \mathbf{a}\right)}\left[V\left(\mathbf{s}^{\prime}\right)\right]$$&lt;/p&gt;

&lt;p&gt;In Q learning, we use greedy policy, the equation becomes:&lt;/p&gt;

&lt;p&gt;$$Q(\mathbf{s}, \mathbf{a}) \leftarrow r(\mathbf{s}, \mathbf{a})+\gamma \max _{\mathbf{a}^{\prime}} \mathbb{E}_{\mathbf{s}^{\prime} \sim p\left(\mathbf{s}^{\prime} | \mathbf{s}, \mathbf{a}\right)}\left[Q\left(\mathbf{s}^{\prime}, \mathbf{a}^{\prime}\right)\right]$$&lt;/p&gt;

&lt;p&gt;But in real implementation, we use:&lt;/p&gt;

&lt;p&gt;$$Q(\mathbf{s}, \mathbf{a}) \leftarrow r(\mathbf{s}, \mathbf{a})+\gamma  \mathbb{E}_{\mathbf{s}^{\prime} \sim p\left(\mathbf{s}^{\prime} | \mathbf{s}, \mathbf{a}\right)}\left[\max _{\mathbf{a}^{\prime}} Q\left(\mathbf{s}^{\prime}, \mathbf{a}^{\prime}\right)\right]$$&lt;/p&gt;

&lt;p&gt;Apparently, there is a bias since we know the following Jenson Inequality: $$\mathbb{E}_{\mathbf{s}^{\prime} \sim p\left(\mathbf{s}^{\prime} | \mathbf{s}, \mathbf{a}\right)}\left[\max _{\mathbf{a}^{\prime}} Q\left(\mathbf{s}^{\prime}, \mathbf{a}^{\prime}\right)\right] \geq \max _{\mathbf{a}^{\prime}} \mathbb{E}_{\mathbf{s}^{\prime} \sim p\left(\mathbf{s}^{\prime} | \mathbf{s}, \mathbf{a}\right)}\left[Q\left(\mathbf{s}^{\prime}, \mathbf{a}^{\prime}\right)\right]$$&lt;/p&gt;

&lt;p&gt;Double Q-Learning was born to deal with the positive bias. The idea is using two different and independent Q-tables or Q-network to decorrelate the selection of the best action from the evaluation of this action. $max(E[Q_1], E[Q_2]) &amp;lt;= E[max(Q_1, Q_2)]$. Basically, two Q estimation should be both noisy, but if they are noisy in different ways, it should be OK. So, Double Q-learning does not use the same network to choose action and evaluate value:&lt;/p&gt;

&lt;p&gt;$$Q_1(S_t, A_t) = Q_1(S_t, A_t) + \alpha [R_{t+1} + \gamma Q_2(S_{t+1},argmax_a Q_1(S_{t+1},a)) - Q_1(S_t, A_t)]$$&lt;/p&gt;

&lt;p&gt;And for another 50% probability, switch $Q_1$ and $Q_2$ to update $Q_2$. This is the double version of Q-learning. Of course, there are also double versions of Sarsa and Expected Sarsa.&lt;/p&gt;

&lt;h2 id=&#34;n-step-return&#34;&gt;N-step Return&lt;/h2&gt;

&lt;p&gt;N-step return is an intermediate algorithm between MC and TD(0) methods. MC method can be treated as an infinity time steps return algorithm, we ahve to wait until eposide ends. And TD(0) method is one-step return, which directly uses the evaluation of next time step.
N-step return algorithms can migitate problems of both algorithms when using a proper N.&lt;/p&gt;

&lt;p&gt;Here is a typical backup diagram of MC, TD(0), and N-step algorithms (image from Sutton&amp;rsquo;s book):&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/RL/n-step.jpg&#34; width=600&gt;&lt;/p&gt;

&lt;p&gt;Originally in MC method,&lt;/p&gt;

&lt;p&gt;$$G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + &amp;hellip; + \gamma^{T-t-1} R_{T}$$&lt;/p&gt;

&lt;p&gt;And in TD(0),&lt;/p&gt;

&lt;p&gt;$$G_{t:t+1} = R_{t+1} + \gamma V(S_{t+1})$$&lt;/p&gt;

&lt;p&gt;So in N-step:&lt;/p&gt;

&lt;p&gt;$$G_{t:t+n} = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + &amp;hellip; + \gamma^{n-1} R_{t+n} + \gamma^n V(S_{t+n+1})$$&lt;/p&gt;

&lt;p&gt;And the update rule for state-value TD method:&lt;/p&gt;

&lt;p&gt;$$V_{t+n}(S_t) = V_{t+n-1}(S_t) + \alpha [G_{t:t+n} - V_{t+n-1}(S_t)]$$&lt;/p&gt;

&lt;p&gt;Update rule for action-state TD method, like Sarsa:&lt;/p&gt;

&lt;p&gt;$$Q_{t+n}(S_t, A_t) = Q_{t+n-1}(S_t, A_t) + \alpha [G_{t:t+n} - Q_{t+n-1}(S_t, A_t)]$$&lt;/p&gt;

&lt;p&gt;But for N-step return, we have to wait N steps until getting the update values. It can be viewed as a control over variance and bias trade-off. Also, it gives us more freedom. Bootstrapping works best if it is over a length of time in which a significant and recognizable state change has occurred, and a significant and recognizable state change may takes several time steps. Accordingly, N-step usually works better than pure MC or TD method. But theoritically, this is only correct for online algorithms. We can ignore this difference, and it oftern works well.&lt;/p&gt;

&lt;h2 id=&#34;eligibility-trace-and-td-lambda&#34;&gt;Eligibility Trace and TD($\lambda$)&lt;/h2&gt;

&lt;p&gt;At this point, the N-step return method has been defined. It averages N step return in the future for a compound update. one could average one-step and infinite-step returns to obtain another way of interrelating TD and Monte Carlo methods.&lt;/p&gt;

&lt;p&gt;In practice, one may want to limit the length of the longest component update because of the corresponding delay. When the delay gets longer, the update component seems less correlated to current update since the trajectory probability from current step to far away step is getting smaller.&lt;/p&gt;

&lt;p&gt;The TD(λ) algorithm can be understood as one particular way of averaging n-step
updates. This average contains all the n-step updates, each weighted proportional to $\lambda ^{n-1}$. The resulting update is toward a return, called the λ-return,
defined in its state-based form by:&lt;/p&gt;

&lt;p&gt;$$G_{t}^{\lambda} \doteq(1-\lambda) \sum_{n=1}^{\infty} \lambda^{n-1} G_{t : t+n}$$&lt;/p&gt;

&lt;p&gt;This is called λ-return method, and it&amp;rsquo;s an offline algorithm since one have to wait until trajectory ends to update. Sutton&amp;rsquo;s book provides perfect diagram to illustrate this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/RL/lambda-return.jpg&#34; width=500&gt;&lt;/p&gt;

&lt;p&gt;Right now, the method is a forward view algorithm. Basically, when processing time step t, we stay at time t and collection TD error from all time steps in the future and make update for current step. The disgram from Sutton&amp;rsquo;s book is very good:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/RL/forward-lambda.jpg&#34; width=900&gt;&lt;/p&gt;

&lt;p&gt;Essentailly, this method is not quite efficient. We don&amp;rsquo;t want to wait several steps until making updates. So, usually backward view TD(λ) is widely used. In backward view TD(λ), TD error of every time step will be propagate to future time step with a fade away of $\gamma \lambda$. And the propagated error will also be updated at each step, which can be considered as an update for all previous steps. For example, standing at time step t, we have all previous time step $(0~t-1)$ TD error accumulated. The update will updat all TD errors from time step 0 to t-1. Here is a diagram for the backward view method:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/RL/backward-lambda.jpg&#34; width=900&gt;&lt;/p&gt;

&lt;p&gt;If considered in a whole trajectory, backward view and forward view is equivalent but more efficient. In Sutton&amp;rsquo;s book, value gradient is propagated, which is the same strategy:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/RL/TD-lambda.jpg&#34; width=700&gt;&lt;/p&gt;

&lt;h2 id=&#34;deep-q-network&#34;&gt;Deep Q network&lt;/h2&gt;

&lt;p&gt;All the TD methods discussed above are tabular methods, in which the state and action spaces are small enough for the approximate value functions to be represented as arrays, or tables. These methods can often find exact solutions, that is, they can often find exactly the optimal value function and the optimal policy.&lt;/p&gt;

&lt;p&gt;But they have problems when the state/action space is combinatorial and enormous, like images. The &amp;ldquo;table&amp;rdquo; in tabular methods would be inefficiently enormous and cannot fit into memory. Generalization is needed for such cases, like function approximation. And neural network is a good tool to do this.&lt;/p&gt;

&lt;p&gt;So, if we change Q table into a deep neural network, the algorithm becomes deep Q network (DQN), the core algorithm of Alpha Go and Alpha Zero. And the update is not directly modification anymore, we can use gradient based algorithm to update the network.&lt;/p&gt;

&lt;p&gt;Here is a neural network based Q-learning:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Take some actions, like &amp;ldquo;$\varepsilon$-greedy&amp;rdquo; policy, and get (s, a, s&amp;rsquo;, r)&lt;/li&gt;
&lt;li&gt;$\phi = \phi - \alpha \frac{\nabla Q_{\phi} (s, a)}{\nabla \phi}  (Q_{\phi}(s,a) - r(s, a) - \gamma max_{a&amp;rsquo;} Q_{\phi}(s&amp;rsquo;, a&amp;rsquo;))$&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;But the samples are strongly correlated because the policy only changes little in deep neural network training. We tend to locally overfit the samples. For this, we can use a replay buffer to deal with in off-policy algorithms. This replay buffer is easy to use, quite effective, so widely used. Here is a diagram of replay buffer:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/RL/replaybuffer.png&#34; width=1000&gt;&lt;/p&gt;

&lt;p&gt;We keep a lot of transitions in the buffer and keep updating. Every training batch is a random sample of all transition in the buffer.&lt;/p&gt;

&lt;p&gt;The algorithm becomes:
&lt;img src=&#34;/img/RL/replaybufferalgorithm.png&#34; width=800&gt;&lt;/p&gt;

&lt;p&gt;Actually, the updating step of this algorithm is optimizing over a moving target since we are consistently changing $\phi$ and $max_{a&amp;rsquo;} Q_{\phi}(s&amp;rsquo;, a&amp;rsquo;)$. And we cannot run too much steps in the inner loop because it will overfit. Here is a modification:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/RL/movingtarget.png&#34; width=900&gt;&lt;/p&gt;

&lt;p&gt;$\phi$ is update far less frequently to keep the target still. That&amp;rsquo;s DQN. A more general overview of these algorithms are avaiable &lt;a href=&#34;http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-8.pdf&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;q-learning-with-continuous-action&#34;&gt;Q-learning with continuous action&lt;/h2&gt;

&lt;p&gt;If the action space are continuous, we need a neat method to find the maximum of $Q(s,a)$, since every Q iteration step need to find $argmax_{a&amp;rsquo;} Q_{\phi}(s&amp;rsquo;, a&amp;rsquo;)$. Here are three method:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Optimization methods, like SGD,stochastic optimization. Or easily sample a bunch and choose the best one.&lt;/li&gt;
&lt;li&gt;Functions wasy to maximize, like quadratic funtions.&lt;/li&gt;
&lt;li&gt;Learn an approximate maximizer&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;advanced-policy-gradient&#34;&gt;Advanced policy gradient&lt;/h1&gt;

&lt;p&gt;At this point, it&amp;rsquo;s time to wrap up all previous method and go deeper. Learning from previous tabular methods and basic policy gradient, I will make a comparison between policy gradient and action-value or state-value methods, concrete the mathematical theory of policy gradient and go to more advanced policy gradient methods.&lt;/p&gt;

&lt;p&gt;We have talked about basic policy gradient above. Instead of learning action-value or state-value estimations, policy gradient introduces a parameterized policy that can select actions without consulting a value function. Write the policy as: $\pi(a|s, \theta) = P(A=a|S=s, \theta)$ for the porbability that taking action a at state s with parameterized policy P.&lt;/p&gt;

&lt;p&gt;And the update formula looks like:&lt;/p&gt;

&lt;p&gt;$$\theta_{t+1} = \theta_{t} + \alpha \nabla J(\theta_{t})$$&lt;/p&gt;

&lt;p&gt;Here $J(\theta)$ is optimizing objective, $J(\theta) = v_{\pi_{\theta}}(s_0)$, which is the reward of a trajectory from $s_0$. One thing to note, in order to ensure the exploration, the policy should never be deterministic $\pi(a|s, \theta) \in (0, 1)$. And the parameterize policy can make it easier to deal with continuous action and states.&lt;/p&gt;

&lt;p&gt;One advantage of parameterized policy is that it can be nearly deterministic, while $\varepsilon$-greedy policy always has $\varepsilon$ probability to select a random action. But we should prevent it being too deterministic to keep exploration. For example, in softmax function, we need to control the temperature to make sure.&lt;/p&gt;

&lt;p&gt;Another advantage is the parameterized policy can be stochastic, and it can fit arbitrary probability distributions. In some cases, the optimal policy should be stochastic, like poker games.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Perhaps the simplest advantage that policy parameterization may have over action- value parameterization is that the policy may be a simpler function to approximate. Problems vary in the complexity of their policies and action-value functions.  &amp;ndash; From Sutton&amp;rsquo;s book&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;And also, policy gradient has convergence gaurantee, while $\varepsilon$-greedy selection the action probabilities may change dramatically for an arbitrarily small change in the estimated action values, if that change results in a different action having the maximal value.&lt;/p&gt;

&lt;h3 id=&#34;policy-gradient-theorem&#34;&gt;Policy gradient theorem&lt;/h3&gt;

&lt;p&gt;The objective of policy gradient is $\theta_{t+1} = \theta_{t} + \alpha \nabla J(\theta_{t})$. It&amp;rsquo;s kind of tricky to compute this. The problem is the system performance is related to system dynamic $P(s&amp;rsquo; | s, a)$ as well as policy $P(a|s)$. Given a state, the effect of policy parameters are not only on action selection, but also on the system dynamic, which is irrelevant and usually hard to measure. How can we estimate the reward and get the gradient, regardless of the unknown effect of policy changes on system dynamics?&lt;/p&gt;

&lt;p&gt;Thanks to policy gradient theorem, it provides an explicit, excellent answer to this problem.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/RL/policygradienttheorem.jpg&#34; width=1000&gt;&lt;/p&gt;

&lt;p&gt;After unrolling, we give several terms:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/RL/theoremunrolling.jpg&#34; width=1000&gt;&lt;/p&gt;

&lt;p&gt;We can easily find the first half of every unrolling piece is the trajector probability to get to state certain s. And the second half is a a Q function weighted gradient over all action&amp;rsquo;s gradient. So, sum up the unrolling result as:&lt;/p&gt;

&lt;p&gt;$$\nabla J(\theta) = \nabla_{\theta} v_{\pi_{\theta}} = \sum_s d(s) \sum_a \nabla \pi(a|s)q_{\pi}(s,a)$$&lt;/p&gt;

&lt;p&gt;where $d(s)$ is the trajectory probability. This derivation beautifully removes the derivatives of Q function, making it possible to directly update the policy. Also, updating with respect to the policy parameter does not involve th gradient of state distribution $p(s&amp;rsquo;|s,a)$. All policy gradient algorithms are based on this theorem.&lt;/p&gt;

&lt;p&gt;If changing $\nabla \pi(a|s)$ into $ \pi(a|s) \nabla log\pi(a|s)$, the above formula can be further written as:&lt;/p&gt;

&lt;p&gt;$$\nabla J(\theta) = E_{\pi} [\nabla log \pi(a|s)q_{\pi}(s,a)]$$&lt;/p&gt;

&lt;p&gt;We can use a batch of data to approximate the expection. Here is a general form of policy gradient family &lt;a href=&#34;https://arxiv.org/pdf/1506.02438.pdf&#34; target=&#34;_blank&#34;&gt;image source&lt;/a&gt;：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/RL/generalpolicygradient.jpg&#34; width=900&gt;&lt;/p&gt;

&lt;p&gt;There are a lot of poicy gradient algorithms, besides the REINFORCE algorithm and actor-critic algorithm talked above.&lt;/p&gt;

&lt;h3 id=&#34;off-policy-policy-gradient&#34;&gt;Off-policy policy gradient&lt;/h3&gt;

&lt;p&gt;Usually, policy gradient is on-polic, we optimize the exact policy we used to collect data. But off-policy have several advantages：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;We can take advantage of the replay buffer introduced before to avoid sample correlation and improve sample efficiency.&lt;/li&gt;
&lt;li&gt;Do not need to directly optimize on behavior policy and it can give the model more exploration space.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;But how to make it off-policy? We only need to make a little approximation.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/RL/offpolicygradient.jpg&#34; width=700&gt;&lt;/p&gt;

&lt;p&gt;We need to ignore the second term in the third step. In policy gradient theorem, we unroll the $\nabla_{\theta}Q(s,a)$ term, while here, we just ignore it, since it is usually very hard compute it. As shown in the result, the only difference is an importance sampling weight $\frac{\pi_{\theta}(a|s)}{\pi_{\theta_{old}}(a|s)}$. Although we make a big approximation, the off-policy policy gradient is also gauranteed to converge to the optimal &lt;a href=&#34;https://arxiv.org/pdf/1205.4839.pdf&#34; target=&#34;_blank&#34;&gt;(proof)&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;So, it is totally ok to make policy gradient off-policy, we only need to add a reweighting ratio of the target policy over behavior policy.&lt;/p&gt;

&lt;h3 id=&#34;a3c&#34;&gt;A3C&lt;/h3&gt;

&lt;p&gt;Instead of experience replay, A3C asynchronously execute multiple agents in parallel, on multiple instances of the environment. Each process contains an actor that acts in its own copy of the environment, a separate replay memory, and a learner that samples data, updating shared policy parameters are sent to the actor-learners at fixed intervals.&lt;/p&gt;

&lt;p&gt;Run seperately in different threads can introduce exploration into the system, so A3C does not uses replay buffer in order to use some on-policy algorithms, like Sarsa. Here is A3C &lt;a href=&#34;https://arxiv.org/pdf/1602.01783.pdf&#34; target=&#34;_blank&#34;&gt;image source&lt;/a&gt;:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/RL/A3C.jpg&#34; width=700&gt;&lt;/p&gt;

&lt;p&gt;A3C enables large scale training in different threads. The algorithm computes gradients for n-step updates for each of the state-value pairs encountered since the last update. This accumulated gradient can alao mkae the model more robust.&lt;/p&gt;

&lt;h3 id=&#34;dpg&#34;&gt;DPG&lt;/h3&gt;

&lt;p&gt;The first step of DPG should be building a mathematical foundation: the Deterministic Policy Gradient Theorem.&lt;/p&gt;

&lt;p&gt;Previously, our policy gradient theorem gives:&lt;/p&gt;

&lt;p&gt;$$J(\theta) = \int_\mathcal{S} \rho^\mu(s) Q(s, \mu_\theta(s)) ds$$&lt;/p&gt;

&lt;p&gt;But for deterministic policy gradient:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/RL/dpgt.jpg&#34; width=600&gt;&lt;/p&gt;

&lt;p&gt;By convention $\nabla_\theta \mu_\theta(s)$ is a Jacobian matrix such that each
column is the gradient. In the paper, the authurs claims DPG can be treated as a special case of stochastic policy gradient.
By parametrising stochastic policies with a Gaussian, DPG is equivalent to the case that $\sigma = 0$. Compared to stochastic policy gradient, DPG should be more sample efficiency because it does not need to fit policy over the whole state and action space.&lt;/p&gt;

&lt;p&gt;So one possible DPG algorithm using Sarsa updates is:
&lt;img src=&#34;/img/RL/DPG1.jpg&#34; width=500&gt;&lt;/p&gt;

&lt;p&gt;Note that we cannot just use deterministic policies without any exploration. Usually some noise are needed for DPG training. But in the way, DPG is not deterministic anymore&amp;hellip; This is kind of similar to &amp;ldquo;$\varepsilon$-greedy&amp;rdquo; policy.&lt;/p&gt;

&lt;p&gt;In off-policy cases, it&amp;rsquo;s similar to common off-policy policy gradient.
&lt;img src=&#34;/img/RL/offpolicyDPG.jpg&#34; width=550&gt;&lt;/p&gt;

&lt;p&gt;Basically, we are using TD error to update Q function and use deterministic policy gradient algorithm to update policy. Similarly, it drops a term that depends on $\nabla_{\theta} Q^\mu(s, a)$. And since the policy is deterministic, $Q^\mu(s, \mu_\theta(s)) = \pi_{\theta}(a \vert s) Q^\pi(s, a)$. We don&amp;rsquo;t need to do importance sampling like previous off-policy algorithms.&lt;/p&gt;

&lt;h3 id=&#34;ddpg&#34;&gt;DDPG&lt;/h3&gt;

&lt;p&gt;DDPG is a combination of DPG and DQN. It adopts replay buffer and target network from DQN and extend DQN to continuous action space. Also, it uses delayed update for both policy and critic.&lt;/p&gt;

&lt;p&gt;Since deterministic policy cannot explore around, DDPG inject noise to the deterministic action to ensure exploration:&lt;/p&gt;

&lt;p&gt;$$a_{t}=\mu\left(s_{t} | \theta^{\mu}\right)+\mathcal{N}_{t}$$&lt;/p&gt;

&lt;p&gt;Overall, DDPG is self exploratory:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/RL/DDPG.jpg&#34; width=800&gt;&lt;/p&gt;

&lt;p&gt;Note that when updating critic, we don&amp;rsquo;t want to flow gradient into $y_i$, which is just the target for critic update. So, when calculating $y_i$, tf.stop_gradient() should be used to restrict gradient.&lt;/p&gt;

&lt;h3 id=&#34;td3&#34;&gt;TD3&lt;/h3&gt;

&lt;p&gt;Like discussed before in &lt;a href=&#34;#double-q-learning&#34;&gt;Double Q-learning&lt;/a&gt;, we find Q-learning suffer a lot from its overestimation of Q values. And this bias propagate through Bellman equation and further affect the whole trajectory. &lt;a href=&#34;#double-q-learning&#34;&gt;Double Q-learning&lt;/a&gt; tries to mitigate this bias with decoupled Q functions. The authors of TD3 also observed significant overestimation in DDPG since it also adopts Q network. In order to address this problem, they added a series of tricks into DDPG and created TD3. Actually, there are only two main difference compared to DDPG:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Chipping Double Q-learning. In practice the target network strategy in DQN does not work with actor-critic because of the slow changing policy, and Double Q-learning cannot eliminate the bias. So, they took a kind of extreme way:
$$y_{1}=r+\gamma \min _{i=1,2} Q_{\theta_{i}^{\prime}}\left(s^{\prime}, \pi_{\phi_{1}}\left(s^{\prime}\right)\right)$$
While this update rule may induce
an underestimation bias, this is far preferable to overestimation bias, as unlike overestimated actions, the value of
underestimated actions will not be explicitly propagated
through the policy update.&lt;/li&gt;
&lt;li&gt;Target Policy Smoothing Regularization. This approach enforces
the notion that similar actions should have similar value. They fit
the value of a small area around the target action:
$$y=r+\mathbb{E}_{\epsilon}\left[Q_{\theta^{\prime}}\left(s^{\prime}, \pi_{\phi^{\prime}}\left(s^{\prime}\right)+\epsilon\right)\right]$$
where $\epsilon \sim \operatorname{clip}(\mathcal{N}(0, \sigma),-c, c)$&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;So, here is TD3 overall:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/RL/TD3.jpg&#34; width=500&gt;&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s quite similar to DDPG, with two critic network and a policy smooth regularization.&lt;/p&gt;

&lt;h3 id=&#34;trpo&#34;&gt;TRPO&lt;/h3&gt;

&lt;p&gt;In order to stabilize training process, every step the parameters should not change dramatially. We can add some constrains to the distance between old and new parameters or the gradients, like gradient clipping. Trust region policy optimization introduces &lt;strong&gt;KL divergence&lt;/strong&gt; to set a constrain on the distance of every update.&lt;/p&gt;

&lt;p&gt;But the internel idea of TRPO is a little tricky. In policy gradient, we optimize the policy without considering the change of state change. We still assume the state is $s_0, s_1, s_2, &amp;hellip; $ and optimize the policy to maximize reward/advantage based on this series of states. But actually, states in every time step might change with the change of policy. What we should do is to optimize based on the new, optimized policy state distribution. Apparently, this is not easy to do.&lt;/p&gt;

&lt;p&gt;This property is very clear via some derivation &lt;a href=&#34;http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-9.pdf&#34; target=&#34;_blank&#34;&gt;(source)&lt;/a&gt;:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/RL/policyiteration.jpg&#34; width=1000&gt;&lt;/p&gt;

&lt;p&gt;Here. $p_{\theta&amp;rsquo;}$ is new policy $p_{\theta}$ is old policy. Decompose $p_{\theta&amp;rsquo;}(\tau)$, we get:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/RL/policyiteration1.jpg&#34; width=800&gt;&lt;/p&gt;

&lt;p&gt;We can use importance sampling to obtain an off-policy fashion to optimize $\pi_{\theta}(a_t|s_t)$ accurately, but we still cannot deal with the new state distribution $p_{\theta&amp;rsquo;}$.&lt;/p&gt;

&lt;p&gt;TRPO claims if new policy and old policy are close enough, we can assume these two states in every time step will also be close enough.&lt;/p&gt;

&lt;p&gt;And we can prove if $|\pi_{\theta&amp;rsquo;}(a_t|s_t) - \pi_{\theta}(a_t|s_t)| &amp;lt;= \epsilon$ for every time step, then $|p_{\theta&amp;rsquo;} - p_{\theta}| &amp;lt;= 2\epsilon t$&lt;/p&gt;

&lt;p&gt;TRPO introduces a more efficient constrain on policy distribution change, $$D_{KL}(\pi_{\theta&amp;rsquo;}(a_t|s_t) || \pi_{\theta}(a_t|s_t)) &amp;lt;= \epsilon $$&lt;/p&gt;

&lt;p&gt;Overall, TRPO is like:
&lt;img src=&#34;/img/RL/TRPO.jpg&#34; width=700&gt;&lt;/p&gt;

&lt;p&gt;In practice, we can use duel gradient ascent to solve this constrained problem.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Maximize the Lagrangian multiplier: &lt;img src=&#34;/img/RL/lagrange.jpg&#34; width=1000&gt;&lt;/li&gt;
&lt;li&gt;Optimize $\lambda$ if the constrain is violated. $\lambda = \lambda + \alpha (D_{KL}(\pi_{\theta&amp;rsquo;}(a_t|s_t) || \pi_{\theta}(a_t|s_t)) - \epsilon)$&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;ppo&#34;&gt;PPO&lt;/h3&gt;

&lt;p&gt;PPO is based on the same idea of TRPO. But compared to TRPO, it is less complicated and compatible with architectures that include noise (such as dropout) or parameter sharing
(between the policy and value function, or with auxiliary tasks).&lt;/p&gt;

&lt;p&gt;PPO propose a novel objective with clipped probability ratios, which forms a pessimistic estimate
(i.e., lower bound) of the performance of the policy. Here is the TRPO objective [(image source)[&lt;a href=&#34;https://arxiv.org/pdf/1707.06347.pdf]]:&#34; target=&#34;_blank&#34;&gt;https://arxiv.org/pdf/1707.06347.pdf]]:&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/RL/PPO.jpg&#34; width=600&gt;&lt;/p&gt;

&lt;p&gt;If one directly optimizes this objective, it might lead a big change to $r_t (\theta)$, which is not what we want. That&amp;rsquo;s why there is a KL divergence constrain on policy change. While PPO have a different objective:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/RL/PPO1.jpg&#34; width=700&gt;&lt;/p&gt;

&lt;p&gt;It clips $r_t (\theta)$ into a small range, $[1-\epsilon, 1+\epsilon]$, which removes the incentive for moving $r_t$ outside of the
interval. And also takes a minimum of the clipped and unclipped objective, so the
final objective is a &lt;strong&gt;lower bound&lt;/strong&gt; (i.e., a pessimistic bound) on the unclipped objective. PPO just want to be very &lt;strong&gt;conservative&lt;/strong&gt; to optimize the policy to some behaviors with a big reward, while for some known bad behaviors, PPO does not put any constrain on it.&lt;/p&gt;

&lt;p&gt;If using a neural network architecture that shares parameters
between the policy and value function, we must use a loss function that combines the policy
change constrain and a value function error term.&lt;/p&gt;

&lt;p&gt;$$L_{t}(\theta) = L_{t}^{CLIP}(\theta) - c_1 (V_t^{\theta} - V_t^{target})^2 + c_2 EN[\pi_{\theta}]$$&lt;/p&gt;

&lt;p&gt;We need to include the value function approximation loss, as well as the third policy entropy bonus term, to ensure enough exploration to the algorithm.&lt;/p&gt;

&lt;p&gt;Currently, PPO should be a default algorithm policy gradient algorithm because of its simplicity and effectiveness.&lt;/p&gt;

&lt;h3 id=&#34;acer&#34;&gt;ACER&lt;/h3&gt;

&lt;p&gt;off-policy methods can greatly increase the sample efficient and decrease the correlation in training. A3C works great, but it&amp;rsquo;s on-policy. ACER is the off-policy version of A3C. It&amp;rsquo;s not as easy as it sounds, ACER takes a lot of effort to keep the training stable with off-policy data. Overall, it uses three major tricks to make it work:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Multi-step estimation of the state-action value function using Retrace.&lt;/li&gt;
&lt;li&gt;Importance weight truncation with bias correction&lt;/li&gt;
&lt;li&gt;Efficient TPRO&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In off-policy setting, Importance Sampling (IS) is commonly used to compensate the policy difference for different trajectories. Q function update can be generalized like this:&lt;/p&gt;

&lt;p&gt;$$ Q(x, a) :=Q(x, a)+\mathbb{E}_{\mu}\left[\sum_{t \geq 0} \gamma^{t}\left(\prod_{s=1}^{t} c_{s}\right)\left(r_{t}+\gamma \mathbb{E}_{\pi} Q\left(x_{t+1}, \cdot\right)-Q\left(x_{t}, a_{t}\right)\right)\right]$$&lt;/p&gt;

&lt;p&gt;For importance sampling, $\mathcal{C}_{\mathcal{S}}=\frac{\pi\left(a_{s} | x_{s}\right)}{\mu\left(a_{s} | x_{s}\right)}$. . It is well known that IS estimates can suffer from large – even possibly infinite – variance, like scenarios $\mu\left(a_{s} | x_{s}\right)$ is very small.&lt;/p&gt;

&lt;p&gt;In Retrace($\lambda$), $c_{s}=\lambda \min \left(1, \frac{\pi\left(a_{s} | x_{s}\right)}{\mu\left(a_{s} | x_{s}\right)}\right)$. Retrace(λ) uses an importance sampling ratio truncated
at 1. Compared to IS, it does not suffer from the variance explosion of the product of IS ratio.&lt;/p&gt;

&lt;p&gt;ACER interprete Retrace in a recursive fashion. Rearrange terms from above Retrace equation:&lt;/p&gt;

&lt;p&gt;$$Q^{\mathrm{ret}}\left(x_{t}, a_{t}\right)=r_{t}+\gamma \overline{\rho}_{t+1}\left[Q^{\mathrm{ret}}\left(x_{t+1}, a_{t+1}\right)-Q\left(x_{t+1}, a_{t+1}\right)\right]+\gamma V\left(x_{t+1}\right)$$&lt;/p&gt;

&lt;p&gt;where $\overline{\rho} = \lambda \min \left(1, \frac{\pi\left(a_{s} | x_{s}\right)}{\mu\left(a_{s} | x_{s}\right)}\right)$ and $V(x)=\mathbb{E}_{a \sim \pi} Q(x, a)$. After recursively calculate $Q^{ret}(s,a)$ over older Q functions, ACER uses $Q^{ret}(s,a)$ to fit Q network with MSE. And note that ACER uses two head network to calcuate policy and Q values jointly.&lt;/p&gt;

&lt;p&gt;After a hard clipping in Retrace, the estimator becomes biased, so ACER introduces a correction term for this.&lt;/p&gt;

&lt;p&gt;$$\widehat{g}_{t}^{\text {acer }}=\overline{\rho}_{t} \nabla_{\theta} \log \pi_{\theta}\left(a_{t} | x_{t}\right)\left[Q^{\text {ret }}\left(x_{t}, a_{t}\right)-V_{\theta_{v}}\left(x_{t}\right)\right] + \underset{a \sim \pi}{\mathbb{E}}\left(\left[\frac{\rho_{t}(a)-c}{\rho_{t}(a)}\right]_{+} \nabla_{\theta} \log \pi_{\theta}\left(a | x_{t}\right)\left[Q_{\theta_{v}}\left(x_{t}, a\right)-V_{\theta_{v}}\left(x_{t}\right)\right]\right)$$&lt;/p&gt;

&lt;p&gt;where $\overline{\rho} = \lambda \min \left(1, \rho\right)$ and $\rho_{t}=\frac{\pi\left(a_{t} | x_{t}\right)}{\mu\left(a_{t} | x_{t}\right)}$ as before. The second term is the correction to make it unbiased.&lt;/p&gt;

&lt;p&gt;ACER is also not satisfied with the effectiveness of TPRO because of its Fisher-vector products for each update. Instead of putting complex constrains to policy update, ACER propose to maintain an average policy network that represents a running average of past policies
and forces the updated policy to not deviate far from this average.&lt;/p&gt;

&lt;p&gt;The three tricks seems straight forward, but they are not. The authors really took effort to realize off-policy and the ACER paper is quite dense. It&amp;rsquo;s not easy at all to bridge on-policy and off-policy RL algorithms.&lt;/p&gt;

&lt;h3 id=&#34;soft-actor-critic&#34;&gt;Soft Actor-critic&lt;/h3&gt;

&lt;p&gt;Soft actor-critic is based on the maximum entropy reinforcement learning framework, which considers the entropy augmented term inside its objective. Previously, common RL algorithm have this objective:&lt;/p&gt;

&lt;p&gt;$$\pi^* = \arg\max_{\pi} \mathbb{E}_{\pi}\left[ \sum_{t=0}^T r_t \right]$$&lt;/p&gt;

&lt;p&gt;Only consider future reward sum with current policy. But the objective with maximum entropy also optimize the entropy of policy, it goes like this:&lt;/p&gt;

&lt;p&gt;$$ \pi_{\mathrm{MaxEnt}}^* = \arg\max_{\pi} \mathbb{E}_{\pi}\left[ \sum_{t=0}^T r_t + \mathcal{H}(\pi(\cdot | \mathbf{s}_t)) \right] $$&lt;/p&gt;

&lt;p&gt;In this way, out policy not only optimize future reward, but also want to achieve higher entropy, in another words, more diversity. It will automatically make the policy distribution &amp;ldquo;wider&amp;rdquo; to ensure exploration wisely, rather than common some random exploration. A blog from UC berkeley explains this clearly: &lt;a href=&#34;https://bair.berkeley.edu/blog/2017/10/06/soft-q-learning/&#34; target=&#34;_blank&#34;&gt;Learning Diverse Skills via Maximum Entropy Deep Reinforcement Learning
&lt;/a&gt;:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Note that this objective differs qualitatively from the behavior of Boltzmann exploration (Sallans &amp;amp; Hinton, 2004) and PGQ (O’Donoghue et al., 2016),
which greedily maximize entropy at the current time step,
but do not explicitly optimize for policies that aim to reach
states where they will have high entropy in the future.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Different with previous methods, this objective tends to maximize the entropy of the entire trajectory distribution for the policy, rather than greedily maximize current policy.&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;Common RL objective&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;maximum entropy RL&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;/img/unimodal-policy.png&#34; alt=&#34;&#34; /&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;/img/multimodal_policy.png&#34; alt=&#34;&#34; /&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;With maximum entropy objective, we can explore more widely in a multimodal setting and ensure the agent explores all promising states while prioritizing the more promising ones. This kind of exploration can be very useful for transfer learning as well. If we formalize this idea by defining the policy directly in terms of exponentiated Q-value:&lt;/p&gt;

&lt;p&gt;$$\pi(\mathbf{a}|\mathbf{s}) \propto \exp Q(\mathbf{s}, \mathbf{a})$$&lt;/p&gt;

&lt;p&gt;This is &lt;a href=&#34;https://arxiv.org/pdf/1702.08165.pdf&#34; target=&#34;_blank&#34;&gt;Soft Q-learning&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;And Soft Actor-critic, the authors adopted this &amp;ldquo;soft&amp;rdquo; function to ensure enough exploration. Overall, soft actor critic has three main advantages: 1. Actor Critic; 2. Off policy; 3. Better exploration with soft function.&lt;/p&gt;

&lt;p&gt;For &lt;a href=&#34;https://arxiv.org/pdf/1801.01290.pdf&#34; target=&#34;_blank&#34;&gt;soft actor-critic&lt;/a&gt;,we will derive soft policy iteration. Very similar to common policy iteration, only need to replace value function with soft value function:&lt;/p&gt;

&lt;p&gt;Note the actor will sample actions from policy, so the entropy term can be written as $E_{a_t \sim \pi}[- log \pi(a_t|s_t)]$. In this way, the soft value function formula is:&lt;/p&gt;

&lt;p&gt;$$V(s_t) = E_{a_t \sim \pi}[Q(s_t, a_t) - log \pi(a_t|s_t)]$$&lt;/p&gt;

&lt;p&gt;And also, the update of policy uses KL divergence:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/RL/softpolicyupdate.jpg&#34; width=500&gt;&lt;/p&gt;

&lt;p&gt;This $Z^{\pi_{old}}(s_t)$ is just a normalizer, with nothing to do with gradient. Soft policy iteration works fine in tabular cases because the Q-values can be calculated exactly. But for continuous domains, we have to use approximator to represent Q-values, which make the optimization computationally too expensive (make one neural network close to exponential of another). This gives rise to Soft Actor Critic.&lt;/p&gt;

&lt;p&gt;In soft actor critic, we will use function approximator to for both Q-function and policy. The soft Q-function and soft state value can be modeled as expressive neural networks, and the policy as
a Gaussian with mean and covariance, or any parameterized family of distributions, given by neural network (reparameterization trick).&lt;/p&gt;

&lt;p&gt;First soft value function:
&lt;img src=&#34;/img/RL/softvaluefunction.jpg&#34; width=600&gt;&lt;/p&gt;

&lt;p&gt;Second, Q-function, same as before, use soft Bellman residual to approximate:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/RL/softbellmanresidual.jpg&#34; width=700&gt;&lt;/p&gt;

&lt;p&gt;Finally, policy update, minimizing KL divengence, updating the policy towards the exponential of the new Q-function:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/RL/softpolicyupdate2.jpg&#34; width=700&gt;&lt;/p&gt;

&lt;p&gt;The target
density is the Q-function, which is represented by a neural
network an can be differentiated. To that end, we reparameterize
the policy using a neural network transformation:&lt;/p&gt;

&lt;p&gt;$$\mathbf{a}_{t}=f_{\phi}\left(\epsilon_{t} ; \mathbf{s}_{t}\right)$$&lt;/p&gt;

&lt;p&gt;Rewrite the policy update objective:&lt;/p&gt;

&lt;p&gt;$$J_{\pi}(\phi)=\mathbb{E}_{\mathbf{s}_{t} \sim \mathcal{D}, \epsilon_{t} \sim \mathcal{N}}\left[\log \pi_{\phi}\left(f_{\phi}\left(\epsilon_{t} ; \mathbf{s}_{t}\right) | \mathbf{s}_{t}\right)-Q_{\theta}\left(\mathbf{s}_{t}, f_{\phi}\left(\epsilon_{t} ; \mathbf{s}_{t}\right)\right)\right]$$&lt;/p&gt;

&lt;p&gt;The gradient is obvious:&lt;/p&gt;

&lt;p&gt;$$\hat{\nabla}_{\phi} J_{\pi}(\phi)=\nabla_{\phi} \log \pi_{\phi}\left(\mathbf{a}_{t} | \mathbf{s}_{t}\right) +\left(\nabla_{\mathbf{a}_{t}} \log \pi_{\phi}\left(\mathbf{a}_{t} | \mathbf{s}_{t}\right)-\nabla_{\mathbf{a}_{t}} Q\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right) \nabla_{\phi} f_{\phi}\left(\epsilon_{t} ; \mathbf{s}_{t}\right)$$&lt;/p&gt;

&lt;p&gt;With reparameterization trick applied, we can estimate policy for any tractable stochastic policy. It also extend the policy update style of DDPG, which only works for deterministic policy.&lt;/p&gt;

&lt;p&gt;Alternatively, we can use REINFORCE style policy update:&lt;/p&gt;

&lt;p&gt;$$ J_{\pi}(\phi)=\mathbb{E}_{\mathbf{s} \sim \mathcal{D}}\left[\mathbb{E}_{\mathbf{a} \sim \pi_{\phi}(\mathbf{a} | \mathbf{s})}\left[ \left(\color{red} {\alpha \log \pi_{\phi}(\mathbf{a} | \mathbf{s})-Q_{\theta}(\mathbf{s}, \mathbf{a})}+b(\mathbf{s})\right) | \mathbf{s}\right]\right]$$&lt;/p&gt;

&lt;p&gt;$b(s)$ is baseline. The red part is the new objective for REINFORCE to minimize KL divergence, replacing the role of rewards in original policy gradient method. The gradient is:&lt;/p&gt;

&lt;p&gt;$$\nabla_{\phi} J_{\pi}(\phi)=\mathbb{E}_{\mathbf{s} \sim \mathcal{D}}\left[\mathbb{E}_{\mathbf{a} \sim \pi_{\phi}(\mathbf{a} | \mathbf{s})}\left[\nabla_{\phi} \log \pi(\mathbf{a} | \mathbf{s})\left&amp;lt;\alpha \log \pi_{\phi}(\mathbf{a} | \mathbf{s})-Q_{\theta}(\mathbf{s}, \mathbf{a})+b(\mathbf{s})\right&amp;gt; | \mathbf{s}\right]\right]$$&lt;/p&gt;

&lt;p&gt;Note that gradient should not flow inside the $&amp;lt; &amp;gt;$ notation because the inside policy is just used for sample action (tf.stop_gradient()).&lt;/p&gt;

&lt;p&gt;After formulized all loss functions above, Soft actor critic is straight forward. Here is soft actor critic:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/RL/softactorcritic.jpg&#34; width=500&gt;&lt;/p&gt;

&lt;h1 id=&#34;sum-up&#34;&gt;Sum up&lt;/h1&gt;

&lt;h1 id=&#34;reference&#34;&gt;Reference&lt;/h1&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://see.stanford.edu/materials/aimlcs229/cs229-notes12.pdf&#34; target=&#34;_blank&#34;&gt;Reinforcement learning lecture of CS229 by Andew Ng&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html&#34; target=&#34;_blank&#34;&gt;David Sliver&amp;rsquo;s RL lecture at UCL&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://rail.eecs.berkeley.edu/deeprlcourse/&#34; target=&#34;_blank&#34;&gt;Sergey Levine&amp;rsquo;s RL course at UC Berkeley&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://drive.google.com/file/d/1opPSz5AZ_kVa1uWOdOiveNiBFiEOHjkG/view&#34; target=&#34;_blank&#34;&gt;Sutton&amp;rsquo;s RL book&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1506.02438.pdf&#34; target=&#34;_blank&#34;&gt;Generalized Advantage Estimation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1707.06347&#34; target=&#34;_blank&#34;&gt;Proximal Policy Optimization Algorithms&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://proceedings.mlr.press/v32/silver14.pdf&#34; target=&#34;_blank&#34;&gt;Deterministic Policy Gradient (DPG)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1702.08165&#34; target=&#34;_blank&#34;&gt;Reinforcement Learning with Deep Energy-Based Policies&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1801.01290&#34; target=&#34;_blank&#34;&gt;Soft Actor-critic&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Neural Network based Object Recognition: Fast/Faster/Mask R-CNN, SSD, YOLO and RetinaNet</title>
      <link>/post/neural-network-based-object-recognition-fast-faster-mask-r-cnn-ssd-and-yolo/</link>
      <pubDate>Wed, 01 Aug 2018 21:00:00 -0500</pubDate>
      
      <guid>/post/neural-network-based-object-recognition-fast-faster-mask-r-cnn-ssd-and-yolo/</guid>
      <description>

&lt;h2 id=&#34;table-of-content&#34;&gt;Table of Content&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#history&#34;&gt;History&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#alexnet&#34;&gt;AlexNet&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#resnet&#34;&gt;ResNet&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#inception-network-series&#34;&gt;Inception Network Series&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#deformable-parts-model&#34;&gt;Deformable Parts Model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#overfeat&#34;&gt;Overfeat&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#metrics--map&#34;&gt;Metrics: mAP&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#precision-and-recall&#34;&gt;Precision and Recall&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#average-precision&#34;&gt;Average precision&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#mean-average-precision&#34;&gt;Mean Average Precision&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#r-cnn-families&#34;&gt;R-CNN Families&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#rcnn&#34;&gt;RCNN&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#selective-search&#34;&gt;Selective Search&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#bounding-box-regression&#34;&gt;Bounding Box Regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#non-maximum-suppression&#34;&gt;Non Maximum Suppression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#workflow&#34;&gt;Workflow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#speed-bottleneck&#34;&gt;Speed Bottleneck&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#fast-rcnn&#34;&gt;Fast-RCNN&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#roi-pooling&#34;&gt;ROI pooling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#workflow-1&#34;&gt;Workflow&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#faster-rcnn&#34;&gt;Faster-RCNN&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#region-proposal-network&#34;&gt;Region Proposal Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#workflow-2&#34;&gt;Workflow&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#mask-rcnn&#34;&gt;Mask-RCNN&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#roi-align&#34;&gt;ROI Align&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#key-point-detection&#34;&gt;Key Point Detection&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#single-shot-detector&#34;&gt;Single Shot Detector&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#single-shot-multibox-detector&#34;&gt;Single Shot MultiBox Detector&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#architecture&#34;&gt;Architecture&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#implementation-details&#34;&gt;Implementation Details&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#yolov1&#34;&gt;YOLOv1&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#architecture-1&#34;&gt;Architecture&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#loss-function&#34;&gt;Loss function&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#workflow-3&#34;&gt;Workflow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#limitation&#34;&gt;Limitation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#yolov2&#34;&gt;YOLOv2&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#improvements&#34;&gt;Improvements&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#yolov3&#34;&gt;YOLOv3&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#feature-pyramid-network--fpn-&#34;&gt;Feature Pyramid Network (FPN)&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#architecture-2&#34;&gt;Architecture&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#feature-pyramid-networks-for-rpn&#34;&gt;Feature Pyramid Networks for RPN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#feature-pyramid-networks-for-faster-mask-r-cnn&#34;&gt;Feature Pyramid Networks for Faster/Mask R-CNN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#experimental-result&#34;&gt;Experimental Result&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#focal-loss-and-retinanet&#34;&gt;Focal Loss and RetinaNet&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#class-imbalance&#34;&gt;Class Imbalance&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#retinanet&#34;&gt;RetinaNet&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#implementation-details-1&#34;&gt;Implementation Details&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#results&#34;&gt;Results&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#summary&#34;&gt;Summary&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#reference-materials&#34;&gt;Reference Materials&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It&amp;rsquo;s well known that deep learning has been a real game changer in machine learning, especially in computer vision. Similar to image classification, deep learning/neural network represent the state of the art in modern object recognition.&lt;/p&gt;

&lt;h1 id=&#34;history&#34;&gt;History&lt;/h1&gt;

&lt;p&gt;To have a better intuition about the challenges and algorithm evolving, first we will have an overview about the progress of deep learning approach in the last couple of years. This progress also comes with the progress of image classification.&lt;/p&gt;

&lt;h3 id=&#34;alexnet&#34;&gt;AlexNet&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf&#34; target=&#34;_blank&#34;&gt;AlexNet&lt;/a&gt; famously won the 2012 ImageNet LSVRC-2012 competition by a large margin (15.3% VS 26.2% (second place) error rates). It stands for the revival of neural network as well as deep learning.&lt;/p&gt;

&lt;p&gt;The main contribution of AlexNet is as follows:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Use ReLU activation function instead of tanh and sigmoid to introduce nonlinearity to the network. Also, it eases the gradient vanishing problem when training deep neural network.&lt;/li&gt;
&lt;li&gt;Introduce dropout as a regularization. It works like an ensemble of different networks.&lt;/li&gt;
&lt;li&gt;Use two NVIDIA GPUs and CUDA, also parallel architecture to accelerate training.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Here is a diagram of famous AlexNet:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/AlexNet.png&#34; alt=&#34;AlexNet&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;resnet&#34;&gt;ResNet&lt;/h3&gt;

&lt;p&gt;After AlexNet, the next significant work should be &lt;a href=&#34;https://arxiv.org/abs/1512.03385&#34; target=&#34;_blank&#34;&gt;ResNet&lt;/a&gt;.&lt;br /&gt;
These networks led to 1st-place winning entries in all five main tracks of the ImageNet and COCO 2015 competitions, which covered image classification, object detection, and semantic segmentation.&lt;/p&gt;

&lt;p&gt;The main difference of ResNet is just a skip connection between layers. The author’s hypothesis is that it is easy to optimize the residual mapping function F(x) than to optimize the original, unreferenced mapping H(x). And this method turns out to be very efficient to solve network degradation in deep network.&lt;/p&gt;

&lt;p&gt;Here is a diagram of residual blocks (image source: ):&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/residualblock.png&#34; alt=&#34;residual&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Plus, Resnet uses the bottleneck architecture from Inception network to reduce computation. After $1\times1$ convolution, the feature map channels are greatly reduced in order to save computation for next $3 \times 3$ convolution layer. Another $1\times1$ convolution after that can resume the feature map channels. Amazingly, this operation won&amp;rsquo;t affect the performance. So, this bottleneck architecture is quite popular in modern neural network.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/bottleneck.jpg&#34; alt=&#34;bottleneck&#34; /&gt;&lt;/p&gt;

&lt;p&gt;ResNet can be interpreted as ensembles of relatively shallow networks, as described &lt;a href=&#34;https://arxiv.org/abs/1605.06431?context=cs&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;inception-network-series&#34;&gt;Inception Network Series&lt;/h3&gt;

&lt;p&gt;Inception network series comes from Google. It should be the most well-designed and popular network in image classification community. The architecture becomes very complicated in later version of Inception networks. For example, here is a diagram of InceptionResnetV2, with more than 600 layers:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/image00.png&#34; alt=&#34;inceptionresnetv2&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The core of Inception network is the Inception module. The inspiration comes from the idea that you need to make a decision as to what type of convolution you want to make at each layer: $3 \times 3$ or $5 \times 5$? You may need specific image content to decide. How about use them all together? Here come the Inception module:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/inception_implement.png&#34; alt=&#34;Inceptionmodule&#34; /&gt;&lt;/p&gt;

&lt;p&gt;It uses all $1 \times 1$, $3 \times 3$, $5 \times 5$ convolution and  altogether and concate their result together. Plus, inception module introduces $N \times 1$ and $1 \times N$ convolution instead of traditional $N \times N $ convolution. This puts constraints on convolution kernel (center symmetric), but can greatly reduces computation. And residual connection, bottleneck architecture also appears in modern Inception network. Here is a diagram of Inception modules in InceptionV4:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/inceptionv4.jpeg&#34; alt=&#34;v4&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;deformable-parts-model&#34;&gt;Deformable Parts Model&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;http://vision.stanford.edu/teaching/cs231b_spring1213/slides/dpm-slides-ross-girshick.pdf&#34; target=&#34;_blank&#34;&gt;Deformable Parts Model&lt;/a&gt; was invented by Pedro Felzenszwalb ands Ross Girshick, who becomes the leader of object recognition commumity with his R-CNN families.&lt;/p&gt;

&lt;p&gt;The model consists of three major components:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;A coarse root filter defines a detection window that approximately covers an entire object. A filter specifies weights for a region feature vector.&lt;/li&gt;
&lt;li&gt;Multiple part filters that cover smaller parts of the object. Parts filters are learned at twice resolution of the root filter.&lt;/li&gt;
&lt;li&gt;A spatial model for scoring the locations of part filters relative to the root.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Deformable Part Models model an object as the sum of the geometric deformations of it. So take person model for example:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/DPM.jpeg&#34; alt=&#34;DPM&#34; /&gt;&lt;/p&gt;

&lt;p&gt;A root location with high score detects a region with high chances to contain an object, while the locations of the parts with high scores confirm a recognized object hypothesis. The paper adopted latent SVM to model the classifier.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/DPMmodel.png&#34; alt=&#34;DPMmodel&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Also, DPM and later CNN models are not two different method to object recognition. Actually, DPM can be unrolled and interpreted as several equivalent CNN layers. See Ross&amp;rsquo;s paper &lt;a href=&#34;https://arxiv.org/abs/1409.5403&#34; target=&#34;_blank&#34;&gt;Deformable Part Models are Convolutional Neural Networks&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;overfeat&#34;&gt;Overfeat&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1312.6229&#34; target=&#34;_blank&#34;&gt;Overfeat&lt;/a&gt; is a poineer to incorprate CNN models into object detection, localization and classification tasks.&lt;/p&gt;

&lt;h1 id=&#34;metrics-map&#34;&gt;Metrics: mAP&lt;/h1&gt;

&lt;p&gt;mAP is the metric to measure the accuracy of object detectors like Faster R-CNN, SSD, etc. AP is the average of the maximum precisions at different recall values. And mAP is the mean of AP in different classes. This sounds complicated, so let&amp;rsquo;s explain this step by step.&lt;/p&gt;

&lt;h3 id=&#34;precision-and-recall&#34;&gt;Precision and Recall&lt;/h3&gt;

&lt;p&gt;According to &lt;a href=&#34;https://en.wikipedia.org/wiki/Precision_and_recall#F-measure&#34; target=&#34;_blank&#34;&gt;wiki&lt;/a&gt;, &lt;strong&gt;precision&lt;/strong&gt; (also called positive predictive value) is the fraction of relevant instances among the retrieved instances, for example, percentage of your positive predictions are correct.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Recall&lt;/strong&gt; (also known as sensitivity) is the fraction of relevant instances that have been retrieved over the total amount of relevant instances. for example, percentage of positive instances in your positive predictions.&lt;/p&gt;

&lt;p&gt;Here is an image of precision and recall:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/precision-recall.jpg&#34; alt=&#34;precision and recall&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;average-precision&#34;&gt;Average precision&lt;/h3&gt;

&lt;p&gt;First of all, a prediction is consider to be correct if IoU is greater than a threshold, usually 0.5. It is hard to explain AP by words, so let&amp;rsquo;s create an example to make it clear. (&lt;a href=&#34;https://medium.com/@jonathan_hui/map-mean-average-precision-for-object-detection-45c121a31173&#34; target=&#34;_blank&#34;&gt;example source&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;For example, if our image has 5 objects. First, we sort the predictions of our detector based on the confidence level. We get correct prediction in 1st, 2nd, 4th, 6th and 10th prediction. So, in the top 4 predictions, we get 3 of them correct. A prediction is consider to be correct if IoU is greater than 0.5. So the precision and the recall for our top 4 predictions is:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Precision&lt;/strong&gt; is the proportion of TP = &lt;sup&gt;3&lt;/sup&gt;&amp;frasl;&lt;sub&gt;4&lt;/sub&gt; = 0.75.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Recall&lt;/strong&gt; is the proportion of TP out of the possible positives = &lt;sup&gt;3&lt;/sup&gt;&amp;frasl;&lt;sub&gt;5&lt;/sub&gt; = 0.6.&lt;/p&gt;

&lt;p&gt;Here is a table of different corresponding precision and recall ranks:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/precisionrecallrank.png&#34; alt=&#34;prrank&#34; /&gt;&lt;/p&gt;

&lt;p&gt;AP (average precision) is computed as the average of &lt;strong&gt;maximum precision&lt;/strong&gt; at different recall levels. Here are two kinds of AP calculation:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Before 2010, people get the precision value at 11 recall levels (0.0, 0.1, 0.2 &amp;hellip; 1.0), and average them.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;$$AP = \frac{1}{11} (AP_r(0.0) + AP_r(0.1) + &amp;hellip; AP_r(1.0))$$&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;After 2010, people calculate the precision value at all different recall levels and average them. Here K is the number of total different recall levels.&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;$$AP = \sum_{i = 0}^K AP_r(i)$$&lt;/p&gt;

&lt;p&gt;The $AP_r$ represent the &lt;strong&gt;maximum precision&lt;/strong&gt; in certain recall level.&lt;/p&gt;

&lt;h3 id=&#34;mean-average-precision&#34;&gt;Mean Average Precision&lt;/h3&gt;

&lt;p&gt;Latest research papers tend to give results for the COCO dataset only. For COCO, AP is the average over multiple IoU. &lt;strong&gt;AP@[.5:.95]&lt;/strong&gt; corresponds to the average AP for IoU from 0.5 to 0.95 with a step size of 0.05. AP (which is also called mAP in COCO) averages AP over all class categories.&lt;/p&gt;

&lt;h1 id=&#34;r-cnn-families&#34;&gt;R-CNN Families&lt;/h1&gt;

&lt;h2 id=&#34;rcnn&#34;&gt;RCNN&lt;/h2&gt;

&lt;p&gt;The goal of &lt;a href=&#34;https://arxiv.org/abs/1311.2524&#34; target=&#34;_blank&#34;&gt;R-CNN&lt;/a&gt; is to take in an image, and correctly identify where are objects (via a bounding box) in the image. The intuition of RCNN is quite simple, just proposing some possible regions, and determine whether they are objects or not. Here is a diagram of RCNN:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/RCNN.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;selective-search&#34;&gt;Selective Search&lt;/h3&gt;

&lt;p&gt;R-CNN uses Selective Search method to extract region proposals.&lt;/p&gt;

&lt;p&gt;This method uses an over-segmentation method to divide the image into small regions (1k to 2k). Merge the two most probable adjacent areas according to the consolidation rules. This is is a hierarchical grouping algorithm by J.R.R. Uijlings.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/grouping.png&#34; alt=&#34;grouping&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Actually the similarity of regions is determined by their color histogram, texture(gradient histogram) and some other aspects. Note that this method tends to merge relatively small regions to avoid a large area gradually &amp;ldquo;eat&amp;rdquo; other small areas. Also, the method tend to merge the regions whose merged result takes large percentage in its bounding box.&lt;/p&gt;

&lt;p&gt;Repeat until the entire image merges into one area position. Output all areas that existed once, so-called candidate areas. And generate bounding box for them as region proposals.&lt;/p&gt;

&lt;h3 id=&#34;bounding-box-regression&#34;&gt;Bounding Box Regression&lt;/h3&gt;

&lt;p&gt;The Bounding Box Regressors are essential because the initial region proposals might not fully coincide with the region that is indicated by the learned features of the Convolutional Neural Network. It is a kind of a refinement step.&lt;/p&gt;

&lt;p&gt;So, based on the features obtained at the end of the final pooling layer, the region proposals are regressed. Note that only when region proposals have large IoU(&amp;gt;0.6) with ground truth, we can treat this transformation as linear model and do regression. The bounding box regression process is quite straight forward in RCNN paper.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/bbregression.png&#34; alt=&#34;bbregression&#34; /&gt;&lt;/p&gt;

&lt;p&gt;We directly regress the location $P_x$ and $P_y$, while regress the width $P_w$ and height $P_h$ in log-space which produce better results on the wide range of bounding-box shift.&lt;/p&gt;

&lt;h3 id=&#34;non-maximum-suppression&#34;&gt;Non Maximum Suppression&lt;/h3&gt;

&lt;p&gt;In the end, we can have a lot of bounding boxes on an object. We need to determine the best bounding box for this object. So, what NMS does is as follows:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;First calculate the area of each bounding box, and then sort according to the score&lt;/li&gt;
&lt;li&gt;Use the largest bounding box with the score as the selected box, calculate the rest of the bounding box and the current maximum score and the IoU of the box, remove the IoU greater than the set threshold Bounding box.&lt;/li&gt;
&lt;li&gt;Then repeat the above process until the candidate bounding box is empty&lt;/li&gt;
&lt;li&gt;Delete the selected box with the score less than a certain threshold to get this kind of result&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&#34;/img/NMS.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;workflow&#34;&gt;Workflow&lt;/h3&gt;

&lt;p&gt;R-CNN is formulated as follows:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Apply &lt;a href=&#34;#selective-search&#34;&gt;selective search&lt;/a&gt; to input image, and generate ~2k candidates per image containing target objects with different sizes. Region candidates are warped to have a fixed size as required by CNN.&lt;/li&gt;
&lt;li&gt;Start from a pre-train a CNN network on image classification tasks. Continue fine-tuning the CNN on warped proposal regions for K + 1 classes; The additional one class refers to the background (no object of interest). In the fine-tuning stage, we should use a much smaller learning rate and the mini-batch oversamples the positive cases because most proposed regions are just background.&lt;/li&gt;
&lt;li&gt;Given every image region, one forward propagation through the CNN generates a feature vector. This feature vector is then consumed by a binary SVM trained for each class independently.
The positive samples are proposed regions with IoU (intersection over union) overlap threshold &amp;gt;= 0.3, and negative samples are irrelevant others.&lt;/li&gt;
&lt;li&gt;Use &lt;a href=&#34;#bounding-box-regression&#34;&gt;bounding box regression&lt;/a&gt; to refine the region proposals with CNN feature vector. Also &lt;a href=&#34;#non-maximum-suppression&#34;&gt;NMS&lt;/a&gt; to delete redundant region proposals.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;speed-bottleneck&#34;&gt;Speed Bottleneck&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;Selective search for proposal is slow.&lt;/li&gt;
&lt;li&gt;Calculate feature vectors for every region proposals.&lt;/li&gt;
&lt;li&gt;Detection process consists of three steps without any computation sharing.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Later work mainly concentrate on the three aspects above to achieve real-time detection.&lt;/p&gt;

&lt;h2 id=&#34;fast-rcnn&#34;&gt;Fast-RCNN&lt;/h2&gt;

&lt;p&gt;Fast RCNN mainly deal with the second speed bottleneck of RCNN: Calculate feature vectors for every region proposals.&lt;/p&gt;

&lt;p&gt;Instead of extracting CNN feature vectors independently for each region proposal, this model aggregates them into one CNN forward pass over the entire image and the region proposals share this feature matrix. Then the same feature matrix is branched out to be used for learning the object classifier (No SVM but softmax) and the bounding-box regressor.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/fastrcnn.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;roi-pooling&#34;&gt;ROI pooling&lt;/h3&gt;

&lt;p&gt;Indeed, there is nothing magic about ROI pooling. It is used to generate fixed size feature map regardless of the input image(region proposal) size.&lt;/p&gt;

&lt;p&gt;For example, the feature size of a projected region is $h \times w$, and we want a fixed feature map with size $H \times W$. ROI pooling will divide original feature map into $H \times W$ grids, approximately every subwindow of size $h/H \times w/W$. Then apply max-pooling in each grid.&lt;/p&gt;

&lt;p&gt;Here is a diagram of ROI pooling from cs231n Lecture:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/roi-pooling.png&#34; width=&#34;600&#34;&gt;&lt;/p&gt;

&lt;h3 id=&#34;workflow-1&#34;&gt;Workflow&lt;/h3&gt;

&lt;p&gt;Fast R-CNN is formulated as follows:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;The same with R-CNN, Apply &lt;a href=&#34;#selective-search&#34;&gt;selective search&lt;/a&gt; to input image, and generate ~2k candidates per image containing target objects with different sizes.&lt;/li&gt;
&lt;li&gt;Start from a pretrained ImageNet model.

&lt;ul&gt;
&lt;li&gt;Replace the last max pooling layer of the pre-trained CNN with a RoI pooling layer to ensure fixed output size.&lt;/li&gt;
&lt;li&gt;Different from R-CNN, only do CNN forward pass once, and directly map region proposals to their corresponding feature map. Shared computation speed up the whole process.&lt;/li&gt;
&lt;li&gt;Replace the last fully connected layer and the last softmax layer (K classes) with a fully connected layer and softmax over K + 1 classes.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Finally the model branches into two output layers:

&lt;ul&gt;
&lt;li&gt;A softmax classifier of K + 1 classes (extra one is the background class), output a discrete probability distribution per RoI.&lt;/li&gt;
&lt;li&gt;A bounding-box regression model to refine original RoI for each of K classes.&lt;/li&gt;
&lt;li&gt;The overall loss function is a combination of classification loss and bounding box location loss.
&lt;br /&gt;
&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;faster-rcnn&#34;&gt;Faster-RCNN&lt;/h2&gt;

&lt;p&gt;The intuition of &lt;a href=&#34;https://arxiv.org/pdf/1506.01497.pdf&#34; target=&#34;_blank&#34;&gt;Faster-RCNN&lt;/a&gt; is simple: since CNN is so good and efficient, and using selective search is too slow, why not also using CNN to generate region proposals? So, the main difference between faster-RCNN and fast RCNN is the region proposal network (RPN) to generate region proposals with CNN.&lt;/p&gt;

&lt;p&gt;Here is an illustratioon of Faster RCNN:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/faster-RCNN.png&#34; width=&#34;800&#34;&gt;&lt;/p&gt;

&lt;h3 id=&#34;region-proposal-network&#34;&gt;Region Proposal Network&lt;/h3&gt;

&lt;p&gt;Region proposal network consists of three parts:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Feed input image into CNN and get a set of convlutional feature maps on the last convolutional layer.&lt;/li&gt;
&lt;li&gt;Then a sliding window is running spatially on these feature maps. The size of sliding window is $3 \times 3$. For each sliding window, a set of 9 anchors are generated which all have the same center (the same with sliding window) but with 3 different aspect ratios (1:2, 1:1, 2:1) and 3 different scales. Note that all these coordinates are computed with respect to the original image. (image &lt;a href=&#34;https://www.quora.com/How-does-the-region-proposal-network-RPN-in-Faster-R-CNN-work&#34; target=&#34;_blank&#34;&gt;source&lt;/a&gt;)
&lt;img src=&#34;/img/anchor.png&#34; width=&#34;600&#34;&gt;&lt;/li&gt;
&lt;li&gt;Finally, the 3×3 spatial features extracted from convolution feature maps are fed to a smaller network which has two tasks: classification (cls) and regression (reg). The output of regressor determines a a predicted bounding box (x,y,w,h), The classification network produces a probability p indicating whether the the predicted box contains an object (1) or it is from background (0 for no object).
&lt;img src=&#34;/img/rpn-end.jpg&#34; width=&#34;600&#34;&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;So, the output of rpn is similar to selective search method, which is ~2k region proposals. But the speed of RPN is much faster than selective search method. In this way, the last speed bottleneck of RCNN is addressed. Note that if accuracy is not important, RPN can complete object recognition by its own by modify the classification into N classes.&lt;/p&gt;

&lt;h3 id=&#34;workflow-2&#34;&gt;Workflow&lt;/h3&gt;

&lt;p&gt;Except the RPN, the rest of Faster R-CNN is the same with Fast R-CNN. Faster R-CNN is formulated as follows:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Start from ImageNet pretrained model, fine-tune the RPN (region proposal network) end-to-end for the region proposal task. Positive samples have IoU (intersection-over-union) &amp;gt; 0.7, while negative samples have IoU &amp;lt; 0.3.&lt;/li&gt;
&lt;li&gt;Train a Fast R-CNN object detection model using the proposals generated by the current RPN&lt;/li&gt;
&lt;li&gt;Then use the Fast R-CNN network to initialize RPN training. While keeping the shared convolutional layers, only fine-tune the RPN-specific (background/object classification and bounding box regression) layers. At this stage, RPN and the detection network share convolutional layers.&lt;/li&gt;
&lt;li&gt;Finally fine-tune the unique layers of Fast R-CNN&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;mask-rcnn&#34;&gt;Mask-RCNN&lt;/h2&gt;

&lt;p&gt;So far, we’ve seen how CNN features are interesting and effective in object recognition with bounding boxes. Recently, Kaiming He, Ross Girshick and a team of researchers uses CNN features to another important aspect of object recognition: instance segmentation. They name their work &lt;a href=&#34;https://arxiv.org/abs/1703.06870&#34; target=&#34;_blank&#34;&gt;Mask RCNN&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Semantic segmentation need to segment and cluster pixels of different object classes in a single graph; while instance segmentation need to locate every single object and cluster pixels of this object. This image shows the difference of semantic and instance segmentation:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/instancesegmentation.png&#34; width=&#34;600&#34;&gt;&lt;/p&gt;

&lt;p&gt;So, overall, instance is more difficult than semantic segmentation. And it is highly related to object detection. For semantic segmentation, most modern frameworks are based on &lt;a href=&#34;https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf&#34; target=&#34;_blank&#34;&gt;Fully Convolutional Neural Networks (FCN)&lt;/a&gt;. FCN managed to extract the region of objects in an image but it cannot tell us the exact region of every single object. Here, Mask RCNN combined RCN and Faster RCNN, adding an extra FCN branch to faster RCNN, parallel to localization and classification, to make pixel level prediction and instance segmentation in a single bounding box.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/mask-rcnn.png&#34; width=&#34;600&#34;&gt;&lt;/p&gt;

&lt;h3 id=&#34;roi-align&#34;&gt;ROI Align&lt;/h3&gt;

&lt;p&gt;Different to object detection, pixel level prediction need more fine-grained alignment rather than simple ROI pooling. In ROI pooling, the location information is quantified twice:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Quantize region proposal coordinates as integer point coordinates&lt;/li&gt;
&lt;li&gt;The quantized region is evenly divided into $k \times k$ cells and the boundaries of each cell are quantized.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&#34;/img/ROIquantize.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;From the diagram above, the ground truth bounding box is $655 \times 655$. And after 5 max pooling operation, the corresponding size on feature map is &lt;sup&gt;655&lt;/sup&gt;&amp;frasl;&lt;sub&gt;32&lt;/sub&gt; = 20.78 , but ROI pooling quantizes it into $20$. After that, we need to divide the bounding box into $7 \times 7$ cells, the size of each cell is &lt;sup&gt;20&lt;/sup&gt;&amp;frasl;&lt;sub&gt;7&lt;/sub&gt; = 2.86 , but ROI pooling quantizes it into $2$. So, the largest quantization error of ROI pooling is about $1.5$ pixels on feature map, which is $50$ pixels on input image.&lt;/p&gt;

&lt;p&gt;What ROIalign does is removing the quantization in ROI pooling.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Do not quantize region proposal coordinates, keep it as float numbers.&lt;/li&gt;
&lt;li&gt;Do not quantize $k \times k$ cells boundaries when dividing, keep it as float numbers.&lt;/li&gt;
&lt;li&gt;Calculate the value of four fixed points in each cell using bilinear interpolation method, and then perform max pooling for these four points.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Here is a figure for ROI align:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/ROIAlignDiagram.png&#34; width=&#34;600&#34;&gt;&lt;/p&gt;

&lt;p&gt;And according to the paper, four fixed points in a cell works best, but only use one fixed point in a cell merely hurt the performance. Bilinear interpolation is totally differentiable, so back propogation is also straight forward.&lt;/p&gt;

&lt;p&gt;The training process is quite similar with faster RCNN, except for an extra predicting head for segmentation.&lt;/p&gt;

&lt;h3 id=&#34;key-point-detection&#34;&gt;Key Point Detection&lt;/h3&gt;

&lt;p&gt;In addition, mask RCNN can handle human pose key point detection. We just need to replace the segmentation mask with one-hot key point mask. In mask RCNN, human pose has 17 key points. The result of Mask RCNN is pretty good:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/maskrcnnresult.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;single-shot-detector&#34;&gt;Single Shot Detector&lt;/h1&gt;

&lt;p&gt;The previous methods of object detection all share one thing in common: they have one part of their network dedicated to providing region proposals followed by a high quality classifier to classify these proposals. These methods are very accurate but come at a big computational cost. In embedded systems, we have to control the computation cost. These method may not be suitable.&lt;/p&gt;

&lt;p&gt;Another way of doing object detection is by combining these two tasks into one network. Similar to region proposal network, we use different pre-defined boxes with various shape to capture objects, and predict class scores and bounding box offsets.&lt;/p&gt;

&lt;h2 id=&#34;single-shot-multibox-detector&#34;&gt;Single Shot MultiBox Detector&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1512.02325&#34; target=&#34;_blank&#34;&gt;Single Shot MultiBox Detector&lt;/a&gt; (by C. Szegedy et al.) was released at the end of 2016 and reaches the state-of-the-art performance of object recognition task in real time (58 FPS on a Nvidia Titan X).&lt;/p&gt;

&lt;p&gt;The name of SSD describes itself very well. Single Shot mean this method only need to run CNN forward pass once; Multibox means this method uses multiple boxes (different shape, on multiple CNN feature map scales) to detect object; detector means emm&amp;hellip;. Let&amp;rsquo;s dive into the details of SSD:&lt;/p&gt;

&lt;h3 id=&#34;architecture&#34;&gt;Architecture&lt;/h3&gt;

&lt;p&gt;Here is a diagram of SSD architecture:(image &lt;a href=&#34;https://www.slideshare.net/xavigiro/ssd-single-shot-multibox-detector&#34; target=&#34;_blank&#34;&gt;source&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/SSDarchitecture.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The main property of SSD is that it uses feature map from different CNN layers with different scale. SSD is built on VGG net and has six feature maps in total, each responsible for a different scale of objects, allowing it to identify objects across a large range of scales.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/ssdwork.png&#34; width=&#34;600&#34;&gt;&lt;/p&gt;

&lt;p&gt;The figure above shows how SSD works. In a convolutional fashion, we evaluate a small set
of default boxes (like &lt;a href=&#34;#region-proposal-network&#34;&gt;RPN anchor boxes&lt;/a&gt; of different aspect ratios at each location in several feature maps with
different scales (e.g. 8 $\times$ 8 and 4 $\times$ 4 in b and c). For each default box, we predict
both the shape offsets and the confidences for all object categories $((c_1, c_2, · · · , c_p))$.
At training time, we first match these default boxes to the ground truth boxes. For
example, we have matched two default boxes with the cat and one with the dog, which
are treated as positives and the rest as negatives. Any default box with an IOU of 0.5 or greater with a ground truth box is considered a match.&lt;/p&gt;

&lt;p&gt;For each default boxes of each cell, SSD predict two things:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Confident scores for all object categories $((c_1, c_2, · · · , c_p))$ (the same to RCNN, an extra background class indicating no object in this box)&lt;/li&gt;
&lt;li&gt;An offset vector with 4 entries $(C_x, C_y, C_w, C_h)$ to refine the matched default boxes to ground truth boxes.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;And the model loss is a weighted sum
between localization loss (e.g. Smooth L1) and confidence loss (e.g. Softmax). In the figure above, only three default get matched. All other default boxes need to predict a background class.&lt;/p&gt;

&lt;p&gt;SSD is quite similar to a [RPN]() with multi scale boxes. The default boxes are called prior here. Note that the prior can be customed according to our specific task. For example, if you want to detect pedestrian, you may need default boxes with large height and small width.&lt;/p&gt;

&lt;h3 id=&#34;implementation-details&#34;&gt;Implementation Details&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;Start from pretrained ImageNet model (VGG), get multiple-scale convolutional feature map.&lt;/li&gt;
&lt;li&gt;Hard negative Mining: during training, as most of the bounding boxes will have low IoU, interpreted as negative training examples. We may have disproportionate amount of negative examples. The paper suggests keeping a ratio of negative to positive examples of around 3:1.&lt;/li&gt;
&lt;li&gt;Data Augmentation: Sample patches with 0.1, 0.3, 0.5, 0.7, or 0.9 overlap to the ground truth; Random sample patches; 0.5 probability horizonal flip.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;yolov1&#34;&gt;YOLOv1&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://pjreddie.com/darknet/yolo/&#34; target=&#34;_blank&#34;&gt;You only look once (YOLO)&lt;/a&gt; is a state-of-the-art, real-time object detection system. On a Pascal Titan X it processes images at 30 FPS and has a mAP of 57.9% on COCO test-dev. More importantly, YOLO keeps evolving in recent years.&lt;/p&gt;

&lt;h3 id=&#34;architecture-1&#34;&gt;Architecture&lt;/h3&gt;

&lt;p&gt;YOLOv1 is a little bit less precise than R-CNN families, but it is the first work to realize real time detection with CNN. Similar to other single shot detectors, the intuition of YOLOv1 is quite straight forward.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/YOLOv1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;YOLOv1 models detection as a regression problem. It divides the image into an S × S grid and for each grid cell predicts B bounding boxes, confidence for those boxes, and C class probabilities. These predictions are encoded as an S × S × (B × 5 + C) tensor. For example, on PASCAL VOC, S = 7,
B = 2. PASCAL VOC has 20 labelled classes so C = 20. Our final prediction is a 7 × 7 × 30 tensor.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/YOLOv1arch.png&#34; width=&#34;800&#34;&gt;&lt;/p&gt;

&lt;p&gt;The figure above is the architecture of YOLOv1. Based on pretrain the convolutional layers on the ImageNet classification
task at half the resolution (224 × 224 input image) and then double the resolution for detection.&lt;/p&gt;

&lt;h3 id=&#34;loss-function&#34;&gt;Loss function&lt;/h3&gt;

&lt;p&gt;Here is the multi-part loss function that we want to optimize:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/YOLOv1lossfunction.png&#34; width=&#34;600&#34;&gt;&lt;/p&gt;

&lt;p&gt;where:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;B: Number of bounding boxes (2)&lt;/li&gt;
&lt;li&gt;$x_i,y_i,w_i,h_i$:  Bounding box parameters&lt;/li&gt;
&lt;li&gt;$C_i$: Some particular class i&lt;/li&gt;
&lt;li&gt;S: Grid size (7)&lt;/li&gt;
&lt;li&gt;$\bf{1}^{obj}_i$ : If object appear on the cell i, if does not appear it will be zero&lt;/li&gt;
&lt;li&gt;$\bf{1}^{obj}_{ij}$ : Bounding box j, from cell i responsible for prediction (i.e. has the highest
IOU of any predictor in that grid cell)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;And the weight in loss function $\lambda_{coord} = 5$ and $\lambda_{noobj} = 0.5$. Different to bounding box regression, YOLO uses square root instead of logrithm, but they should act in the same way. Note that the loss function &lt;strong&gt;only&lt;/strong&gt; penalizes classification
error if an object is present in that grid cell (hence the conditional
class probability discussed earlier). It also &lt;strong&gt;only&lt;/strong&gt; penalizes
bounding box coordinate error if that predictor is
“responsible” for the ground truth box (i.e. has the highest
IOU of any predictor in that grid cell). So, the first step of every training phase is &lt;strong&gt;bounding box matching&lt;/strong&gt;.&lt;/p&gt;

&lt;h3 id=&#34;workflow-3&#34;&gt;Workflow&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;Start from a pre-train CNN network on image classification tasks.&lt;/li&gt;
&lt;li&gt;Split an image into S x S cells. Each cell is responsible for identifying the object (if any) with its &lt;strong&gt;center&lt;/strong&gt; located in this cell. Each cell predicts the location of B bounding boxes and a confidence score, and a probability of object class conditioned on the existence of an object in the bounding box.

&lt;ul&gt;
&lt;li&gt;A bounding box is defined by a tuple of $(C_x, C_y, C_w, C_h)$. x and y are normalized to be the offsets of a cell location; w and h are normalized by the image width and height, and thus between (0, 1].&lt;/li&gt;
&lt;li&gt;A confidence score is: probability(containing an object) x IoU(pred, truth).&lt;/li&gt;
&lt;li&gt;If the cell contains an object, it predicts a probability of this object belonging to one class $C_i$, i=1,2,…, K. At this stage, the model only predicts one set of class probabilities per cell, regardless of the number of boxes B.&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;The final layer of the pre-trained CNN is modified to output a prediction tensor of size S x S x (5B + C).&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;limitation&#34;&gt;Limitation&lt;/h3&gt;

&lt;p&gt;YOLOv1 also have a lot of limitations:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Each grid cell only predicts two boxes and can only have one class. This prevent YOLO from detecting nearby objects.&lt;/li&gt;
&lt;li&gt;Compared to SSD, YOLO only uses coarse features. Cannot detect relative small objects.&lt;/li&gt;
&lt;li&gt;YOLO has higher localization errors and the recall is lower compared to SSD.&lt;/li&gt;
&lt;li&gt;The final FC layers are slow and kind of redundant.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;yolov2&#34;&gt;YOLOv2&lt;/h2&gt;

&lt;p&gt;Inspired by faster RCNN and SSD, &lt;a href=&#34;https://arxiv.org/pdf/1612.08242.pdf&#34; target=&#34;_blank&#34;&gt;YOLOv2&lt;/a&gt; improves the accuracy significantly while making it faster.&lt;/p&gt;

&lt;h3 id=&#34;improvements&#34;&gt;Improvements&lt;/h3&gt;

&lt;p&gt;First, YOLOv2 is &lt;strong&gt;Better&lt;/strong&gt;:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Batch Normalization&lt;/strong&gt;. Batch normalization leads to significant
improvements in convergence while eliminating the
need for other forms of regularization. Improve mAP by 2%. See my another blog for &lt;a href=&#34;https://shen338.github.io/post/going-deeper-into-batch-normalization/&#34; target=&#34;_blank&#34;&gt;batch norm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;High Resolution Classifier&lt;/strong&gt;: we first fine tune the classification network
at the full 448 × 448 resolution for 10 epochs on ImageNet different to original use 224 × 224 network directly for 448 × 448 detection. Improve mAP by 4%.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Convolutional With Anchor Boxes&lt;/strong&gt;: We remove the fully connected layers from YOLO and
use anchor boxes like &lt;a href=&#34;#faster-rcnn&#34;&gt;faster RCNN&lt;/a&gt; to predict bounding boxes. No improvement on mAP but 7% increase on recall rate, which makes the model has more room to improve.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Dimension Clusters&lt;/strong&gt;: The author run K-means algorithm to determine the anchor box sizes. Shown in the following figure, the authors choose
k = 5 as a good tradeoff between model complexity and
high recall.
&lt;img src=&#34;/img/clusterbox.png&#34; width=&#34;600&#34;&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Direct location prediction&lt;/strong&gt;: Instead of predicting offsets we follow the approach of
YOLO and predict location coordinates relative to the location
of the grid cell. (Predict coordinates directly and predict scale using archor boxes). Improve YOLO by 5%. The network predicts 5 coordinates
for each bounding box, $t_x, t_y, t_w, t_h$, and $t_o.$ If the cell is
offset from the top left corner of the image by $(c_x, c_y)$ and
the bounding box prior has width and height $p_w, p_h$, then
the predictions correspond to:
&lt;img src=&#34;/img/YOLOv2direct.png&#34; width=&#34;600&#34;&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Fine-Grained Features&lt;/strong&gt;: Similar to faster RCNN and SSD, adding a passthrough layer that brings
features from an earlier layer at 26 × 26 resolution along with original 13 × 13 resolution. Improve mAP about 1%.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Multi-Scale Training&lt;/strong&gt;: During training, YOLO takes images of size {320, 352, &amp;hellip;, 608} (with a step of 32 = YOLO downsampling rate). For every 10 batches, YOLOv2 randomly selects another image size to train the model. This acts as data augmentation and forces the network to predict well for different input image dimension and scale.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Second, YOLOv2 is &lt;strong&gt;Faster&lt;/strong&gt;:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;GoogleNet. Reduce billions of floating points operations but YOLO’s
custom model gets 88.0% ImageNet compared to 90.0% for
VGG-16.&lt;/li&gt;
&lt;li&gt;DarkNet 19: Compare to VGG 19, double the number of channels after every
pooling step. Replace redundant FC layer with global average pooling; use 1 × 1 filters to compress the feature representation
between 3 × 3 convolutions (bottleneck). Requires only 5.58 billion operations and 91.2% top-5 accuracy on ImageNet.&lt;/li&gt;
&lt;li&gt;Although SSD500 runs at 45 FPS, a lower resolution version of YOLOv2 with mAP 76.8 (the same as SSD500) runs at 67 FPS, thus showing us the high performance capabilities of YOLOv2 as a result of its design choices.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Third, YOLOv2 is &lt;strong&gt;Stronger&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;Hierarchical classification: Without going into details, YOLO combines labels in different datasets to form a tree-like structure WordTree. The children form an is a relationship with its parent like biplane is a plane. But the merged labels are now not mutually exclusive.&lt;/p&gt;

&lt;p&gt;YOLO9000 extends YOLO to detect objects over 9000 classes using hierarchical classification with a 9418 node WordTree. It combines samples from COCO and the top 9000 classes from the ImageNet.  ImageNet is a much larger dataset so we balance
the dataset by oversampling COCO so that ImageNet
is only larger by a factor of 4:1. It learns to find objects using the detection data in COCO and to classify these objects with ImageNet samples.&lt;/p&gt;

&lt;p&gt;YOLO9000 evaluates its result from the ImageNet object detection dataset which has 200 categories. It shares about 44 categories with COCO. But for another 156 categories, YOLO is never trained how to locate them. But YOLO9000 gets 19.7 mAP overall with 16.0 mAP on those 156 categories. YOLO9000 performs well with new species of animals not found in COCO because their shapes can be generalized easily from their parent classes.&lt;/p&gt;

&lt;h2 id=&#34;yolov3&#34;&gt;YOLOv3&lt;/h2&gt;

&lt;h2 id=&#34;feature-pyramid-network-fpn&#34;&gt;Feature Pyramid Network (FPN)&lt;/h2&gt;

&lt;p&gt;In tradition neural network, due to pooling layer, early layers tends to have higher resolution and more spatial information; while higher layers tend to have more semantic information and low resolution. Before FPN, people often use a pyramid of images or features to utilize the advantages of different feature scales (see following figure). But this method are compute and memory intensive. And features close to input image is not so effective to detect objects.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1612.03144.pdf&#34; target=&#34;_blank&#34;&gt;Feature Pyramid Network (FPN)&lt;/a&gt; extends the idea of feature pyramid, incorprating a topdown architecture with lateral connections to different feature map scales. FPN shows significant improvement as a generic feature extractor
in several applications.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/FPN.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;architecture-2&#34;&gt;Architecture&lt;/h3&gt;

&lt;p&gt;FPN composes of a bottom-up and a top-down pathway. The bottom-up pathway is the usual convolutional network for feature extraction. Close to the end of CNN, the feature maps tend to have more semantic information but low resolution.&lt;/p&gt;

&lt;p&gt;FPN also provide top-down path to preserve the semantic information from higher layer to lower, high resolution layer. In this way, we can utilize the advantages of different feature scales. As move up in CNN, the spatial dimension is reduced by &lt;sup&gt;1&lt;/sup&gt;&amp;frasl;&lt;sub&gt;2&lt;/sub&gt;, so we need to upsample it back in top-down path (using
nearest neighbor upsampling for simplicity). And before every lateral connection, faeture maps undergo a 1×1 convolutional layer to reduce channel dimensions. The top-down path is just elementwise addition from different lateral connection.  Finally,
a 3×3 convolution is appended on each merged map to
generate the final feature map, which is to reduce the aliasing
effect of upsampling. Here is an illustration of FPN:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/FPNarch.png&#34; width=&#34;600&#34;&gt;&lt;/p&gt;

&lt;h3 id=&#34;feature-pyramid-networks-for-rpn&#34;&gt;Feature Pyramid Networks for RPN&lt;/h3&gt;

&lt;p&gt;Similar to the idea of &lt;a href=&#34;#ii&#34;&gt;SSD&lt;/a&gt;, RPN with feature pyramid network put anchor boxes in different feature map scales (15 in total). Formally, define the anchors
to have areas of $[32^2, 64^2, 128^2, 256^2, 512^2]$ pixels
on $[P_2, P_3, P_4, P_5, P_6]$ respectively.&lt;/p&gt;

&lt;p&gt;According to the authors, the parameters of the heads are shared
across all feature pyramid levels; they have also evaluated the
alternative without sharing parameters and observed similar
accuracy.&lt;/p&gt;

&lt;h3 id=&#34;feature-pyramid-networks-for-faster-mask-r-cnn&#34;&gt;Feature Pyramid Networks for Faster/Mask R-CNN&lt;/h3&gt;

&lt;p&gt;To use FPN in Faster/Mask R-CNN, we need
to assign RoIs of different scales to the pyramid levels. We have to assign ROIs to a proper scale and easy to extract features. So, if ROI is bigger, it should use the feature map in higher level and vice versa. Formally, we assign an RoI of
width w and height h (on the input image to the network) to
the level $P_k$ of our feature pyramid by:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/FPNformula.png&#34; width=&#34;400&#34;&gt;&lt;/p&gt;

&lt;p&gt;where:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;$k_0 = 4$, Analogous to the ResNet-based
Faster R-CNN system that uses C4 as the single-scale
feature map&lt;/li&gt;
&lt;li&gt;k indicates using $P_k$ from FPN&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;experimental-result&#34;&gt;Experimental Result&lt;/h3&gt;

&lt;p&gt;FPN for region proposal network:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/FPN-RPN.png&#34; width=&#34;1000&#34;&gt;&lt;/p&gt;

&lt;p&gt;FPN for faster RCNN:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/FPN-FasterRCNN.png&#34; width=&#34;1000&#34;&gt;&lt;/p&gt;

&lt;p&gt;FPN on COCO dateset:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/FPNcoco.png&#34; width=&#34;1000&#34;&gt;&lt;/p&gt;

&lt;p&gt;Top-down pathway plus lateral connections increases 8.0 points over the single-scale RPN
baseline. For small objects, it improves 12.9 points. Recently, FPN has enabled new top results in all tracks
of the COCO competition, including detection, instance
segmentation, and keypoint estimation.&lt;/p&gt;

&lt;p&gt;Currently, FPN is a standard baseline of almost every object recognition system.&lt;/p&gt;

&lt;h2 id=&#34;focal-loss-and-retinanet&#34;&gt;Focal Loss and RetinaNet&lt;/h2&gt;

&lt;p&gt;One stage detectors are applied over a regular, dense sampling of object locations, scales, and aspect ratios.  Recent work on one-stage detectors, like YOLO and SSD, yielding faster detectors with accuracy within 10-
40% relative to state-of-the-art two-stage methods, like RCNN families.&lt;/p&gt;

&lt;p&gt;The work &lt;a href=&#34;https://arxiv.org/pdf/1708.02002.pdf&#34; target=&#34;_blank&#34;&gt;Focal Loss for Dense Object Detection&lt;/a&gt; from facebook AI research pushs this envelop: theironestage
object detector that, for the first time, matches the
state-of-the-art, more complex two stage detectors.&lt;/p&gt;

&lt;h3 id=&#34;class-imbalance&#34;&gt;Class Imbalance&lt;/h3&gt;

&lt;p&gt;The authors identify class imbalance as the main obstacle hurting the performance of one stage detectors. Unlike two stage detectors, one stage detecors must process a much
larger set of candidate object locations regularly sampled
across an image(~100k), and most of them are easy samples. Detectors can easily classify them as background or object. Although their gradient is not large, their overwhelming amount may influence the performance of detectors. Hilariously, these easy samples are called &lt;strong&gt;&amp;ldquo;inliers&amp;rdquo;&lt;/strong&gt; in thie paper. In previous detectors, people use bootstrapping, hard negative mining to address this problem, which is not good enough.&lt;/p&gt;

&lt;p&gt;So, here comes focal loss:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/focalloss.png&#34; width=&#34;600&#34;&gt;&lt;/p&gt;

&lt;p&gt;The focal loss just modify the cross entropy loss into weight cross entropy. The weight is given by the prediction confidence: $(1 − p_t)^{\gamma}$. In this way, the weight for very confident sample will decrease a lot during training, which perfectly addresses the class imbalance problem discussed above. In experiment, the recommended value of $\gamma$ is 2. Also, you can slightly improve the accuracy by using $\alpha$ balanced form ($\alpha$ is weight on rare class, $\gamma = 2$, $\alpha = -0.25$ works best):&lt;/p&gt;

&lt;p&gt;$$FL(p_t) = - \alpha_t (1 - p_t)^{\gamma} log(p_t)$$&lt;/p&gt;

&lt;h3 id=&#34;retinanet&#34;&gt;RetinaNet&lt;/h3&gt;

&lt;p&gt;To express the effective of Focal Loss, the authors designed a simple ont stage detector, &lt;strong&gt;RetinaNet&lt;/strong&gt;, achieves a COCO test-dev AP of 39.1
while running at 5 fps, surpassing current all single stage and two stage detectors.&lt;/p&gt;

&lt;p&gt;Here is a diagram for RetinaNet architecture:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/retinanetarch.png&#34; width=&#34;800&#34;&gt;&lt;/p&gt;

&lt;h3 id=&#34;implementation-details-1&#34;&gt;Implementation Details&lt;/h3&gt;

&lt;p&gt;RetinaNet uses FPN backbone. Reason: preliminary experiments using features
from only the final ResNet layer yielded low AP. Addition to original {1:2, 1:1, 2:1} anchor boxes, RetinaNet add anchor boxes wih size {$2^0, 2^{\frac{1}{3}}, 2^{\frac{2}{3}}$}, totally 9 anchor boxes. This brings in more computation but also high recall and accuracy. Anchor boxes with IOU &amp;gt; 0.5 is treated as positive examples, IOU &amp;lt; 0.4 are negative, 0.4 &amp;lt; IOU &amp;lt; 0.5 are ignored. Also, classification and bounding box regression subnets are sharing a common structure, use separate parameters.&lt;/p&gt;

&lt;p&gt;Focal loss in RetinaNet are normalized by the number of anchors assigned to a ground-truth box, since the vast majority of anchors are easy
negatives and receive negligible loss values under the focal loss.&lt;/p&gt;

&lt;p&gt;Note that RetinaNet uses an &lt;strong&gt;initialization trick&lt;/strong&gt; to avoid overwhelming gradient from frequent class. All new conv layers except the final
one in the RetinaNet subnets are initialized with bias b = 0
and a Gaussian weight fill with $\sigma = 0.01$. For the final conv
layer of the classification subnet, we set the bias initialization
to $b = − log((1 − π)/π)$.&lt;/p&gt;

&lt;p&gt;In this way, predictions of first iteration would be centered at $\pi$ (0.01 in experiment, not sensitive at all), which is a relatively small number. Since the frequent class is negative, the gradient of frequent, negative sample would be less than positive, rare samples. In this way, we can compensate the gradient advantage of frequent samples and avoid too large loss values in the first iteration of training.&lt;/p&gt;

&lt;h3 id=&#34;results&#34;&gt;Results&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;/img/retinanetresult.png&#34; width=&#34;1000&#34;&gt;&lt;/p&gt;

&lt;p&gt;According this figure, RetinaNet achieves top results,
outperforming both one-stage and two-stage models. Plus, compared to existing
one-stage methods, RetinaNet achieves a healthy
5.9 point AP gap (39.1 vs. 33.2) with the closest competitor,
DSSD.&lt;/p&gt;

&lt;h1 id=&#34;summary&#34;&gt;Summary&lt;/h1&gt;

&lt;p&gt;From all the discussion above, we can see the rapid progress of object recognition in recent five years. These models inspired and equipped each other to move forward.&lt;/p&gt;

&lt;p&gt;Up to now, object tends to converge to a optimal solution. For example, &lt;strong&gt;anchor boxes&lt;/strong&gt; from Faster RCNN, &lt;strong&gt;Boxes in multi scale features&lt;/strong&gt; from SSD, &lt;strong&gt;image grid prediction&lt;/strong&gt; from YOLO, &lt;strong&gt;non-maximal suppression, bounding box regression&lt;/strong&gt; from RCNN, &lt;strong&gt;ROIpooling/align&lt;/strong&gt; from Fast/Mask RCNN are widely used and become an essential part of object recognition systems.&lt;/p&gt;

&lt;p&gt;Also, various clever innovations really boost the performance of object recognition systems, like recent &lt;strong&gt;RetinaNet with Focal Loss&lt;/strong&gt;, &lt;strong&gt;Feature Pyramid network&lt;/strong&gt;, becomes the standard component of every state-of-the-art system.&lt;/p&gt;

&lt;p&gt;One thing for sure, this optimal solution is definitely not the global optimal. Looking forward to the next stage of object recognition!&lt;/p&gt;

&lt;h1 id=&#34;reference-materials&#34;&gt;Reference Materials&lt;/h1&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks&#34; target=&#34;_blank&#34;&gt;AlexNet&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1512.03385&#34; target=&#34;_blank&#34;&gt;ResNet&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1409.4842.pdf&#34; target=&#34;_blank&#34;&gt;GoogLeNet&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1512.00567&#34; target=&#34;_blank&#34;&gt;Inception v3 network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1602.07261?context=cs&#34; target=&#34;_blank&#34;&gt;Inception Resnet v2 and Inception v4&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://vision.stanford.edu/teaching/cs231b_spring1213/slides/dpm-slides-ross-girshick.pdf&#34; target=&#34;_blank&#34;&gt;Deformable Part Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1312.6229&#34; target=&#34;_blank&#34;&gt;Overfeat&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1311.2524&#34; target=&#34;_blank&#34;&gt;R-CNN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1504.08083&#34; target=&#34;_blank&#34;&gt;Fast R-CNN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1506.01497&#34; target=&#34;_blank&#34;&gt;Faster R-CNN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1703.06870&#34; target=&#34;_blank&#34;&gt;Mask R-CNN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1512.02325&#34; target=&#34;_blank&#34;&gt;Single Shot Multibox Detector(SSD)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1506.02640&#34; target=&#34;_blank&#34;&gt;YOLO&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1612.08242&#34; target=&#34;_blank&#34;&gt;YOLO9000: Better, Faster, Stronger&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://pjreddie.com/media/files/papers/YOLOv3.pdf&#34; target=&#34;_blank&#34;&gt;YOLOv3&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1612.03144&#34; target=&#34;_blank&#34;&gt;Feature Pyramid Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1708.02002&#34; target=&#34;_blank&#34;&gt;Focal Loss and RetinaNet&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>CapsNet and dynamic routing</title>
      <link>/post/capsnet-and-dynamic-routing/</link>
      <pubDate>Thu, 19 Jul 2018 21:00:00 -0500</pubDate>
      
      <guid>/post/capsnet-and-dynamic-routing/</guid>
      <description>

&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;In late 2017, Geoffrey Hinton, one of the biggest names in deep learning community, finally published his work about capsule theory. Hintion has worked on this for years, like &lt;a href=&#34;http://www.cs.toronto.edu/~fritz/absps/transauto6.pdf&#34; target=&#34;_blank&#34;&gt;Transforming Auto-Encoders&lt;/a&gt;. This should be a big step for us to understand human brain.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1710.09829.pdf&#34; target=&#34;_blank&#34;&gt;CapsNet&lt;/a&gt; consists of many capsules. Rather than output a scalar, capsules output a vector. The length of the vector will represent the probability of certain entity and the vector itself will represent the property of this entity. These properties can include many different types of instantiation parameter such as pose (position, size, orientation), deformation, velocity, albedo, hue, texture, etc. Also, high level can only be activated when two or more lower level capsules predictions agree with each other, while CNN high layer only preserve big activations from lower layer.&lt;/p&gt;

&lt;p&gt;This vector agreement immediately remind me of the attention mechanism I previously used in seq2seq models and object localization models. Attention mechanism also use vector agreement, like dot product, to measure the weight of previous output(seq2seq)/different part of image(object localization), and use the weighted version as input for the next step. And this perspective is also mentioned in the paper:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Dynamic routing can be viewed as a parallel attention mechanism that allows each capsule at one
level to attend to some active capsules at the level below and to ignore others.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;But why CapsNet works? We need to start from the drawback of CNN.&lt;/p&gt;

&lt;h2 id=&#34;problems-with-cnn&#34;&gt;Problems with CNN&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;“The pooling operation used in convolutional neural networks is a big mistake and the fact that it works so well is a disaster.” &amp;ndash; Hinton&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Based on the argument of Hinton, the main problem of convolutional neural network lies in max pooling layer. Maxpooling layers in CNN are used to squeeze the size of feature map, reduce its dimensionality and push the CNN to represent original image into another persperctive. Also, it can make the higher level neurons have larger receptive field to mkae semantic prediction. However it will lose a lot of valuable information, like the &lt;strong&gt;exact location information&lt;/strong&gt; of object in the image.&lt;/p&gt;

&lt;p&gt;For example, assume we have 5 maxpooling layer in a network, and an object in a 32x32 block of input image. In this way, no matter how we move the object in the image, there will be no change in the final feature map. This will cause problems. The network not cares about what the object is, rather than where the object lies. So, if different parts of an object move a little bit, CNN will stay the same. Here is a famous picture illustrating this point:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/capsnet_face.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Every part of human face is tweaked, but CNN still recognizes it as human face. Real world situation is not so extreme, but this example can give us an intuition of this.&lt;/p&gt;

&lt;p&gt;Additionally, CNNs cannot handle &lt;strong&gt;rotation&lt;/strong&gt; at all - if they are trained on objects in one orientation, they will have trouble when the orientation is changed. According to Hinton, CNNs cannot do ‘handedness’ detection at all, even in principle. In other words, CNNs could never tell a left shoe from a right shoe, even if they were trained on both. Also, human visual system also has this drawback. Human vision system appears to impose a rectangular coordinate frames on objects. If an object is not placed along to the coordinate, people have to mentally rotate it.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;But Capsnet has good ablity on rotations&lt;/strong&gt;. Unlike neurons in the output layer of a CNN, a capsule outputs a probability of whether an entity is present, but additionally has pose information on it. And when calculating agreement, we only care about the relative agreement over different part. So the pose of object will also incorporate the pose information. This can also transform part pose to object pose. So, when using CapsNet, we may not need to rotate the image, the capsule output already take care of it.&lt;/p&gt;

&lt;p&gt;According to Hinton, CNN is a very inefficient way of learning, in that it requires a lot of data. While CapsNet utilizes data in a more efficient way. When taking entity properties into account, it&amp;rsquo;s like automatic data augmentation (My personal understanding).&lt;/p&gt;

&lt;p&gt;To sum up, CNN&amp;rsquo;s problems lies in two part:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Lose concise location information to object parts and hierarchical pose relationships between object parts. Predict based on feature (activation) counting.&lt;/li&gt;
&lt;li&gt;Impose a rectangular coordinate frames on objects. Make it hard to recognize affine transformed objects.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;inverse-graphics&#34;&gt;Inverse Graphics&lt;/h2&gt;

&lt;p&gt;According to Hinton, the concept of CapsNet comes from the idea of inverse graphics. In computer graphics, if people want to render something, they will first render different parts, and use some vectors to put them in a proper &lt;strong&gt;relative location&lt;/strong&gt;. This process is illustrated in the following figure (image &lt;a href=&#34;https://jhui.github.io/2017/11/03/Dynamic-Routing-Between-Capsules/&#34; target=&#34;_blank&#34;&gt;source&lt;/a&gt;):&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/inverse_graphics.jpg&#34; alt=&#34;inverse_graphics&#34; /&gt;&lt;/p&gt;

&lt;p&gt;In computer graphics, people do it in the top-down way. For inverse graphics, we need to do it in a bottom-up fashion. First, we detect different small parts and their propertis, represented in the capsule output. And then, if the output &amp;ldquo;agrees&amp;rdquo;, it is reversely equivalent to the process in graphics that assembling different part into a bigger object. So, lower level capsules are more concise for entity location and high-level capsules are more about sementic information, which is the same to CNNs. In the paper, Hinton called the lower level capsule &amp;ldquo;place-encoded&amp;rdquo; and the higher-level capsules &amp;ldquo;rate-encoded&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;The most important part of CapsNet is routed by &amp;ldquo;agreement&amp;rdquo; between capsules. We only care about the &lt;strong&gt;relative&lt;/strong&gt; relationship between capsules. So, the higher level capsules only cares about whether lower level capsules agree or not, rather than what exact consists in the capsule. This is kind of like graphics, rended object is not related to view angle. This can address CNN&amp;rsquo;s weakness in affine transforms like rotation. Capsule will treat the liberty statue in different pictures equally.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/statue.jpg&#34; alt=&#34;statue&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Also, for occluded object, CapsNet only need the agreement of some parts of an object to predict the object, rather than counting feature appearance to predict the object in CNN. Also, if two objects are overlapped, their feature map in CNN may inference each other, and make prediction inaccurate, while CapsNet can avoid this with &amp;ldquo;agreement routing&amp;rdquo;. So CapsNet has advantage on recognize partly showed object in images, like the occluded faces in the following image (&lt;a href=&#34;http://www.lce.hut.fi/research/eas/object/&#34; target=&#34;_blank&#34;&gt;source&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/occluded_face.png&#34; alt=&#34;occluded_face&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Based on my understanding, the inverse graphics idea is quite interesting and mind blowing.&lt;/p&gt;

&lt;p&gt;But I want to say, CNN also cannot analyze the image in 3D space that objects actually lies in. In computer graphics, people always construct object in a 3D sapce and project it on screen as images. Why not push inverse graphics to the limit, make it exactly the inverse of graphics? I believe the next generation of CNNs would be the breakthrough of 3D reconstruction.&lt;/p&gt;

&lt;h2 id=&#34;capsnet-architecture&#34;&gt;CapsNet architecture&lt;/h2&gt;

&lt;h3 id=&#34;iterative-dynamic-routing&#34;&gt;Iterative Dynamic Routing&lt;/h3&gt;

&lt;p&gt;At first, we don&amp;rsquo;t know the weight of each capsule to calculate next layer of capsules. So, the initial value of the weight is all zero.&lt;/p&gt;

&lt;p&gt;$$b_{ij} \leftarrow 0$$&lt;/p&gt;

&lt;p&gt;In this article, all the lower right corner indexes, like $b_{ij}$, means the parameter to calculate higher level capsules $j$ with lower level capsule $i$&amp;rsquo;s output.&lt;/p&gt;

&lt;p&gt;So, with the weight of all lower capsules, we need a softmax to normalize it, just like what we do in CNNs. When calculating $j-th$ capsule in higher layer, we get:&lt;/p&gt;

&lt;p&gt;$$c_i = softmax(b_i)$$&lt;/p&gt;

&lt;p&gt;And the output of every capsule, $u_i$, need to go through a transformation:&lt;/p&gt;

&lt;p&gt;$$\hat{u}_{j|i} = W_{ij} * u_i$$&lt;/p&gt;

&lt;p&gt;We need to apply the softmax weight on the capsule output, which will be the input of next layer:&lt;/p&gt;

&lt;p&gt;$$s_j \leftarrow \sum_i c_{ij} \hat{u}_{j|i} $$&lt;/p&gt;

&lt;p&gt;The nonlinearity of CapsNet comes from the squash operation. Each capsule will simply squash long capsule input to 1 length, and squash short capsule input to 0, following this formula:&lt;/p&gt;

&lt;p&gt;$$v_j = \frac{||s_j||^2}{1+||s_j||^2} \frac{s_j}{||s_j||}$$&lt;/p&gt;

&lt;p&gt;And after each iteration, we accumulatively add $a_{ij}$ to $b_{ij}$:&lt;/p&gt;

&lt;p&gt;$$b_{ij} \leftarrow b_{ij} + a_{ij}$$&lt;/p&gt;

&lt;p&gt;where $a_{ij}$ is equal to  $\hat{u}_{j|i} * v_j$, simple dot product of last layers capsule output and current layers output.&lt;/p&gt;

&lt;p&gt;$$b_{ij} \leftarrow b_{ij} + \hat{u}_{j|i} v_j$$&lt;/p&gt;

&lt;p&gt;So, every higher layer capsule is trying to select certain lower layer capsules, whose output vector has a bigger dot product (agreement measure) with higher capsule&amp;rsquo;s own output. Note there would be a lot of other ways to do this routing by agreement thing as well.&lt;/p&gt;

&lt;p&gt;The overall algorithm goes like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/dynamic_routing.png&#34; alt=&#34;dynamic routing&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;loss-function&#34;&gt;Loss function&lt;/h3&gt;

&lt;p&gt;CapsNet uses margin loss for digit existence. The top-level capsules are responsible to present the existence of digits. And also, the loss function should talk care multi-digit cases. The loss function goes like:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/lossfunction.png&#34; alt=&#34;marginloss&#34; /&gt;&lt;/p&gt;

&lt;p&gt;where $T_k = 1$ iff a digit of class k is present
and $m^+$ = 0.9 and $m^-$ = 0.1. The λ down weighting
of the loss for absent digit classes stops the initial learning from shrinking the lengths of the activity
vectors of all the digit capsules. We use λ = 0.5. The total loss is simply the sum of the losses of all
digit capsules.&lt;/p&gt;

&lt;p&gt;And also, they use an additional reconstruction loss to encourage the digit capsules to encode the instantiation
parameters of the input digit. The reconstruction is quite simple:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/capsnet_reconstruction.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;architecture&#34;&gt;Architecture&lt;/h3&gt;

&lt;p&gt;The overall architecture of CapsNet is quite straightforward. Just copy from the paper:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;First layer is just like CNNs&lt;/strong&gt;: Conv1 has 256, 9 × 9 convolution kernels with a
stride of 1 and ReLU activation. This layer converts pixel intensities to the activities of local feature
detectors that are then used as inputs to the primary capsules.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Second layer is a preparation of Routing (output vectors):&lt;/strong&gt; The second layer (PrimaryCapsules) is a convolutional capsule layer with 32 channels of convolutional 8D capsules (i.e. each primary capsule contains 8 convolutional units with a 9 × 9 kernel and a stride
of 2)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Third layer is DigitCaps layer:&lt;/strong&gt; It has one 16D capsule per digit class and each of these
capsules receives input from all the capsules in the layer below. Dynamic routing is implemented between PrimaryCapsules layer and DigitCaps layer.&lt;/p&gt;

&lt;p&gt;Overall diagram is as follows:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/capsnet.png&#34; alt=&#34;capsNet&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;result-comparasion&#34;&gt;Result comparasion&lt;/h2&gt;

&lt;p&gt;I just want to analyze this figure:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/capsnet_result.png&#34; alt=&#34;capsNet&#34; /&gt;&lt;/p&gt;

&lt;p&gt;We can clearly see CapsNet surpasses the state-of-the-art network. Amusingly, the improvement from reconstruction loss is larger than capsules and dynamic routing&amp;hellip;&lt;/p&gt;

&lt;h2 id=&#34;advantages&#34;&gt;Advantages&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Robust to affine transform&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Experiments show that each DigitCaps capsule learns a more robust representation for each class
than a traditional convolutional network. Because there is natural variance in skew, rotation, style, etc
in hand written digits, the trained CapsNet is moderately robust to small affine transformations of the
training data.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Interpretability&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The dimensions of a
digit capsule should learn to span the space of variations in the way digits of that class are instantiated.
These variations include stroke thickness, skew and width. They also include digit-specific variations
such as the length of the tail of a 2.  After computing the activity vector for the correct digit capsule, we can
feed a perturbed version of this activity vector to the decoder network and see how the perturbation
affects the reconstruction. We can find that one
dimension (out of 16) of the capsule almost always represents the width of the digit. While some
dimensions represent combinations of global variations, there are other dimensions that represent variation in a localized part of the digit. For example, different dimensions are used for the length of
the ascender of a 6 and the size of the loop.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/CapsNet_inter.png&#34; alt=&#34;capsNet&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;problems&#34;&gt;Problems&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Computational cost&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The biggest problem of dynamic routing is of course the computational cost. For each higher level capsule, we need to calculate its agreement with all the lower layer capsules. The complexity is huge (O(N^3)). And that is also the main reason that Capsnet can only used in simple dataset like smallNORB, MNIST. In these dataset, the dimension of single capsule, as well as the network width, do not need to be very large.&lt;/p&gt;

&lt;p&gt;So, the next step must be more advanced and efficient routing method. Lucklily, we already have one now: &lt;a href=&#34;https://openreview.net/pdf?id=HJWLfGWRb&#34; target=&#34;_blank&#34;&gt;EM routing&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;em-routing&#34;&gt;EM routing&lt;/h2&gt;

&lt;h3 id=&#34;implementation&#34;&gt;Implementation&lt;/h3&gt;

&lt;h3 id=&#34;result&#34;&gt;Result&lt;/h3&gt;

&lt;h2 id=&#34;sum-up&#34;&gt;Sum up&lt;/h2&gt;

&lt;p&gt;To sum up, the advantage of CapsNet has two folders:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Instead of output a scalar in a neuron, capsules output a vector to encode the properties of an entity, like pose (position, size, orientation), deformation, velocity, albedo, hue, texture, etc.&lt;/li&gt;
&lt;li&gt;With a vector output, higher layers are able to choose any lower capsule as input. While in CNN, their input neurons are fixed.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;reference-materials&#34;&gt;Reference materials&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1710.09829.pdf&#34; target=&#34;_blank&#34;&gt;Dynamic Routing Between Capsules&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://openreview.net/pdf?id=HJWLfGWRb&#34; target=&#34;_blank&#34;&gt;MATRIX CAPSULES WITH EM ROUTING&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://moreisdifferent.com/2017/09/hinton-whats-wrong-with-CNNs&#34; target=&#34;_blank&#34;&gt;Hinton: What&amp;rsquo;s wrong with CNNs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[]&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>NLP basics - Word2vec: Skip-gram, CBOW, GloVe</title>
      <link>/post/nlp-basics---word2vec/</link>
      <pubDate>Fri, 01 Jun 2018 21:00:00 -0500</pubDate>
      
      <guid>/post/nlp-basics---word2vec/</guid>
      <description>

&lt;h2 id=&#34;table-of-content&#34;&gt;Table of Content&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#overview&#34;&gt;Overview&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#basic-algorithm&#34;&gt;Basic Algorithm&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#skip-gram&#34;&gt;Skip-Gram&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#intuition&#34;&gt;Intuition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#implementation-details&#34;&gt;Implementation details&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#cbow&#34;&gt;CBOW&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#co-occurrence-models&#34;&gt;Co-occurrence Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#glove&#34;&gt;GloVe&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#evaluation-metrics&#34;&gt;Evaluation Metrics&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#limitation&#34;&gt;Limitation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#reference-materials&#34;&gt;Reference Materials&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;overview&#34;&gt;Overview&lt;/h1&gt;

&lt;p&gt;First of all, we need to figure out how do we represent the meaning of a word. A common solution is using WordNet: a    resource containing lists   of synonym  sets and hypernyms (&amp;ldquo;is a&amp;rdquo; relationships).&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/word2vec/WordNet.png&#34; width=700&gt;&lt;/p&gt;

&lt;p&gt;But this method is not so good. It is great as a resource but   missing nuance. Also it misses the new meaning of a word.&lt;/p&gt;

&lt;p&gt;Another way is representing words   as discrete symbols. For example, like &amp;ldquo;hotel&amp;rdquo; and &amp;ldquo;motel&amp;rdquo;. We can represent them as $hotel = [0, 0, 0, 1]$ while $motel = [0, 0, 1, 0]$. In this way, word vector dimension is equal to the number of words in vocabulary. But apparently, this method is not perfect. The word &amp;ldquo;hotel&amp;rdquo; and &amp;ldquo;motel&amp;rdquo; is quite similar in meaning, but the word vectors of them are orthogonal, which is hard to quantify their similarity.&lt;/p&gt;

&lt;p&gt;So, here comes the word2vec models. The core idea is: A word’s meaning is   given   by the words that frequently appear close-by. This is one of the most successful ideas in modern NLP. We will build a dense word vector for each word (dimension far less than the number of words in vocabulary)   so  that    the words   that    appear  in  similar contexts have similar word vectors.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1310.4546.pdf&#34; target=&#34;_blank&#34;&gt;Word2vec (Mikolov et   al. 2013)&lt;/a&gt; is  a   framework   for learning
word vectors. The idea is as follows:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;We   have    a   large   corpus  of  text&lt;/li&gt;
&lt;li&gt;Every    word    in  a   fixed   vocabulary  is  represented by  a   vector&lt;/li&gt;
&lt;li&gt;Go   through each    position    t in    the text,   which   has a   center  word c and  context (“outside”) words   o&lt;/li&gt;
&lt;li&gt;Use  the similarity  of  the word    vectors for c   and o to    calculate   the probability of  o given c   (or vice    versa)&lt;/li&gt;
&lt;li&gt;Keep adjusting   the word    vectors to  maximize    this    probability&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Here is an example of calculating the   probability of c given o:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/word2vec/wordwindow.png&#34; width=800&gt;&lt;/p&gt;

&lt;p&gt;So the total objective function is:&lt;/p&gt;

&lt;p&gt;$$L(\theta) = \prod_{t=1}^{T} \prod_{j  \in [-m, 0) \bigcup (0, m]} P(w_{t+j}|w_t;\theta)  $$&lt;/p&gt;

&lt;p&gt;Where the $T$ is corpus size, and m is the half window size. Our goal would be minimizing the negative log-likelihood:&lt;/p&gt;

&lt;p&gt;$$J(\theta) = -\frac{1}{T} \sum_{t=1}^{T} \sum_{j  \in [-m, 0) \bigcup (0, m] }  P(w_{t+j}|w_t;\theta)  $$&lt;/p&gt;

&lt;p&gt;So, the question is: how to calculate $P(w_{t+j}|w_t;\theta)$, the probability of  o given c. Here we will use the most simple one: vector inner product and softmax function to present the probability:&lt;/p&gt;

&lt;p&gt;$$P(o|c) = \frac{exp(u_o^Tv_c)}{\sum_{w \in V} exp(u_w^Tv_c)}$$&lt;/p&gt;

&lt;p&gt;Hmm&amp;hellip;Softmax again. Neural network then comes to play.&lt;/p&gt;

&lt;h1 id=&#34;basic-algorithm&#34;&gt;Basic Algorithm&lt;/h1&gt;

&lt;h2 id=&#34;skip-gram&#34;&gt;Skip-Gram&lt;/h2&gt;

&lt;h3 id=&#34;intuition&#34;&gt;Intuition&lt;/h3&gt;

&lt;p&gt;The &lt;a href=&#34;https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf&#34; target=&#34;_blank&#34;&gt;skip-gram algorithm&lt;/a&gt; is surprisingly simple and elegant. We’re going to train a simple neural network with a single hidden layer to minimizing the negative log-likelihood listed above. The input word is the &lt;strong&gt;center word&lt;/strong&gt; of windows on the text corpus, while the output word is the &lt;strong&gt;surrounding words&lt;/strong&gt;. And every word have two word vectors, $v$ for center and $u$ for context. But we are not actually using the neural work afterwards, instead just using the weight we trained in the  hidden layer. This weight matrix is actually the word vectors we want.&lt;/p&gt;

&lt;p&gt;First, say we have a vacabulary of 10,000 words. We’re going to represent an input word like &amp;ldquo;hotel&amp;rdquo; as a one-hot vector. So the one hot vector will be 10000 dimensional. The output of the network is a single vector (also 10,000 dimensional), predicting the probability that certain word is appearing nearby the input word.&lt;/p&gt;

&lt;p&gt;The architecture of skip-gram network is simple:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/word2vec/skip-gram.png&#34; width=800&gt;&lt;/p&gt;

&lt;p&gt;There is no activation function on the hidden layer neurons, but the output neurons use softmax to predict the probability of each word&amp;rsquo;s appearance in each nearby location.&lt;/p&gt;

&lt;p&gt;Since the input is a one-hot vector, it just select the matrix row corresponding to the &amp;ldquo;1&amp;rdquo;. So, this is just a simple lookup table on the weight $W$ in the above figure. &lt;strong&gt;The corresponding row in weight W is the word vector of input word&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;The word vector will be sent into the output layer to produce the probability of each word&amp;rsquo;s appearance in each location. And number of prediction head is equal to the window size - 1.&lt;/p&gt;

&lt;p&gt;The brilliance of skip-gram is that it makes sure the words frequently appearing in similar context would have similar word vectors, because the network only uses context words as ground truth. So, similar context words will certainly produce similar results.&lt;/p&gt;

&lt;h3 id=&#34;implementation-details&#34;&gt;Implementation details&lt;/h3&gt;

&lt;p&gt;The authors proposed some &lt;a href=&#34;https://arxiv.org/pdf/1310.4546.pdf&#34; target=&#34;_blank&#34;&gt;improvements&lt;/a&gt; to the original skip-gram model. There are three main modifications:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Use Word Pairs and &amp;ldquo;phrases&amp;rdquo;&lt;/li&gt;
&lt;li&gt;Subsample frequent words&lt;/li&gt;
&lt;li&gt;Negative sampling&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Use Word Pairs and “phrases”&lt;/strong&gt;: The author pointed out that some common word phrases, like &amp;ldquo;New York City&amp;rdquo;, should be represented as a single word with its own word vector. This is quite natural to understand. We often use word phrase as a word and their meanings are different when splitting them apart.&lt;/p&gt;

&lt;p&gt;The phrase detection mechanism is quite simple, too. We just need to count the number of times each combination of two words appears in the training text, and then these counts are used in an equation to determine which word combinations to turn into phrases. This equation should be related to words co-occurence and individual occurence.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Subsample frequent words&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;There are two “problems” with common words like “the”:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;When looking at word pairs, (&amp;ldquo;fox&amp;rdquo;&amp;ldquo;, &amp;ldquo;the&amp;rdquo;) doesn’t tell us much about the meaning of &amp;ldquo;fox&amp;rdquo;. &amp;ldquo;the&amp;rdquo; appears in the context of pretty much every word.&lt;/li&gt;
&lt;li&gt;We will have many more samples of (&amp;ldquo;the&amp;rdquo;, …) than we need to learn a good vector for “the”.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;So, we can subsample frequent words using the follow equation:&lt;/p&gt;

&lt;p&gt;$$P(w_i) = (\sqrt{\frac{z(w_i)}{0.001}} + 1) \times \frac{0.001}{z(w_i)}$$&lt;/p&gt;

&lt;p&gt;Where $P(w_i)$ is probability of keeping the word and $z(w_i)$ is the frequency of the word. For example, if &amp;ldquo;we&amp;rdquo; appears 100 time in a corpus of 10000 words, $z(&amp;ldquo;we&amp;rdquo;)$ should be 0.01.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Negative sampling&lt;/strong&gt;: Although the network only has three layers, but the weight size is huge because of a large vacabulary. And the nature of softmax (gradient on all input) function will result in huge amount of computation in single training step.&lt;/p&gt;

&lt;p&gt;Negative sampling addresses this by having each training sample only modify a small percentage of the weights, rather than all of them. We are instead going to randomly select just a small number of “not ground truth” words (let’s say 5) to update their weights. In the paper, the authors suggested 5-20 words for smaller datasets, and you can get away with only 2-5 words for large datasets.&lt;/p&gt;

&lt;p&gt;Here a totally random selection is not good. some rare words may have little probability to be selected. So, the sampling prob comes from an equation:&lt;/p&gt;

&lt;p&gt;$$P(w_i) = \frac{f(w_i)^{\frac{3}{4}}}{\sum_{j=0}^{n} {f(w_i)^{\frac{3}{4}}}}$$&lt;/p&gt;

&lt;p&gt;where $f(w_i)$ is the word appearance count. And the power $\frac{3}{4}$ is an empirical value, which works very well. Recent research result also proved the effectiveness of this value.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.tensorflow.org/tutorials/word2vec&#34; target=&#34;_blank&#34;&gt;Here&lt;/a&gt; is an official tutorial of word2vec on TensorFlow.&lt;/p&gt;

&lt;h2 id=&#34;cbow&#34;&gt;CBOW&lt;/h2&gt;

&lt;p&gt;In the other hand, Continuous   Bag of  Words   (CBOW) does the opposite thing compared to skip-gram. It uses the context words to predict the center word. (image &lt;a href=&#34;https://zhuanlan.zhihu.com/p/35074402&#34; target=&#34;_blank&#34;&gt;source&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/word2vec/CBOW.jpg&#34; width=800&gt;&lt;/p&gt;

&lt;p&gt;Note the weight matrix for every input is &lt;strong&gt;shared&lt;/strong&gt;. All hidden layer weights are &lt;strong&gt;added&lt;/strong&gt; and send into next layer.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Skip-Gram and CBOW difference&lt;/strong&gt;: CBOW smoothes over a lot of the distributional information (by treating an entire context as one observation). For the most part, this turns out to be a useful thing for smaller datasets. However, skip-gram treats each context-target pair as a new observation, and this tends to do better when we have larger datasets. Also, this makes the skip-gram model performs better on infrequent words.&lt;/p&gt;

&lt;h2 id=&#34;co-occurrence-models&#34;&gt;Co-occurrence Models&lt;/h2&gt;

&lt;p&gt;There are two kinds of co-occurrence models, window based and full document based models. Herem we only talk about window based models: Similar to  word2vec,   use window  around each word, and count the co-occurrence situations.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s use the example from &lt;a href=&#34;http://web.stanford.edu/class/cs224n/lectures/lecture3.pdf&#34; target=&#34;_blank&#34;&gt;cs224 lecture&lt;/a&gt; to illustrate:&lt;/p&gt;

&lt;p&gt;The example corpus contains three sentences: &amp;ldquo;I like    deep    learning.&amp;rdquo;, &amp;ldquo;I like NLP.&amp;rdquo;, &amp;ldquo;I enjoy flying.&amp;rdquo;. The co-occurrence matrix would be:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/word2vec/cooccurrence.png&#34; width=800&gt;&lt;/p&gt;

&lt;p&gt;Problems of the model is obvious. It&amp;rsquo;s bery high dimensional and may need future process. And the matrix is quite sparse, which makes the model less robust. So, further we can use some dimensionality reduction technique to reduce the dimension of our word vectors, like singular  value   decomposition. However, the computational cost of SVD is quite high($O(mn^2)$), which is not suitable for large vacabulary. The &lt;a href=&#34;http://web.stanford.edu/class/cs224n/lectures/lecture3.pdf&#34; target=&#34;_blank&#34;&gt;cs224 lecture&lt;/a&gt; lecture summarized the pros and cons of Count   based model vs direct   prediction model:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/word2vec/compare.png&#34; width=800&gt;&lt;/p&gt;

&lt;h2 id=&#34;glove&#34;&gt;GloVe&lt;/h2&gt;

&lt;p&gt;Since count based method and direct prediction method both have pros and cons, how about combine them together to make it stronger? &lt;a href=&#34;https://nlp.stanford.edu/projects/glove/&#34; target=&#34;_blank&#34;&gt;GloVe&lt;/a&gt; is a good attempt. The loss function of GloVe model is:&lt;/p&gt;

&lt;p&gt;$$J(\theta) = \frac{1}{2} \sum_{i,j=1}^{W} f(P_{ij})(u_i^Tv_j - logP_{ij})^2$$&lt;/p&gt;

&lt;p&gt;Where the $u_i$ $v_j$ is the same to skip-gram model, and $P_{ij}$ is the co-occurrent matrix. $F(x)$ is a step-like function:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/word2vec/glove-fig-1.png&#34; width=400&gt;&lt;/p&gt;

&lt;p&gt;The cut-off value $x_{max}$ is not sensitive. In experiment choose $x_{max} = 100$. Instead of direct predict center or context words, the GloVe model trains word vectors to fit the log probability of the co-occurrence matrix. The GloVe model utilizes the overall statistical information, and is also scalable for huge corpus. It can achieve good performance with small corpus. But it need to store the co-occurrence matrix, which requires a large storage space. But it is faster than skip-gram with negative sampling. So, the choice of GloVe or prediction model has a trade-off of speed and storage.&lt;/p&gt;

&lt;h2 id=&#34;evaluation-metrics&#34;&gt;Evaluation Metrics&lt;/h2&gt;

&lt;p&gt;For every machine learning model, we need a metrics to evaluate its performance. Word vectors encode the similarity of words, but how to exam their effectiveness on the encoding? Of course we can use real world tasks to do extrinsic evaluation, but it takes a long cycle to update the word vector model.&lt;/p&gt;

&lt;p&gt;One popular method utilize syntactic    analogy to evaluate word vectors. We will evaluate  word    vectors by  how well    their   cosine  distance    after   addition    captures    intuitive   semantic    and
syntactic   analogy questions. For example, given man over woman, we want the use the mapping to map king to queen. This mapping accuracy (cosine distance) is the key for evaluation. Here is an illustration:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/word2vec/evaluation.png&#34; width=600&gt;&lt;/p&gt;

&lt;p&gt;Our model should uses the differential vector of &amp;ldquo;nephew&amp;rdquo; and &amp;ldquo;niece&amp;rdquo; to find &amp;ldquo;aunt&amp;rdquo; from &amp;ldquo;uncle&amp;rdquo;. A&lt;/p&gt;

&lt;p&gt;Another way is just evaluate the word vector similarity (usually inner produce) of synonyms. Dataset like wordsim353, simlex999 should be helpful.&lt;/p&gt;

&lt;p&gt;Also, people developed normal document classification tasks to reversely reflect the quality of word vectors.&lt;/p&gt;

&lt;p&gt;But these similarity-based evaluation method is very sensitive to the choice of training data size, domain, source, and vocabulary. And the data set is too small and often does not adequately determine the quality of word vectors. Recently, people tend to learning task-specific word vectors, because it is hard to evaluate word embeddings using the same standard. In my mind, word vectors can be treated as a tool of further NLP process and it&amp;rsquo;s quality can not be evaluated directly.&lt;/p&gt;

&lt;p&gt;Here is a great paper on this problem: &lt;a href=&#34;http://www.aclweb.org/anthology/D15-1036&#34; target=&#34;_blank&#34;&gt;Evaluation methods for unsupervised word embeddings&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&#34;limitation&#34;&gt;Limitation&lt;/h1&gt;

&lt;p&gt;The limitation of word embedding model has three folds:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;For each word, we only have one word vector&lt;/strong&gt;. When certain word is a polysemy, like &amp;ldquo;book&amp;rdquo;, this model obviously fails.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Cannnot leverage the benefits of pre-trained models&lt;/strong&gt;. This means you probably need to train word vector models for your own dataset rather than using pretrained models like image classification tasks. The pretrained word vectors is a result representing the similarity of words in specific dataset rather than a feature extrator.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;No shared representations at sub-word levels&lt;/strong&gt;. For example, words like &amp;ldquo;newspaper&amp;rdquo; is assembled by two word, and their meaning is a combination of these two words. Like &amp;ldquo;unhappy&amp;rdquo;, the meaning is determine by &amp;ldquo;happy&amp;rdquo; and its prefix. But word vector model does not take this into account. I believe the word embedding model should be better with shared representation for sub-word.&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;reference-materials&#34;&gt;Reference Materials&lt;/h1&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;http://web.stanford.edu/class/cs224n/syllabus.html&#34; target=&#34;_blank&#34;&gt;Stanford cs224n lectures&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/&#34; target=&#34;_blank&#34;&gt;Word2Vec Tutorial - The Skip-Gram Model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1310.4546.pdf&#34; target=&#34;_blank&#34;&gt;Distributed Representations of Words and Phrases and their Compositionality&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1301.3781.pdf&#34; target=&#34;_blank&#34;&gt;Efficient Estimation of Word Representations in Vector Space&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://nlp.stanford.edu/projects/glove/&#34; target=&#34;_blank&#34;&gt;GloVe Model&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Going Deeper in Batch Normalization</title>
      <link>/post/going-deeper-into-batch-normalization/</link>
      <pubDate>Thu, 17 May 2018 21:00:00 -0500</pubDate>
      
      <guid>/post/going-deeper-into-batch-normalization/</guid>
      <description>

&lt;h3 id=&#34;table-of-contents-optimization&#34;&gt;Table of contents/optimization&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#interpretation-and-advantage-of-batch-norm&#34;&gt;Interpretation and Advantage of Batch Norm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#algorithm-and-implementation&#34;&gt;Algorithm and implementation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#improvements-and-alternatives&#34;&gt;Improvements and Alternatives&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#batch-norm-fused-with-convolution&#34;&gt;Batch norm fused with Convolution&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#layer-normalization&#34;&gt;Layer normalization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#instance-normalization&#34;&gt;Instance Normalization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#group-normalization&#34;&gt;Group Normalization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#other-normalization-techniques-&#34;&gt;Other normalization techniques*&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#reference-materials-&#34;&gt;Reference Materials:&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This article will thoroughly explain batch normalization in a simple way.
I wrote this article after getting failed an interview because of detailed batchnorm related question.
I will start with why we need it, how it works, then how to fuse it into conv layer, and finally how to implement it in tensorflow.&lt;/p&gt;

&lt;p&gt;Here is the original paper about batch normalization on Arxiv:&lt;br /&gt;
&lt;a href=&#34;https://arxiv.org/abs/1502.03167&#34; target=&#34;_blank&#34;&gt;Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;interpretation-and-advantage-of-batch-norm&#34;&gt;Interpretation and Advantage of Batch Norm&lt;/h3&gt;

&lt;p&gt;Of course, batch norm is used to normalize the input for certain layer. We can think it in this way: if some of our input image have a scale between 0-1 while others are
between 1-1000. It is better to normalize them before training. We can apply the same idea to the input of every layer input.
There are several advantages to use batch norm:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;span class=&#34;markup-quote&#34;&gt;Batch norm can reduce the covariance shift&lt;/span&gt;. For example, we train a model to classify cat and flowers.
And the training data of cat are all black cats. In this way, the model won&amp;rsquo;t work because it can only
classify the distribution of black cat and flowers. What batch norm does is to reduce this kind of error and make the
input shift, like reduce the difference between black cat and other cats. And the same thing also applies to
every layer in the neural network. Batch norm can reduce the shift around of previous output and make
the training of next layers easier.&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;markup-quote&#34;&gt;Batch norm can remove the linear interactions in output&lt;/span&gt;. In this way, linear layers would be useless, because they cano only have effect on
linear component.  In a deep neural network with nonlinearactivation functions, the lower layers can perform nonlinear transformations of the data, so they remain useful.
Batch normalization acts to standardize only the mean and variance of each unit in order to stabilize learning, but it allows therelationships
between units and the nonlinear statistics of a single unit to change.&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;markup-quote&#34;&gt;Batch normalization can greatly speed up training process&lt;/span&gt;. Batch normalization accelerates training by requiring less iterations to
converge to a given loss value. This can be done by using higher learning rates, but with smaller learning rates you can still see an improvement.&lt;br /&gt;
Batch normalization also makes the optimization problem &amp;ldquo;easier&amp;rdquo;, as minimizing the covariate shift avoid lots of plateaus where the loss stagnates
or decreases slowly. It can still happen but it is much less frequent.&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;markup-quote&#34;&gt;Batch norm also has some regularization effect&lt;/span&gt;. Every mini-batch is a biased sample from the total dataset.
When doing batch norm, we will subtract mean and divide it by variance. This can also be treated as add
noise to data. Similar to regularization techniques like dropout, network can gain some regularization
from this. But this effect is quite minor.&lt;br /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;algorithm-and-implementation&#34;&gt;Algorithm and implementation&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;Here is the algorithm diagram batch norm.
&lt;img src=&#34;/img/batch_norm_fp.png&#34; alt=&#34;Batch norm algorithm&#34; /&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Nothing fancy but extremely practical algorithm. One thing has to mention, &lt;span class=&#34;markup-quote&#34;&gt;the learnable variables
$\gamma$ and $\beta$. The deep learning book gives clear explaination about this&lt;/span&gt;. Normalizing the mean and deviation of a unit can
reduce the expressive power of a neural network. In this way, it is common to multiply the normalized result with $\gamma$ and add $\beta$. For exmaple,
if we have sigmoid activation afterwards, the network may don&amp;rsquo;t want the output lies in the near linear part of sigmoid. With $\gamma$ and $\beta$,
the network has the freedom to shift whatever it wants. Another thing to note is that batch norm in CNN is different. Instead of calculate mean and variance in size $[H, W, C]$, it also averages over H and W, and the mean and variance has C dimensions, because CNN weight are shared cross H and W.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;This is a new parametrization can represent the same family of functions of the input as the old parametrization, but the new parametrization
 has different learning dynamics. In the old parametrization, the mean of H was determined by a complicated interaction between the parameters
 in the layers below H. In the new parametrization, the mean of $y=\gamma x + \beta$ is determined solely by $\gamma$. The new parametrization
 is much easier to learn with gradient descent.    &amp;ndash; Deep Learning Book&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;span class=&#34;markup-quote&#34;&gt;At test time, we need the mean and variance directly. So, the method is using an exponentially weighted average across mini-batches. &lt;/span&gt;
We have $ x_1, x_2, &amp;hellip; ,x_i $ outputs from different mini-batches. What we do is put expotential
weight on previous processed mini-batches. The calculation is quite simple:
$$Mean_{running} = \mu * Mean_{running} + (1.0 - \mu) * Mean_{sample}$$
$$Var_{running} = \mu * Var_{running} + (1.0 - \mu) * Var_{sample}$$
And we use running mean and var to calculate batchnorm.&lt;br /&gt;
Alternatively, we can first calculate the total mean and variance of total test dataset. But
this exponential weighted method are more popular in practice.&lt;/p&gt;

&lt;p&gt;And last but not least, the code for forward and backward pass(from my cs231n homework):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def batchnorm_forward(x, gamma, beta, bn_param):
    &amp;quot;&amp;quot;&amp;quot;
    Forward pass for batch normalization.
    During training the sample mean and (uncorrected) sample variance are
    computed from minibatch statistics and used to normalize the incoming data.
    During training we also keep an exponentially decaying running mean of the
    mean and variance of each feature, and these averages are used to normalize
    data at test-time.
    At each timestep we update the running averages for mean and variance using
    an exponential decay based on the momentum parameter:
    running_mean = momentum * running_mean + (1 - momentum) * sample_mean
    running_var = momentum * running_var + (1 - momentum) * sample_var
    Note that the batch normalization paper suggests a different test-time
    behavior: they compute sample mean and variance for each feature using a
    large number of training images rather than using a running average. For
    this implementation we have chosen to use running averages instead since
    they do not require an additional estimation step; the torch7
    implementation of batch normalization also uses running averages.
    Input:
    - x: Data of shape (N, D)
    - gamma: Scale parameter of shape (D,)
    - beta: Shift paremeter of shape (D,)
    - bn_param: Dictionary with the following keys:
      - mode: &#39;train&#39; or &#39;test&#39;; required
      - eps: Constant for numeric stability
      - momentum: Constant for running mean / variance.
      - running_mean: Array of shape (D,) giving running mean of features
      - running_var Array of shape (D,) giving running variance of features
    Returns a tuple of:
    - out: of shape (N, D)
    - cache: A tuple of values needed in the backward pass
    &amp;quot;&amp;quot;&amp;quot;
    mode = bn_param[&#39;mode&#39;]
    eps = bn_param.get(&#39;eps&#39;, 1e-5)
    momentum = bn_param.get(&#39;momentum&#39;, 0.9)

    N, D = x.shape
    running_mean = bn_param.get(&#39;running_mean&#39;, np.zeros(D, dtype=x.dtype))
    running_var = bn_param.get(&#39;running_var&#39;, np.zeros(D, dtype=x.dtype))

    out, cache = None, None
    if mode == &#39;train&#39;:
       
        sample_mean = np.mean(x, axis=0)
        sample_var = np.var(x, axis=0)
        x_stand = (x - sample_mean.T) / np.sqrt(sample_var.T + eps)

        out = x_stand * gamma + beta

        running_mean = momentum * running_mean + (1.0 - momentum) * sample_mean
        running_var = momentum * running_var + (1.0 - momentum) * sample_var

        cache = (sample_mean, sample_var, x_stand, x, gamma, beta, eps)

       
    elif mode == &#39;test&#39;:
        

        x_stand = (x - running_mean) / np.sqrt(running_var)
        out = x_stand * gamma + beta

        
    else:
        raise ValueError(&#39;Invalid forward batchnorm mode &amp;quot;%s&amp;quot;&#39; % mode)

    # Store the updated running means back into bn_param
    bn_param[&#39;running_mean&#39;] = running_mean
    bn_param[&#39;running_var&#39;] = running_var

    return out, cache

&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def batchnorm_backward(dout, cache):
    &amp;quot;&amp;quot;&amp;quot;
    Backward pass for batch normalization.
    For this implementation, you should write out a computation graph for
    batch normalization on paper and propagate gradients backward through
    intermediate nodes.
    Inputs:
    - dout: Upstream derivatives, of shape (N, D)
    - cache: Variable of intermediates from batchnorm_forward.
    Returns a tuple of:
    - dx: Gradient with respect to inputs x, of shape (N, D)
    - dgamma: Gradient with respect to scale parameter gamma, of shape (D,)
    - dbeta: Gradient with respect to shift parameter beta, of shape (D,)
    &amp;quot;&amp;quot;&amp;quot;
    dx, dgamma, dbeta = None, None, None
    

    sample_mean, sample_var, x_stand, x, gamma, beta, eps = cache
    N, D = dout.shape

    dbeta = np.sum(dout, axis=0)
    dgamma = np.sum(x_stand * dout, axis=0)
    dx = (1. / N) * gamma * (sample_var + eps)**(-1. / 2.) * (
         N * dout - np.sum(dout, axis=0) - (x - sample_mean) * (
         sample_var + eps)**(-1.0) * np.sum(dout * (x - sample_mean), axis=0))


    return dx, dgamma, dbeta

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;During training, the moving_mean and moving_variance need to be updated.
By default the update ops are placed in tf.GraphKeys.UPDATE_OPS, so they need to be added as a dependency to the train_op.
So, the template is:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;batchnorm = tf.layers.batch_normalization(x, training=training)
update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)
with tf.control_dependencies(update_ops):
    train_op = optimizer.minimize(loss)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And also tensorflow official evaluate function (classification model zoo):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, first_clone_scope)
with tf.control_dependencies([update_op]):
      train_tensor = tf.identity(total_loss, name=&#39;train_op&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;improvements-and-alternatives&#34;&gt;Improvements and Alternatives&lt;/h3&gt;

&lt;h4 id=&#34;batch-norm-fused-with-convolution&#34;&gt;Batch norm fused with Convolution&lt;/h4&gt;

&lt;p&gt;This is the question makes me fail the interview. Actually, there is no magic stuff about fused batch normalization. Just mathematically
calculate two layers together and treat them as one layer in forward and backward pass:&lt;/p&gt;

&lt;p&gt;Conv:
$$convout(N_i,C_{out})=bias(C_{out})+\sum_{k=1}^{C} weight(C_{out},k)*input(N_i,k)$$
Batch Norm:
$$bnout(N_i,C_{out})=(convout(N_i,C_{out})-\mu)/\sqrt{((\epsilon + \sigma)^2)}$$
$$bnout(N_i,C_{out})=\gamma*bnout(N_i,C_{out})+\beta$$
Here, $convout(N_i,C_{out})$ means the $N_ith$ sample in the $C_{out}$ channel. Same notation applies to input and bnout.
$weight(C_{out},k)$ is the conv kernel corresponding to $C_{out}$. And $\epsilon, \sigma, \mu, \gamma, \beta$ are the same as above.&lt;br /&gt;
After fusion, the total calculation becomes:&lt;/p&gt;

&lt;p&gt;$$out(N_i,C_{out})=\gamma*(bias(C_{out})+\sum_{k=1}^{C} weight(C_{out},k)*input(N_i,k)/\sqrt{((\epsilon + \sigma)^2)}+\beta$$
In this way, the weight and bias of fused conv layer is:&lt;/p&gt;

&lt;p&gt;$$bias = \gamma*(bias(C_{out})$$
$$weight = weight(C_{out},k)/\sqrt{((\epsilon + \sigma)^2)}+\beta$$&lt;/p&gt;

&lt;p&gt;We can use the fused bias and weight in previous conv layers. We can drop the intermediate result between conv and batch norm using this method,
which can save up to 50% memory and a minor increase of training time.&lt;/p&gt;

&lt;h4 id=&#34;layer-normalization&#34;&gt;Layer normalization&lt;/h4&gt;

&lt;p&gt;Just understand from its name, layer normalization. Instead of using a batch of data to produce $\mu $ and $\sigma$ at every location,
It uses all the neuron activations in one layer to produce $\mu$ and $\sigma$.
This method is especially useful when not using mini-batch like RNN, where batch norm cannot be used. But its performance in convs layers are not as good
as batch norm.&lt;/p&gt;

&lt;h4 id=&#34;instance-normalization&#34;&gt;Instance Normalization&lt;/h4&gt;

&lt;p&gt;Just simply replace all batch normalization layers with instance normalization layers. Batch normalization normalizes using the information from the whole batch, while instance normalization normalizes each feature map on its own.&lt;br /&gt;
Formula comparasion:&lt;br /&gt;
&lt;img src=&#34;/img/instancenorm.jpg&#34; alt=&#34;instance norm&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;group-normalization&#34;&gt;Group Normalization&lt;/h4&gt;

&lt;p&gt;While batch-norm demonstrates it effectiveness in a variety of fields including computer vision, natural language processing, speech processing, robotics, etc., batch-norm&amp;rsquo;s performance substantially decrease when the training batch size become smaller, which limits the gain of utilizing batch-norm in a task requiring small batches constrained by memory consumption.&lt;/p&gt;

&lt;p&gt;Instead of normalizing along the batch dimension, GN divides the channels into groups and computes within each group the mean and variance. Therefore, GN&amp;rsquo;s computation is independent of batch sizes, and so does its accuracy.&lt;/p&gt;

&lt;p&gt;Here is a diagram for all these Group techniques:&lt;br /&gt;
&lt;img src=&#34;/img/GN.jpg&#34; alt=&#34;GN&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The difference is obvious. Batch norm uses data in the same batch to calculate $\mu $ and $\sigma$; Layer norm uses all the neurons in single layer; Instance norm uses all in data in the same batch and channel; Group norm uses only a small piece of batch.&lt;/p&gt;

&lt;h4 id=&#34;other-normalization-techniques&#34;&gt;Other normalization techniques*&lt;/h4&gt;

&lt;p&gt;Recurrent Batch Normalization (BN) (Cooijmans, 2016; also proposed concurrently by Qianli Liao &amp;amp; Tomaso Poggio, but tested on Recurrent ConvNets,
instead of RNN/LSTM): Same as batch normalization. Use different normalization statistics for each time step. You need to store a set of mean and
standard deviation for each time step.&lt;/p&gt;

&lt;p&gt;Batch Normalized Recurrent Neural Networks (Laurent, 2015): batch normalization is only applied between the input and hidden state, but not between
hidden states. i.e., normalization is not applied over time.&lt;/p&gt;

&lt;p&gt;Streaming Normalization (Liao et al. 2016) : it summarizes existing normalizations and overcomes most issues mentioned above. It works well with
ConvNets, recurrent learning and online learning (i.e., small mini-batch or one sample at a time):&lt;/p&gt;

&lt;p&gt;Weight Normalization (Salimans and Kingma 2016): whenever a weight is used, it is divided by its L2 norm first, such that the resulting weight has
L2 norm 1. That is, output y=x*(w/|w|), where x and w denote the input and weight respectively. A scalar scaling factor g is then multiplied to the
output y=y*g. But in my experience g seems not essential for performance (also downstream learnable layers can learn this anyway).&lt;/p&gt;

&lt;p&gt;Cosine Normalization (Luo et al. 2017): weight normalization is very similar to cosine normalization, where the same L2 normalization is applied
to both weight and input: y=(x/|x|)*(w/|w|). Again, manual or automatic differentiation can compute appropriate gradients of x and w.&lt;/p&gt;

&lt;h3 id=&#34;reference-materials&#34;&gt;Reference Materials:&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;http://www.deeplearningbook.org/contents/optimization.html&#34; target=&#34;_blank&#34;&gt;Deep Learning Book, Chapter 8.7.1&lt;/a&gt;&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow&#34; target=&#34;_blank&#34;&gt;Stackoverflow: How could I use Batch Normalization in TensorFlow?&lt;/a&gt;&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://sifaka.cs.uiuc.edu/jiang4/domain_adaptation/survey/node8.html&#34; target=&#34;_blank&#34;&gt;Explaination on Covariance Shift&lt;/a&gt;&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.tensorflow.org/api_docs/python/tf/nn/batch_normalization&#34; target=&#34;_blank&#34;&gt;Tensorflow batch normalization docs&lt;/a&gt;&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://datascience.stackexchange.com/questions/12956/paper-whats-the-difference-between-layer-normalization-recurrent-batch-normal&#34; target=&#34;_blank&#34;&gt;Various Normalization Techniques in Deep Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1607.08022.pdf&#34; target=&#34;_blank&#34;&gt;Instance Normalization&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Amazing GAN - Wasserstein GAN</title>
      <link>/post/amazing-gan---wasserstein-gan/</link>
      <pubDate>Sat, 17 Feb 2018 21:00:00 -0600</pubDate>
      
      <guid>/post/amazing-gan---wasserstein-gan/</guid>
      <description>

&lt;h2 id=&#34;table-of-content&#34;&gt;Table of Content&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#kl-divergence-and-js-divergence&#34;&gt;KL divergence and JS divergence&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#generative-adversarial-networks&#34;&gt;Generative Adversarial Networks&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#global-optimal-loss&#34;&gt;Global optimal loss&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#problem-with-vanilla-gans&#34;&gt;Problem with Vanilla GANs&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#gradient-vanishing&#34;&gt;Gradient Vanishing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#mode-collapse&#34;&gt;Mode Collapse&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#improved-training-of-gans&#34;&gt;Improved Training of GANs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#wasserstein-gan&#34;&gt;Wasserstein GAN&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#earth-mover-distance&#34;&gt;Earth Mover distance&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#comparasion-between-em-distance-and-kl-js-divergence&#34;&gt;Comparasion between EM distance and KL/JS divergence&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#lipschitz-continuity&#34;&gt;Lipschitz continuity&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#modified-algorithm&#34;&gt;Modified Algorithm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#wasserstein-gan-with-gradient-penalty&#34;&gt;Wasserstein GAN with gradient penalty&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#disadvantages-of-gradient-clipping-in-wgan&#34;&gt;Disadvantages of gradient clipping in WGAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#gradient-penalty&#34;&gt;Gradient Penalty&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#reference-materials&#34;&gt;Reference Materials&lt;/a&gt;
&lt;br /&gt;
&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;kl-divergence-and-js-divergence&#34;&gt;KL divergence and JS divergence&lt;/h2&gt;

&lt;p&gt;Before diving into details, let first review two very important metrics to quantify the similarity of two probability distributions:
&lt;a href=&#34;https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence&#34; target=&#34;_blank&#34;&gt;Kullback-Leibler Divergence&lt;/a&gt; and &lt;a href=&#34;https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence&#34; target=&#34;_blank&#34;&gt;Jensen-Shannon Divergence&lt;/a&gt;.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Kullback-Leibler Divergence measures the divergence of probability distribution p(x) to q(x):
$$KL(P||Q) = \int_x P(x)log\frac{P(x)}{Q(x)}dx$$
KL(P||Q) achieves its minimum zero when P(x) and Q(x) are the same everywhere.&lt;br /&gt;
KL divergence is widely used as a metrics to measure the similarity between two distributions. But according to its formula, its is asymmetric. Also, due to the rapid decreasing of logrithm function, KL divergence put to much weight when P(x) is near zero. This can cause some buggy result in real world measurement.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Jensen-Shannon Divergence. JS divergence is based on KL divergence and it is symmetric.&lt;br /&gt;
$$JS(P||Q) = \frac{1}{2} KL(P||\frac{P+Q}{2}) + \frac{1}{2} KL(Q||\frac{P+Q}{2})$$&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Here is a plot of KL and JS divergence of two normal distributions: N(0, 1) and N(1, 1). Image resource &lt;a href=&#34;https://lilianweng.github.io/lil-log/2017/08/20/from-GAN-to-WGAN.html&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;
&lt;img src=&#34;/img/KL_JS_divergence.png&#34; alt=&#34;KL-JS&#34; /&gt;&lt;/p&gt;

&lt;p&gt;As shown in the plot, JS divergence are symmetric while KL divergence is asymmetric. People believe the success of GANs comes from replacing the traditional maximum likelihood with symmetric similarity measure, JS divergence.&lt;/p&gt;

&lt;h2 id=&#34;generative-adversarial-networks&#34;&gt;Generative Adversarial Networks&lt;/h2&gt;

&lt;p&gt;Original GANs consists of two networks:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Generator. It receive random samples and synthesize fake images feeding into discriminator. This random sample brings a potential output diversity.&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Discriminator. It receive the real dataset images and the fake images from generator. It works as a critic to evaluate the probability of input image coming from dataset and from generator.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This is a diagram showing how GAN works:
&lt;img src=&#34;/img/GANs.png&#34; alt=&#34;GAN&#34; /&gt;&lt;/p&gt;

&lt;p&gt;In one hand, we want the discriminator&amp;rsquo;s output probability over real data to be higher by maximizing $\mathbb{E}_{x \sim p_{data}(x)}[logD(x)]$; In another hand, we want the discriminator&amp;rsquo;s output probability over fake data from generator to be lower by minimizing $\mathbb{E}_{z \sim p_(z)}[log(1-D(G(z)))]$.&lt;/p&gt;

&lt;p&gt;And for the generator, we want it to fool the discriminator by minimizing $\mathbb{E}_{z \sim p_(z)}[log(1-D(G(z)))]$.&lt;/p&gt;

&lt;p&gt;So, the overall process of GAN training is obvious:
$$\min_{G} \max_{D} L(G, D) =  [\mathbb{E}_{x \sim p_{data}(x)}[logD(x)] + \mathbb{E}_{z \sim p_(z)}[log(1-D(G(z)))]]$$&lt;/p&gt;

&lt;p&gt;Overall, it is a minimax game between generator and discriminator. The main concern in training procedure is keeping the G and D evolving at the same speed.&lt;/p&gt;

&lt;h3 id=&#34;global-optimal-loss&#34;&gt;Global optimal loss&lt;/h3&gt;

&lt;p&gt;The global minimum of the training criterion $L(G, D)$ is achieved if and only if
$p_g = p_{data}$. Proof of are in the original &lt;a href=&#34;https://arxiv.org/pdf/1406.2661.pdf&#34; target=&#34;_blank&#34;&gt;GAN paper&lt;/a&gt;.&lt;br /&gt;
First, we need to find the optimal solution for D when G is fixed. (Sorry, there is a problem in my equation alignment)&lt;/p&gt;

&lt;p&gt;$$L(G, D) = \int_x p_{data}(x)logD(x)dx +  \int_z p(z)log(1-D(G(z))dz $$
         $$ = \int_x (p_{data}(x)logD(x) + p_g(x)log(1-D(x)))dx $$&lt;/p&gt;

&lt;p&gt;Assume:
$$F(x) = p_{data}(x)logD(x) + p_g(x)log(1-D(x))$$
Take the derivative over $D(x)$:
$$\frac{d f(x)}{dx} = \frac{p_{data}(x)}{D(x)} + \frac{p_g(x)}{1-D(x)} = 0$$
Solve this equation, easily get :
$$D^{\star}(x) = \frac{p_{data}(x)}{p_{data}(x)+p_g(x)}$$&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;When system is trained well, $p_{data}(x)$ and $p_g(x)$ should be similar, $D^{\star}(x) = \frac{1}{2}$.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;When discriminator is optimal, the loss function becomes:&lt;/p&gt;

&lt;p&gt;$$L(G, D) = \int_x p_{data}(x)logD(x)dx +  \int_z p(z)log(1-D(G(z))dz $$&lt;/p&gt;

&lt;p&gt;$$ = \int_x (p_{data}(x)logD(x) + p_g(x)log(1-D(x)))dx $$
$$ = \int_x p_{data}(x)log(\frac{p_{data}(x)}{p_{data}(x)+p_g(x)}) + p_g(x)log(\frac{p_g(x)}{p_{data}(x)+p_g(x)})dx $$
$$ = -log(4) + KL(p_{data} || \frac{p_{data}+p_g}{2}) + KL(p_g || \frac{p_{data}+p_g}{2})$$
$$ = -log(4) + 2* JS(p_{data} || p_g)$$&lt;/p&gt;

&lt;p&gt;So, the loss function of GAN quantify the JS divergence of $p_{data}$ and $p_g$. The optimal value is $- log(4)$ when $p_{data} = p_g$.&lt;/p&gt;

&lt;h2 id=&#34;problem-with-vanilla-gans&#34;&gt;Problem with Vanilla GANs&lt;/h2&gt;

&lt;h3 id=&#34;gradient-vanishing&#34;&gt;Gradient Vanishing&lt;/h3&gt;

&lt;p&gt;The loss function for training G is: $\mathbb{E}_{z \sim p(z)}[log(1-D(G(z)))]$. But in the early stage of training, discriminator can be very confident in detecting results from G, $D(G(z))$ is always 0. In this way, the gradient to update G vanishes.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/GAN_gradient_vanishing.png&#34; alt=&#34;gradient vanishing&#34; /&gt;&lt;/p&gt;

&lt;p&gt;With the generator fixed we
train a discriminator from scratch and measure the gradients with the original cost function. We see
the gradient norms decay quickly, in the best case 5 orders of magnitude after 4000 discriminator
iterations. Note the logarithmic scale.&lt;/p&gt;

&lt;p&gt;We can use an alternative loss function for G: $\mathbb{E}_{z \sim p(z)}[-log(D(G(z)))]$. Instead of minimizing, let G maximizing the logprobability of the discriminator being mistaken. It is heuristically motivated that generator can still
learn even when discriminator successfully rejects
all generator samples, but not theoretically
guaranteed.&lt;/p&gt;

&lt;p&gt;But this will result in gradient unstable issue because the nature of logrithm function.&lt;/p&gt;

&lt;p&gt;So, the training of GAN faces a dilemma:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;If discriminator is trained badly, it fails to provide correct gradient to update generator.&lt;/li&gt;
&lt;li&gt;If discriminator is trained well, it will be too confident and give near 0 score to generator result, which kills the gradient
in the generator.&lt;br /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;To sum up, the difficulty to train a GAN is how to keep G and D in the same pace. This is quite hard to control in practice without a metrics to quantify it.&lt;/p&gt;

&lt;h3 id=&#34;mode-collapse&#34;&gt;Mode Collapse&lt;/h3&gt;

&lt;p&gt;Mode collapse is when the generator generates a limited diversity of samples, or even the same sample, regardless of the input. The main reason also comes from the nature of loss function.  It
is not equally treated when G generates a unreal
sample and when G fails to generate real sample.&lt;/p&gt;

&lt;p&gt;Without any guidance to ensure the diversity of generator, G only care about how to fool discriminator. Once get a good sample that successfully fools discriminator, it will produce this kind of samples as many as possible to optimize the loss function. When discriminator finally realized the mistake during its training, the generator can easily find another perfect example to fool the discriminator and produce a lot of similar samples. This becomes an endless circle between G and D updates. And the loss function value in this process  will have unnecessary oscillations.&lt;/p&gt;

&lt;p&gt;One method to compensate this is putting regularization on the diversity of generator, forcing it to produce various samples. In practice, this method is still not good enough.&lt;/p&gt;

&lt;p&gt;Here is some result of GAN mode collapse in LSUN dataset:
&lt;img src=&#34;/img/mode_collapse.png&#34; alt=&#34;mode collapse&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;improved-training-of-gans&#34;&gt;Improved Training of GANs&lt;/h2&gt;

&lt;p&gt;The following improvement are proposed to help stabilize and improve the training of GANs. These comes from this paper:
&lt;a href=&#34;http://papers.nips.cc/paper/6125-improved-techniques-for-training-gans.pdf&#34; target=&#34;_blank&#34;&gt;Improved Techniques for Training GANs&lt;/a&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Feature Matching&lt;br /&gt;
Feature matching addresses the instability of GANs by specifying a new objective for the generator
that prevents it from overtraining on the current discriminator. Specifically, we train the generator to match the expected value of the
features on an intermediate layer of the discriminator.&lt;br /&gt;
Our new objective for the generator is defined as: $| \mathbb{E}_{x \sim p_r} f(x) - \mathbb{E}_{z \sim p_z(z)}f(G(z)) |_2^2 $, where $f(x)$ denote activations on an intermediate layer of the discriminator.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Mini-batch Discrimination&lt;br /&gt;
The concept of minibatch discrimination is quite general: any discriminator model that looks
at multiple examples in combination, rather than in isolation, could potentially help avoid collapse
of the generator.&lt;br /&gt;
In one minibatch, we approximate the closeness between every pair of samples, $c(x_i, x_j)$, and get the overall status of one data point by summing up how close it is to other samples in the same batch, $o(x_i) = \sum_{j} c(x_i, x_j)$. Then $o(x_i)$ is explicitly added to the input of the model.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Historical averaging&lt;br /&gt;
When applying this technique, we modify each player&amp;rsquo;s cost to include a term $||\theta - \frac{1}{t} \sum_{i=1}^t \theta[i]||^2$
where $\theta[i]$ is the value of the parameters at past time i. The historical average of the parameters can
be updated in an online fashion so this learning rule scales well to long time series.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;One-sided label smoothing&lt;br /&gt;
Replaces the 0 and 1 targets for a classifier with smoothed values, like .9 or .1, and was
recently shown to reduce the vulnerability of neural networks to adversarial examples&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Virtual batch normalization&lt;br /&gt;
Each example x is normalized based on
the statistics collected on a reference batch of examples that are chosen once and fixed at the start
of training. The reference batch is normalized using only its own statistics.&lt;br /&gt;
VBN is
computationally expensive because it requires running forward propagation on two minibatches of
data, so we use it only in the generator network.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;wasserstein-gan&#34;&gt;Wasserstein GAN&lt;/h2&gt;

&lt;h3 id=&#34;earth-mover-distance&#34;&gt;Earth Mover distance&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Earth_mover%27s_distance&#34; target=&#34;_blank&#34;&gt;Earth Mover distance(EM distance, Wasserstein distance)&lt;/a&gt; is another metrics on the similarity:
$$ W(p_r, p_g) = \inf_{\gamma \sim \Pi(p_r, p_g)} \mathbb{E}_{(x, y) \sim \gamma}[| x-y |] $$&lt;/p&gt;

&lt;p&gt;Looks like a very complex formula, but actually quite simple. Intuitively, $\gamma(x, y)$ indicates how much &amp;lsquo;mass&amp;rsquo;
must be transported from x to y in order to transform the distributions $p_g$
into the distribution $p_r$. The EM distance then is the &amp;lsquo;cost&amp;rsquo; of the optimal
transport plan. PS. Notice it is the expection over $| x-y |$, the total movement should be:&lt;/p&gt;

&lt;p&gt;$$ \sum_{x, y} \gamma(x, y) | x-y | = \mathbb{E}_{x, y \sim \gamma} | x-y | $$&lt;/p&gt;

&lt;p&gt;For example, we get two distributions:&lt;/p&gt;

&lt;p&gt;$$p_r : p_r(0) = \frac{1}{4}, p_r(1) = \frac{1}{4}, p_r(2) = \frac{1}{2}$$&lt;/p&gt;

&lt;p&gt;And:&lt;/p&gt;

&lt;p&gt;$$p_g : p_g(0) = \frac{1}{2}, p_g(1) = \frac{1}{2}, p_g(2) = 0$$&lt;/p&gt;

&lt;p&gt;The optimal plan to move from $p_r$ to $p_g$ should be move $\frac{1}{4}$ from $p_r(2)$ to $p_r(1)$ and $p_r(0)$. So, the EM distance of this two distribution is $\frac{1}{4} * |2-0| + \frac{1}{4} * |2-1| = \frac{3}{4}$.&lt;/p&gt;

&lt;h3 id=&#34;comparasion-between-em-distance-and-kl-js-divergence&#34;&gt;Comparasion between EM distance and KL/JS divergence&lt;/h3&gt;

&lt;p&gt;The example from Wasserstein GAN paper is pretty good. Let $ Z \sim U[0, 1] $ is a uniform distribution on unit interval.
Let $P_0$ be the distribution of $(0, Z) \in R^2 $
(0 on the x-axis and
the random variable Z on the y-axis), uniform on a straight vertical line passing
through the origin. Now let $g_{\theta}(z) = (\theta, z)$ with $\theta$ a single real parameter. It is easy
to see that in this case when $\theta \neq 0$, (if $\theta = 0$, all these are zero):&lt;/p&gt;

&lt;p&gt;$$EM(P_0, P_{\theta}) = |\theta| $$
$$JS(P_0, P_{\theta}) = log2 $$
$$KL(P_0, P_{\theta}) = \infty $$&lt;/p&gt;

&lt;p&gt;KL gives us inifity when two distributions are non-overlapped. And this situation is quite normal in high dimension space. The value of JS has sudden jump, not differentiable at $\theta=0$. Only Wasserstein metric provides a smooth measure, which is super helpful to provide stable gradient to in training.&lt;/p&gt;

&lt;h3 id=&#34;lipschitz-continuity&#34;&gt;Lipschitz continuity&lt;/h3&gt;

&lt;p&gt;The definition of Lipschitz continuity is:
A real-valued function $f: \mathbb{R} \rightarrow \mathbb{R}$ is called $K$-Lipschitz continuous if there exists a real constant $K \geq 0$ such that, for all $x_1, x_2 \in \mathbb{R}$,&lt;/p&gt;

&lt;p&gt;$$\lvert f(x_1) - f(x_2) \rvert \leq K \lvert x_1 - x_2 \rvert$$&lt;/p&gt;

&lt;p&gt;if a function is differentiable everywhere, it&amp;rsquo;s derivative should be bounded in $[-K, K]$.&lt;/p&gt;

&lt;p&gt;The infimum in the earth mover distance is highly intractable. Also, it would be impossible to search all the cases to move one probability distribution to another. Here the Kantorovich-Rubinstein duality tells us that:&lt;/p&gt;

&lt;p&gt;$$ W(p_r, p_g) = \frac{1}{K} \sup_{| f |_L \leq K} \mathbb{E}_{x \sim p_r}[f(x)] - \mathbb{E}_{x \sim p_g}[f(x)] $$&lt;/p&gt;

&lt;p&gt;where the supremum is over all the 1-Lipschitz functions. If you want to know more about Kantorovich-Rubinstein duality, see this awesome &lt;a href=&#34;https://vincentherrmann.github.io/blog/wasserstein/&#34; target=&#34;_blank&#34;&gt;blog&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Suppose this function $f$ comes from a family of K-Lipschitz continuous functions, ${ f_w }$, parameterized by $w$. The above equation becomes:&lt;/p&gt;

&lt;p&gt;$$ W(p_r, p_g) = \max_{w \in W} \mathbb{E}_{x \sim p_r}[f_w(x)] - \mathbb{E}_{z \sim p_r(z)}[f_w(g_\theta(z))] $$&lt;/p&gt;

&lt;p&gt;In the modified Wasserstein-GAN, the &amp;ldquo;discriminator&amp;rdquo; model is used to learn $w$ to find a good $f_w$ and the loss function is equivalent to measure the Wasserstein distance between $p_r$ and $p_g$.&lt;/p&gt;

&lt;p&gt;In this perspective, the discriminator can be treated as a K-Lipschitz function to measure the Wasserstein distance between the distribution of $p_r$ and $p_g$. Since the Wasserstein distance is a smooth measure, the gradient should be stable and make $p_r$ closer to $p_g$.&lt;/p&gt;

&lt;h3 id=&#34;modified-algorithm&#34;&gt;Modified Algorithm&lt;/h3&gt;

&lt;p&gt;The next thing to care about is how to keep the discriminator function satisfying K-Lipschitz continuity. Wasserstein GAN adopts the most simple method, clipping the gradient into a small interval ($[-0.01, 0.01]$), and make the parameter $w$ lies in a compact space and  $f_w$ will preserve its Lipschitz continuity. The modified algorithm is as follows:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/GAN_algorithm.png&#34; alt=&#34;WGAN&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Compared to original GANs, Wasserstein GAN takes these changes:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Remove the sigmoid function at the end of Discriminator&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Remove the log function in generator and discriminator loss function&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Clip the gradient norm into an interval $[-c, c]$&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Empirically use optimizers without momentum term, like RMSprop, not Adam.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The first two modification comes from a brand-new loss function derived from Wasserstein distance. It is not a probability score anymore, so no sigmoid and logrithm is needed. And the last change comes from author&amp;rsquo;s experience, it should be a practical suggestion for training.&lt;/p&gt;

&lt;h3 id=&#34;wasserstein-gan-with-gradient-penalty&#34;&gt;Wasserstein GAN with gradient penalty&lt;/h3&gt;

&lt;p&gt;In the original Wasserstein GAN paper, the author admitted gradient clipping is a terrible idea enforce a Lipschitz constraint:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Weight clipping is a clearly terrible way to enforce a Lipschitz constraint. If the
clipping parameter is large, then it can take a long time for any weights to reach
their limit, thereby making it harder to train the critic till optimality. If the clipping
is small, this can easily lead to vanishing gradients when the number of layers is
big, or batch normalization is not used.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;So, this paper &lt;a href=&#34;https://arxiv.org/pdf/1704.00028.pdf&#34; target=&#34;_blank&#34;&gt;Improved Training of Wasserstein GANs&lt;/a&gt; proposed a new Wasserstein GAN with gradient penalty to replace gradient clipping.&lt;/p&gt;

&lt;h4 id=&#34;disadvantages-of-gradient-clipping-in-wgan&#34;&gt;Disadvantages of gradient clipping in WGAN&lt;/h4&gt;

&lt;p&gt;The author found weight clipping in WGAN leads to optimization difficulties, and that even when optimization
succeeds the resulting critic can have a pathological value surface.&lt;/p&gt;

&lt;p&gt;Their experiments use the specific form of weight constraint like hard clipping of the magnitude
of each weight, L2 norm clipping, weight normalization,
as well as soft constraints (L1 and L2 weight decay) and found that they exhibit similar problems.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Capacity underuse&lt;br /&gt;
Under a weight-clipping constraint, the authors observe that neural
network architectures try to attain their maximum gradient norm k end up learning extremely
simple functions. Also, the critic trained with weight clipping ignores higher moments of the data distribution
and instead models very simple approximations to the optimal functions.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Gradient vanishing and exploding
Without careful tuning of the clipping threshold c, the network can easily stuck in gradient vanishing and exploding.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&#34;gradient-penalty&#34;&gt;Gradient Penalty&lt;/h4&gt;

&lt;p&gt;The authors provide an alternative way to enforce the Lipschitz constraint. First, they proved that a differentiable function
is 1-Lipschtiz if and only if it has gradients with norm at most 1 everywhere. So, gradient penalty term is add to directly
constrain the gradient norm of the critic&amp;rsquo;s output with respect to its input. The new loss function is:
$$L(G, D) = \mathbb{E}_{x \sim p_r}[f_w(x)] - \mathbb{E}_{z \sim p_r(z)}[f_w(g_\theta(z))] + \lambda \mathbb{E}_{\hat x \sim \mathbb{P}_{\hat x}}[(||\nabla{\hat{x}}D(\hat x)||_2 - 1)^2]$$&lt;/p&gt;

&lt;p&gt;So, they use a soft version constraint, putting penalty on gradient norm for random samples $\hat x \sim \mathbb{P}_{\hat x}$, where $\mathbb{P}_{\hat x}$ is drawn from a straight line between pairs of points sampled from the data distribution $\mathbb{P}_r$ and the generator distribution $\mathbb{P}_g$. Note the hyperparameter $\lambda = 10$&lt;/p&gt;

&lt;p&gt;The main reason they do this is that enforcing the unit gradient norm constraint everywhere is intractable, enforcing it only along these straight lines seems sufficient and experimentally results in good performance.&lt;/p&gt;

&lt;p&gt;Also, the batch norm layer can make this penalty invalid, so WGAN-GP simply remove the batch norm layers. And WGAN-GP takes two side penalty. Instead of forcing the gradient norm smaller than 1, it encourage gradient norm closer to 1 according to their proof. In practice, this works slightly better. Here is the algorithm of WGAN-GP:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/WGANGP.jpg&#34; alt=&#34;WGAN-GP&#34; /&gt;&lt;/p&gt;

&lt;p&gt;And the implementation on tensorflow is quite simple with tf.gradient function.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt; 
 with tf.name_scope(&#39;gan_loss&#39;):

    D_loss = tf.reduce_mean(fake_score) - tf.reduce_mean(real_score)

    G_loss = -tf.reduce_mean(fake_score)

    def interpolate(a, b):
        shape = tf.concat((tf.shape(a)[0:1], tf.tile([1], [a.shape.ndims - 1])), axis=0)
        alpha = tf.random_uniform(shape=shape, minval=0., maxval=1.)
        inter = a + alpha * (b - a)
        inter.set_shape(a.get_shape().as_list())
        return inter

    gp_sample = interpolate(gen, image_hr)

    gp_gradient = tf.gradients(net.discrimintor(gp_sample, reuse=True), gp_sample)

    grad_norm = tf.sqrt(tf.reduce_sum(tf.square(gp_gradient[0]), reduction_indices=[-1]))

    gp_loss = tf.reduce_mean(tf.square(grad_norm-1.))

    D_overall_loss = D_loss + gp_rate*gp_loss

    tf.summary.scalar(&#39;G_loss&#39;, (G_loss))
    tf.summary.scalar(&#39;D_loss&#39;, (D_loss))
    tf.summary.scalar(&#39;GP_loss&#39;, gp_loss)

    G_overall_loss = gan_ratio*G_loss + SR_loss 
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;reference-materials&#34;&gt;Reference Materials&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1406.2661&#34; target=&#34;_blank&#34;&gt;Original Paper of GAN: Generative Adversarial Nets&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1611.02163.pdf&#34; target=&#34;_blank&#34;&gt;Unrolled Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1701.04862.pdf&#34; target=&#34;_blank&#34;&gt;Towards Principled Methods for Training Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1701.07875.pdf&#34; target=&#34;_blank&#34;&gt;Wasserstein GAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://papers.nips.cc/paper/6125-improved-techniques-for-training-gans.pdf&#34; target=&#34;_blank&#34;&gt;Improved Techniques for Training GANs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://lilianweng.github.io/lil-log/2017/08/20/from-GAN-to-WGAN.html&#34; target=&#34;_blank&#34;&gt;Another great blog of WGAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1704.00028.pdf&#34; target=&#34;_blank&#34;&gt;WGAN-GP: Improved Training of Wasserstein GANs&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Microscale two-dimensional (2D) temperature mapping by ratiometric fluorescence imaging under orthogonal excitations</title>
      <link>/publication/temperature_mapping/</link>
      <pubDate>Thu, 15 Feb 2018 00:00:00 -0600</pubDate>
      
      <guid>/publication/temperature_mapping/</guid>
      <description></description>
    </item>
    
    <item>
      <title> Fine-Grained Visual Categorization on iNaturalist dataset</title>
      <link>/project/fine-grained-visual-categorization-inaturalist/</link>
      <pubDate>Mon, 27 Nov 2017 00:00:00 -0600</pubDate>
      
      <guid>/project/fine-grained-visual-categorization-inaturalist/</guid>
      <description>

&lt;h2 id=&#34;table-of-content&#34;&gt;Table of Content&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#overview&#34;&gt;Overview&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#main-difficulties&#34;&gt;Main Difficulties&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#data-imbalance&#34;&gt;Data Imbalance&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#weakly-supervised-label&#34;&gt;Weakly supervised label&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#data-insufficiency&#34;&gt;Data insufficiency&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#fine-tuned-model-on-imagenet-pretrained-model&#34;&gt;Fine-tuned model on ImageNet pretrained model.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#fine-grained-classification&#34;&gt;Fine-grained Classification&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#super-category-classification&#34;&gt;Super-category classification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#fine-grained-classification-in-single-super-category&#34;&gt;Fine-grained classification in single super-category&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#bilinear-model&#34;&gt;Bilinear model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#attention-model-class-activation-map-&#34;&gt;Attention model(Class Activation Map)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#reference-materials&#34;&gt;Reference materials&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;

&lt;p&gt;I run into this challenge at early April. This dataset contains about 0.4 million images spanning over 8142 categories. Without expert knowledge, many of these species are extremely difficult to accurately classify due to their visual similarity. This classification task require us to features a large number of fine-grained
categories over class imbalance.&lt;/p&gt;

&lt;h2 id=&#34;main-difficulties&#34;&gt;Main Difficulties&lt;/h2&gt;

&lt;h3 id=&#34;data-imbalance&#34;&gt;Data Imbalance&lt;/h3&gt;

&lt;p&gt;The dataset is quite imbalance, some classes have more than 1000 images, while other classes only have less than 10 images. Here is a table of class image
number distribution.&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;image number range&lt;/th&gt;
&lt;th&gt;class number&lt;/th&gt;
&lt;th&gt;percentage&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;0&amp;lt;n&amp;lt;=10&lt;/td&gt;
&lt;td&gt;472&lt;/td&gt;
&lt;td&gt;5.78%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;10&amp;lt;n&amp;lt;=25&lt;/td&gt;
&lt;td&gt;4675&lt;/td&gt;
&lt;td&gt;57.4%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;25&amp;lt;n&amp;lt;=50&lt;/td&gt;
&lt;td&gt;1599&lt;/td&gt;
&lt;td&gt;19.6%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;50&amp;lt;n&amp;lt;=100&lt;/td&gt;
&lt;td&gt;554&lt;/td&gt;
&lt;td&gt;6.80%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;100&amp;lt;n&amp;lt;=200&lt;/td&gt;
&lt;td&gt;399&lt;/td&gt;
&lt;td&gt;4.90%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;200&amp;lt;n&amp;lt;=1000&lt;/td&gt;
&lt;td&gt;443&lt;/td&gt;
&lt;td&gt;5.44%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Over one half classes have image number in range $[10, 25]$, while the other classes are ranged in $[0, 1000]$. Some method or tricks to compensate classs imbalance is necessary here. Otherwise our model would easily get crashed. The majority class image number range is $[10, 25]$, while some other classes can easily get 1000, which can totally destory the training for the majority class.&lt;/p&gt;

&lt;p&gt;First, we just abandon the excessive images in class with more than 200 images. Second, oversample the minority class and put more aggressive image augmentation
to these classes. For example, affine transformation like random rotate, crop, shear, flap, blurring like gaussian blur, random dropout and so on. Image augmentation is implemented with Augmentor. Finally, we utilize weighted cross entropy loss function, further compensate the remaining
imbalance effect in our dataset. Here is the code for Multiprocessing Augmentor Pipeline:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
# Multiprocessing Augmentor Pipeline
def pipeline():

    P1 = Augmentor.Pipeline()
    P1.flip_left_right(probability=0.3)
    P1.flip_top_bottom(probability=0.05)
    P1.rotate90(probability=0.1)
    P1.rotate_without_crop(probability=0.1, max_left_rotation=15, max_right_rotation=15)
    P1.random_distortion(probability=0.1, grid_width=10, grid_height=10, magnitude=1)
    P1.skew_corner(probability=0.1)
    P1.shear(probability=0.1, max_shear_left=10, max_shear_right=10)
    P1.crop_centre(probability=1.0, percentage_area=0.9)
    P1.crop_random(probability=1.0, percentage_area=0.9)

    return P1


def aug_func(imgs, label, q):

    P = pipeline()
    imgs = np.uint8(imgs)
    g = P.keras_generator_from_array(imgs, label, batch_size=imgs.shape[0]
                                     , image_data_format=&#39;WTF&#39;)
    i, l = next(g)
    q.put([i, l])


def aug_parallel(imgs, labels):

    queue = Queue()
    cpus = multiprocessing.cpu_count()
    step = int(imgs.shape[0] / cpus)
    processes = []
    for ii in range(cpus):

        p = Process(target=aug_func,
                    args=(imgs[ii * step:(ii + 1) * step, :, :, :],
                          labels[ii * step:(ii + 1) * step],
                          queue))
        processes.append(p)
        p.start()

    rets = []
    labels = []

    for p in processes:
        ret, l = queue.get() 
        rets.append(ret)
        labels.append(l)
    for p in processes:
        p.join()

    if step == 1:
        rets = np.squeeze(np.array(rets))
        labels = np.squeeze(np.array(labels), axis=1)
    else:
        rets = np.concatenate(rets, axis=0)
        labels = np.concatenate(labels, axis=0)

    return rets, labels


&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;weakly-supervised-label&#34;&gt;Weakly supervised label&lt;/h3&gt;

&lt;p&gt;After looking into some image samples in the dataset, we find the the image labels are just weakly supervised labels. In other words, usually only
a small area in the image contains the target object. For example, one image is labeled as sunflowers, but the sunflower only appear at one corner,
and the background is grass and woods.&lt;/p&gt;

&lt;p&gt;This kind of labeling will apparently increase the difficulty of precise image classification because we have to do something to localize the object in the
image and prevent the background from affecting the classification result. That&amp;rsquo;s the main reason we incorporate attention model afterwards.&lt;/p&gt;

&lt;p&gt;Here are some cases of weakly supervised labels:&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Image&lt;/th&gt;
&lt;th&gt;Image&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;img src=&#34;/img/weak1.jpg&#34; alt=&#34;&#34; /&gt;&lt;/td&gt;
&lt;td&gt;&lt;img src=&#34;/img/weak2.png&#34; alt=&#34;&#34; /&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&#34;data-insufficiency&#34;&gt;Data insufficiency&lt;/h3&gt;

&lt;p&gt;Although we have about 450,000 images in total, when averaged to every class, we only have 50 images left. And the majority class image number range is $[10, 25]$.
With so few images, it would be impossible to train a model from scratch. The most popular solution would be adopt a ImageNet pretrained model and fine-tuned on our dataset.&lt;/p&gt;

&lt;p&gt;Our first choice is the &lt;a href=&#34;https://arxiv.org/abs/1602.07261&#34; target=&#34;_blank&#34;&gt;Inception Resnet V2&lt;/a&gt; network from google. This network is well designed to save parameters and achieve
good performance at the same time. The same with previous Inception module, Inception resnet v2 also implements nx1 and 1xn convolution to replace nxn convolution, also bottleneck architecture to reduce computation. Besides, Inception Resnet V2 introduces residual connection from ResNet architecture to
eases the gradient vanishing problem when training deep neural nets. And the total layer number of the network goes to 467.&lt;/p&gt;

&lt;h2 id=&#34;fine-tuned-model-on-imagenet-pretrained-model&#34;&gt;Fine-tuned model on ImageNet pretrained model.&lt;/h2&gt;

&lt;p&gt;The first trail is directly classification over 8142 categories. Here is an architecture diagram of Inception Resnet V2:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/InceptionResnetv2.jpg&#34; alt=&#34;InceptionResnetv2.jpg&#34; /&gt;&lt;/p&gt;

&lt;p&gt;We first freeze the CNN feature extraction part, and train our own classifier - the final layers
of the network. Note that we have to training two classifier, one at the end of network, the auxiliary one at the middle. The middle one is used to provide additional
gradient to help the network to avoid gradient vanishing.
After 5 epoches of training, the accurarcy goes up to 40%.&lt;/p&gt;

&lt;p&gt;And after that, we use a relatively low learning rate to fine-tune the network to avoid too much modification on the original network. After about 15 epoches, the training accuracy goes up to about 80% and the validation accuracy is about 60%, the top3 accuracy on test set is about 80%. Apparently there are some kinds of overfit, but this is inevitable with so little data. We will further try to freeze the starting layers throughtout the traiing process to add some regulate the expression capacity of the network. Hope that works.&lt;/p&gt;

&lt;h2 id=&#34;fine-grained-classification&#34;&gt;Fine-grained Classification&lt;/h2&gt;

&lt;h3 id=&#34;super-category-classification&#34;&gt;Super-category classification&lt;/h3&gt;

&lt;p&gt;After finishing the previous model with InceptionResnetv2, we carefully examined our result. We found that our model often predicted three very similar classes, like three different sunflowers with very subtle difference. This means our model may have a hard time to classify images over similar classes. So, the next step is fine grained classification over these sub species. First, we divide our dataset into 11 super categories, like plants, insects, birds and so on. We consider this problem in a high dimensional space, and treat the image classes as clusters. The cluster distance of classes in the same super-category should be signaficantly smaller than cluster distance of classes in different super-categories. But the our image labelling cannot represent this property. After one-hot encoding, we treat every image class equivalently and assume the distance between them are the same.&lt;/p&gt;

&lt;p&gt;According to these analysis, we decided to classify images into their super-categories and do fine-grained classification in every single super-category. In this way, our model would concentrate more on the fine details of similar objects in the same super-category, without wasting resources on the easy cross super-category classification.&lt;/p&gt;

&lt;p&gt;Here is a diagram of image space:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/Figure_1.png&#34; alt=&#34;Diagram of image space&#34; /&gt;&lt;/p&gt;

&lt;p&gt;It would be more efficient to classify ABC and DE first. In this way, the class distance variance would be much less, we can use relatively less resource to achieve comparable performance to direct classification.&lt;/p&gt;

&lt;h3 id=&#34;fine-grained-classification-in-single-super-category&#34;&gt;Fine-grained classification in single super-category&lt;/h3&gt;

&lt;h4 id=&#34;bilinear-model&#34;&gt;Bilinear model&lt;/h4&gt;

&lt;p&gt;The first model is &lt;a href=&#34;http://vis-www.cs.umass.edu/bcnn/docs/bcnn_iccv15.pdf&#34; target=&#34;_blank&#34;&gt;bilinear model&lt;/a&gt;. This method considers the interaction between different feature map channels. If the feature map has the size $[H, W, C]$, treat every pixel in feature map as a feature vector, feature vectors has the size $[1,C]$. And the bilinear model takes the outer product of every feature vector and forms a $[H, W, C, C]$ feature map. After taking global pooling, we get a feature map [C, C]. In this process, bilinear model utilizes higher order information of the feature map. According to our test, its performance is pretty good. It achieves nearly 90% accuracy in single super-category classification. But when applying it to bigger super-category, our GPU memory is not enough. The reason is after bilinear operation, the feature map size becomes so big $(C^2)$, so the paremeters in following fully connected layers become overwhelming. For example, if the feature map has 512 channels, and our biggest super-category has more than 3000 classes, the parameters in FC layer is $512^2*3000 = 78 million$, which is more than all the CNN layers combined.&lt;/p&gt;

&lt;h4 id=&#34;attention-model-class-activation-map&#34;&gt;Attention model(Class Activation Map)&lt;/h4&gt;

&lt;p&gt;Due to the weakly supervised nature of the dataset (discussed &lt;a href=&#34;#weakly-supervised-label&#34;&gt;above&lt;/a&gt;), we want to first roughly localize the target object in the image before fine-grained classification.
Here attention model comes to the rescue. With attention model, we can determine which part of images matters for the network to make predictions, which should be the object we want to localize. Since we already have the network of super-category classification, its feature map can be utilized to generate class activation map &lt;a href=&#34;http://cnnlocalization.csail.mit.edu/Zhou_Learning_Deep_Features_CVPR_2016_paper.pdf&#34; target=&#34;_blank&#34;&gt;(CAM)&lt;/a&gt; to represent the attention level of input images.
Here is a diagram of CAM:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/CAM.png&#34; alt=&#34;CAM&#34; /&gt;&lt;/p&gt;

&lt;p&gt;As shown in the diagram, we can use the feature map and fc layer weight to generate the attention map for images with super-category network. Put an image into the network, we can get a feature map $[H, W, C]$ and a prediction. In order to map the global pooling layers
to predictions, the FC layer weight is $W \in R^{C \times N}$, where N is the output class number. If the prediction is $j$, CAM is calculated as follows:&lt;/p&gt;

&lt;p&gt;$$CAM = \sum_{k=0}^C F(k)*W_{k,j}$$&lt;/p&gt;

&lt;p&gt;where F(k) represent kth feature map channel. Alternatively, we can use top N prediction if top 1 prediction is not confident. Afterwards, we upsample CAM to fit input image size.&lt;/p&gt;

&lt;p&gt;After this, we used OSTU algorithm to make the CAM into binary image and extract the biggest contour and generate bounding box via OpenCV. Here are some results without any cherry picking:&lt;/p&gt;

&lt;p&gt;First column is original images and generated bounding box; second column is CAM heatmap; third column is the binary image after OSTU algorithm and the biggest contour.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/CAM_result.png&#34; alt=&#34;drawing&#34; style=&#34;width: 500px;&#34;/&gt;&lt;/p&gt;

&lt;p&gt;The result is pretty amazing. Regardless of the type of objects, like plants, birds, insects, we can accurately locate them.&lt;/p&gt;

&lt;h4 id=&#34;classification-after-localization&#34;&gt;Classification after Localization&lt;/h4&gt;

&lt;p&gt;Still in process.&lt;/p&gt;

&lt;h2 id=&#34;reference-materials&#34;&gt;Reference materials&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1602.07261&#34; target=&#34;_blank&#34;&gt;Inception ResNet V2 Paper&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://vis-www.cs.umass.edu/bcnn/docs/bcnn_iccv15.pdf&#34; target=&#34;_blank&#34;&gt;Bilinear Model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://cnnlocalization.csail.mit.edu/Zhou_Learning_Deep_Features_CVPR_2016_paper.pdf&#34; target=&#34;_blank&#34;&gt;Class Activation Map Paper&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Obfuscated/Blurred Human Face Reconstruction</title>
      <link>/project/obfuscatedblurred-human-face-reconstruction/</link>
      <pubDate>Mon, 27 Nov 2017 00:00:00 -0600</pubDate>
      
      <guid>/project/obfuscatedblurred-human-face-reconstruction/</guid>
      <description>

&lt;h1 id=&#34;table-of-contents&#34;&gt;Table of Contents&lt;/h1&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;#data-preparation&#34;&gt;Data preparation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#super-resolution-resnet&#34;&gt;Super Resolution ResNet&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#generative-adversarial-network-model&#34;&gt;Generative Adversarial Network Model&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Nowadays, a lot of images on the Internet are intentionally blurred or mosaiced due to various reasons. The main objective of this project is to
reconstruct these images, especially heavily blurred ones, to their original high-resolution counterpart. This problem is an ill-posed problems, we need to
predict fine-grained details only based on little information on degenerated images. Exploring and enforcing strong prior information
about the high-resolution image are necessary to guarantee the stability of
this reconstruction process. Many traditional example-based methods have been devoted to resolving this problem via probabilistic
graphical model, neighbor embedding, sparse coding, linear or nonlinear regression, and random forest.&lt;br /&gt;
Our approach is utilizing deep networks for image reconstruction to learn the mapping between low and high-resolution image pairs, automatically take the prior
information into account. Check out the code &lt;a href=&#34;https://github.com/shen338/Obfuscated-Face-Reconstruction&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Our experiment consists of three steps.&lt;/p&gt;

&lt;h2 id=&#34;data-preparation&#34;&gt;Data preparation&lt;/h2&gt;

&lt;p&gt;In 2017, the most popular dataset about human face is the &lt;a href=&#34;http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html&#34; target=&#34;_blank&#34;&gt;CelebA dataset&lt;/a&gt; from CUHK, which has
 10,177 number of identities, 202,599 number of face images.&lt;br /&gt;
 What we do first is detecting and cropping human face from the image and resize it into 128x128 in convenience. We use OpenCV&amp;rsquo;s module named &amp;ldquo;haarcascade_frontalface&amp;rdquo;
 to detect faces and use bicubic interpolation to resize images. The code are as below:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;facedata = &amp;quot;haarcascade_frontalface_default.xml&amp;quot;
cascade = cv2.CascadeClassifier(facedata)
img = cv2.imread(image_directory)
minisize = (img.shape[1], img.shape[0])
miniframe = cv2.resize(img, minisize)

faces = cascade.detectMultiScale(miniframe)
if(len(faces) == 0): continue
x, y, w, h = [v for v in faces[0]]  # only need the first detected face image

img_raw = img[y:y + h, x:x + w]
img_raw = cv2.resize(img_raw, (128, 128), interpolation=cv2.INTER_LANCZOS4)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then we manually downsample these high-resolution images using gaussian and average pooling. After write both low and high-resolution image pairs into binary files
using TFRecord for multi-threading read in the future. Code:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import tensorflow as tf
import numpy as np
import cv2
import glob
import random
import sys


def load_image(addr):
    # read an image and resize to (224, 224)
    # cv2 load images as BGR, convert it to RGB
    img = cv2.imread(addr)
    #img = cv2.resize(img, (128, 128), interpolation=cv2.INTER_CUBIC)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    img = img.astype(np.uint8)
    return img

def _int64_feature(value):
  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))

def _bytes_feature(value):
  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))


shuffle_data = True
image_path_small = &#39;./origin/small/*.png&#39;
address_small = glob.glob(image_path_small)
print(len(address_small))
image_path_origin = &#39;./origin/origin/*.png&#39;
address_origin = glob.glob(image_path_origin)

if shuffle_data:
    c = list(zip(address_small, address_origin))
    random.shuffle(c)
    address_small,  address_origin= zip(*c)

train_filename = &#39;train_espcn.tfrecords&#39;

# create new TFrecord file
writer = tf.python_io.TFRecordWriter(train_filename)

for i in range(len(address_small)):

    if not i % 1000:
        print(&#39;Train data: {}/{}&#39;.format(i, len(address_small)))
        sys.stdout.flush()

    img_small = load_image(address_small[i])
    img_origin = load_image(address_origin[i])

    feature = {&#39;train/image_small&#39;: _bytes_feature(tf.compat.as_bytes(img_small.tostring())),
               &#39;train/image_origin&#39;: _bytes_feature(tf.compat.as_bytes(img_origin.tostring()))}

    # Create an example protocol buffer
    example = tf.train.Example(features=tf.train.Features(feature=feature))

    # Serialize to string and write on the file
    writer.write(example.SerializeToString())

writer.close()
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;super-resolution-resnet&#34;&gt;Super Resolution ResNet&lt;/h2&gt;

&lt;p&gt;My first trail was using convolution neural network to do end-to-end super-resolution. The intuition is quite simple,
just feed the low resolution images to the network, do downsampling and upsampling, in the end, use
high resolution images as ground truth to train this network. In this way, the neural network will learn how to map from
low-resolution image directly to high-resolution images. One thing worth to mention, In that time, I didn&amp;rsquo;t have a good GPU,
so I have to so every I can to save computational resources.
We replaced the upsampling layers with Pixel Shuffle layers, which directly map the feature map into
high resolution output instead of doing transposed convolution to upsample the feature map.&lt;/p&gt;

&lt;p&gt;Next, we use skip connection from &lt;a href=&#34;https://arxiv.org/abs/1512.03385&#34; target=&#34;_blank&#34;&gt;ResNet paper&lt;/a&gt; to enhance our model&amp;rsquo;s  expression capacity. We put 15 residual
block module in our network before Pixel Shuffle layers. The enhancing is quite obvious in the result comparasion.
We also use pretrained VGG net to build the preceptual loss. Instead of using
vanilla MSE loss to compare the network output and ground truth, we feed the network output and ground truth
into ImageNet pretrained VGG net and compare their feature map difference using MSE. Using direct MSE loss function also works here, but it may
not agree with human observers, because human visual system is not sentitive to color and edgee. While perceptual loss simulates human visual system, making the
loss value more related to human percetion. The architecture of our network is shown as follows:
&lt;img src=&#34;https://raw.githubusercontent.com/shen338/Obfuscated-Face-Reconstruction/master/SRResNet_model.PNG&#34; alt=&#34;SRResnet&#34; /&gt;&lt;/p&gt;

&lt;p&gt;This model works well for 4 times mosaiced/blurred images, even though there is still some smoothing effect in human face detailes. I think our model even makes
these celebrities more beautiful/handsome 😄. After smoothing out their wrinkles and little flaws, they all look younger than before! The result is shown as follows:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/shen338/Obfuscated-Face-Reconstruction/master/result/SRResNet_result.PNG&#34; alt=&#34;SRResnet_result&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;generative-adversarial-network-model&#34;&gt;Generative Adversarial Network Model&lt;/h2&gt;

&lt;p&gt;Although our ResNet model works well in 4 times mosaiced/blurred images, it terribly failed in more than 8 times mosaiced/blurred images. Maybe there is not enough
prior information for our network to reconstruct high-resolution images. To deal with this problem, we incorporate GAN model into our project, and expect GANs can
generate the fine details to the reconstruction process and improve our result. We brought this idea from &lt;a href=&#34;https://arxiv.org/abs/1609.04802&#34; target=&#34;_blank&#34;&gt;SRGAN paper&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Different from original GAN model to generate unique images based on image dataset, instead of feeding random noise to generator, we feed low-resolution
image to the generator and expect it to produce high-resolution images. And the discriminator is still responsible to determine whether the image is from
generator or from dataset. And the training loss also consist of two parts, the super-resolution(SR) loss, and the GAN loss. The SR loss is the perceptual loss (mentioned above)
between ground truth high-resolution image and the generator output, while GAN loss is the loss function for the generator and discriminator combined.&lt;/p&gt;

&lt;p&gt;First, we tried vanilla deep convolution GAN(DCGAN) on our GAN model. After a few days of hyperparamete tunning, we find this model is not that stable.
The whole model can easily crash due to a small shift of a single hyperparameter. But there are better alternatives for DCGAN: the &lt;a href=&#34;https://arxiv.org/abs/1701.07875&#34; target=&#34;_blank&#34;&gt;Wasserstein GAN&lt;/a&gt;.
Although still have imperfections, WGAN almost totally solve issues like training instability, failure to converge or model collapse.&lt;/p&gt;

&lt;p&gt;Here is the algorithm of WGAN:
&lt;img src=&#34;/img/WGAN.jpg&#34; alt=&#34;WGAN&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Just minor modification from original GAN algorithm, how amazing is that! In Wasserstein GAN paper, the author thoroughly analyzed the weakness and holes on
original GANs. For details of the mathematics and implementation about WGAN, see my &lt;a href=&#34;https://shen338.github.io/post/amazing-gan---wasserstein-gan/&#34; target=&#34;_blank&#34;&gt;another blog&lt;/a&gt;. Here I will only list the modification from original GAN to WGAN:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Remove the sigmoid function at the end of Discriminator&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Remove the log function in generator and discriminator loss function&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;clip the gradient norm into an interval $[-c, c]$&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Use optimizers without momentum term, like RMSprop, not Adam.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The core code are as follows:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def train():

    image_lr = tf.placeholder(dtype=tf.float32, shape=(None, 16, 16, 3), name=&#39;lr&#39;)
    image_hr = tf.placeholder(dtype=tf.float32, shape=(None, 128, 128, 3), name=&#39;hr&#39;)

    net = WGAN(gamma)

    gen = net.generator(image_lr, bottleneck_num=2)

    real_score = net.discrimintor(gen)
    fake_score = net.discrimintor(image_hr, reuse=True)

    with tf.name_scope(&#39;SR_loss&#39;):

        residual = image_hr - gen
        square = tf.abs(residual)
        SR_loss = tf.reduce_mean(square)

        tf.summary.scalar(&#39;SR_loss&#39;, SR_loss)

    with tf.name_scope(&#39;gan_loss&#39;):

        D_loss = tf.reduce_mean(fake_score) - tf.reduce_mean(real_score)

        G_loss = -tf.reduce_mean(fake_score)

        tf.summary.scalar(&#39;G_loss&#39;, G_loss)
        tf.summary.scalar(&#39;D_loss&#39;, D_loss)

        G_overall_loss = gan_ratio*G_loss + SR_loss 

    # get variable from G and D
    var_g = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, &#39;generator&#39;)
    var_d = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, &#39;discriminator&#39;)

    with tf.name_scope(&#39;optim&#39;):

        optim_g = tf.train.RMSPropOptimizer(learning_rate=LEARNING_RATE)\
            .minimize(G_overall_loss, var_list=var_g)
        optim_d = tf.train.RMSPropOptimizer(learning_rate=LEARNING_RATE) \
            .minimize(-D_loss, var_list=var_d)

    clip_D = [p.assign(tf.clip_by_value(p, -0.01, 0.01)) for p in var_d]

    # set up logging for tensorboard
    writer = tf.summary.FileWriter(filewriter_path)
    writer.add_graph(tf.get_default_graph())
    summaries = tf.summary.merge_all()
	
	config = tf.ConfigProto()
    config.gpu_options.allow_growth = True
    with tf.Session() as sess:

        init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())
        sess.run(init_op)

        steps, start_average, end_average = 0, 0, 0
        start_time = time.clock()

        coord = tf.train.Coordinator()
        threads = tf.train.start_queue_runners(coord=coord)

        for ii in range(NUM_EPOCHS):

            batch_average = 0
            batch_num = int(np.floor(192794 / BATCH_SIZE / 6.0))

            for jj in range(batch_num):

                g_ops = [optim_g, G_loss, summaries]
                d_ops = [optim_d, D_loss]

                for kk in range(critic):

                    steps += 1
                    img_lr, img_hr = load_batch_date()
                    img_lr = (img_lr.astype(np.float32) - 127.5) / 127.5
                    img_hr = (img_hr.astype(np.float32) - 127.5) / 127.5

                    _, loss_d = sess.run(d_ops, feed_dict=
                                  {image_lr: img_lr, image_hr: img_hr})

                    sess.run(clip_D)

                steps += 1
                img_lr, img_hr = sess.run([images, labels])
                img_lr = (img_lr.astype(np.float32) - 127.5) / 127.5
                img_hr = (img_hr.astype(np.float32) - 127.5) / 127.5

                _, loss_g, summary = sess.run(g_ops,
                                feed_dict={image_lr: img_lr, image_hr: img_hr})

                # update W_loss and Kt

                writer.add_summary(summary, steps)
                batch_average += loss_g

                if (steps % 100 == 0):
                    print(&#39;step: {:d}, G_loss: {:.9f}, D_loss: {:.9f}&#39;.format(steps, loss_g, loss_d))
                    print(&#39;time:&#39;, time.clock())

            batch_average = float(batch_average) / batch_num

            duration = time.time() - start_time
            print(&#39;Epoch: {}, step: {:d}, loss: {:.9f}, &#39;
                  &#39;({:.3f} sec/epoch)&#39;.format(ii, steps, batch_average, duration))

            start_time = time.time()
            net.save(sess, saver, checkpoint_path, steps)
        coord.request_stop()

        # Wait for threads to stop
        coord.join(threads)
        sess.close()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Also, we implement WGAN-GP method, instead of directly clipping gradient, it put penalty on gradient&amp;rsquo;s L2 norm. This method is even better
compared to original Wasserstein GAN.&lt;/p&gt;

&lt;p&gt;And the result is pretty good compared to SRResNet model:
&lt;img src=&#34;https://raw.githubusercontent.com/shen338/Obfuscated-Face-Reconstruction/master/result/SRGAN_result.PNG&#34; alt=&#34;SRGAN_result&#34; /&gt;&lt;br /&gt;
The smoothing effect is quite significant in this case. But the SRGAN result is totally acceptable considering our model is reconstruct the
high-resolution image only using &lt;sup&gt;1&lt;/sup&gt;&amp;frasl;&lt;sub&gt;64&lt;/sub&gt; of original information. I believe our result should be better with a faster GPU and deep network. I finished this
project on a Nivida Quadro K1000 GPU, which takes 6 hours to run one epoch.&lt;/p&gt;

&lt;p&gt;There are always some failure case using GAN model:
&lt;img src=&#34;https://raw.githubusercontent.com/shen338/Obfuscated-Face-Reconstruction/master/result/failure_case.PNG&#34; alt=&#34;failure case&#34; /&gt;
Some of them are funny and some of them are just scary&amp;hellip;&lt;/p&gt;

&lt;p&gt;Reference Materials:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html&#34; target=&#34;_blank&#34;&gt;CelebA Dataset from CUHK&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1501.00092.pdf&#34; target=&#34;_blank&#34;&gt;Image Super-Resolution Using Deep Convolutional Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1701.07875&#34; target=&#34;_blank&#34;&gt;Wasserstein GAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://lilianweng.github.io/lil-log/2017/08/20/from-GAN-to-WGAN.html&#34; target=&#34;_blank&#34;&gt;A great blog on GAN and WGAN&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Subwavelength focusing by binary multi-annular plates: design theory and experiment</title>
      <link>/publication/map/</link>
      <pubDate>Sun, 15 Feb 2015 00:00:00 -0600</pubDate>
      
      <guid>/publication/map/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
