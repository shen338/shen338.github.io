<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Deep-learning on Tong Shen</title>
    <link>/tags/deep-learning/</link>
    <description>Recent content in Deep-learning on Tong Shen</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Thu, 17 May 2018 21:00:00 -0400</lastBuildDate>
    
	<atom:link href="/tags/deep-learning/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Going Deeper in Batch Normalization</title>
      <link>/post/going-deeper-into-batch-normalization/</link>
      <pubDate>Thu, 17 May 2018 21:00:00 -0400</pubDate>
      
      <guid>/post/going-deeper-into-batch-normalization/</guid>
      <description>Table of contents/optimization  Interpretation and Advantage of Batch Norm Algorithm and implementation Improvements and Alternatives  Batch norm fused with Convolution Layer normalization Instance Normalization Group Normalization Other normalization techniques*  Reference Materials:  This article will thoroughly explain batch normalization in a simple way. I wrote this article after getting failed an interview because of detailed batchnorm related question. I will start with why we need it, how it works, then how to fuse it into conv layer, and finally how to implement it in tensorflow.</description>
    </item>
    
    <item>
      <title>Amazing GAN - Wasserstein GAN</title>
      <link>/post/amazing-gan---wasserstein-gan/</link>
      <pubDate>Sat, 17 Feb 2018 21:00:00 -0500</pubDate>
      
      <guid>/post/amazing-gan---wasserstein-gan/</guid>
      <description>Table of Content  KL divergence and JS divergence Generative Adversarial Networks  Global optimal loss  Problem with Vanilla GANs  Gradient Vanishing Mode Collapse  Improved Training of GANs Wasserstein GAN  Earth Mover distance Comparasion between EM distance and KL/JS divergence Lipschitz continuity Modified Algorithm Wasserstein GAN with gradient penalty  Disadvantages of gradient clipping in WGAN Gradient Penalty   Reference Materials   KL divergence and JS divergence Before diving into details, let first review two very important metrics to quantify the similarity of two probability distributions: Kullback-Leibler Divergence and Jensen-Shannon Divergence.</description>
    </item>
    
    <item>
      <title>Neural Network based Object Recognition: Fast/Faster/Mask R-CNN, SSD, and YOLO</title>
      <link>/post/neural-network-based-object-recognition-fast-faster-mask-r-cnn-ssd-and-yolo/</link>
      <pubDate>Mon, 01 Jan 2018 21:00:00 -0500</pubDate>
      
      <guid>/post/neural-network-based-object-recognition-fast-faster-mask-r-cnn-ssd-and-yolo/</guid>
      <description>Table of Content  History  AlexNet ResNet Inception Network Series Deformable Parts Model Overfeat  Metrics: mAP  Precision and Recall Average precision (AP) mean average precision (mAP)  R-CNN Families  RCNN  Selective Search Bounding Box Regression Non Maximum Suppression Workflow Speed Bottleneck  Fast-RCNN  ROI pooling Workflow  Faster-RCNN Mask-RCNN  Single Shot Detector  Single Shot MultiBox Detector(SSD) You Only Look Once(YOLO)  YOLOv1 YOLOv2 YOLOv3   Summary Reference Materials  It&amp;rsquo;s well known that deep learning has been a real game changer in machine learning, especially in computer vision.</description>
    </item>
    
    <item>
      <title>Spatial Transform Network (STN)</title>
      <link>/post/spatial-transform-network/</link>
      <pubDate>Sun, 19 Nov 2017 21:00:00 -0500</pubDate>
      
      <guid>/post/spatial-transform-network/</guid>
      <description> Spatial Transformer Before diving into STNs, we need to concrete our knowledge about image transformer.
Image Transformation Bilinear Interpolation Tensorflow Implement Result Spatial Transform Network Motivation Fun work with STNs </description>
    </item>
    
  </channel>
</rss>