<!DOCTYPE html>
<html lang="en-us">
<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="theme" content="hugo-academic">
  <meta name="generator" content="Hugo 0.40.3" />
  <meta name="author" content="Tong Shen">

  
  
  
  
    
      
    
  
  <meta name="description" content="KL divergence and JS divergence Before diving into details, let first review two very important metrics to quantify the similarity of two probability distributions: Kullback-Leibler Divergence and Jensen-Shannon Divergence.
 Kullback-Leibler Divergence measures the divergence of probability distribution p(x) to q(x): $$KL(P||Q) = \int_x P(x)log\frac{P(x)}{Q(x)}dx$$ KL(P||Q) achieves its minimum zero when P(x) and Q(x) are the same everywhere.
KL divergence is widely used as a metrics to measure the similarity between two distributions.">

  
  <link rel="alternate" hreflang="en-us" href="/post/amazing-gan---wasserstein-gan/">

  


  

  
  
  <meta name="theme-color" content="#0095eb">
  
  
  
  
    
  
  
    
    
      
        <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">
      
    
  
  
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha512-6MXa8B6uaO18Hid6blRMetEIoPqHf7Ux1tnyIQdpt9qI5OACx7C+O3IVTr98vwGnlcg0LOLa02i9Y1HpVhlfiw==" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha512-SfTiTlX6kk+qitfevl/7LibUOeJWlt9rbyDn92a1DqWOw9vWG2MFoays0sgObmWazO5BQPiFucnnEAjpAB+/Sw==" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">
  
  
  
  
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Lato:400,700%7cMerriweather%7cRoboto&#43;Mono%7cPermanent&#43;Marker%7cPrata">
  
  <link rel="stylesheet" href="/styles.css">
  

  

  
  <link rel="alternate" href="/index.xml" type="application/rss+xml" title="Tong Shen">
  <link rel="feed" href="/index.xml" type="application/rss+xml" title="Tong Shen">
  

  <link rel="manifest" href="/site.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="/post/amazing-gan---wasserstein-gan/">

  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="og:site_name" content="Tong Shen">
  <meta property="og:url" content="/post/amazing-gan---wasserstein-gan/">
  <meta property="og:title" content="Amazing GAN - Wasserstein GAN | Tong Shen">
  <meta property="og:description" content="KL divergence and JS divergence Before diving into details, let first review two very important metrics to quantify the similarity of two probability distributions: Kullback-Leibler Divergence and Jensen-Shannon Divergence.
 Kullback-Leibler Divergence measures the divergence of probability distribution p(x) to q(x): $$KL(P||Q) = \int_x P(x)log\frac{P(x)}{Q(x)}dx$$ KL(P||Q) achieves its minimum zero when P(x) and Q(x) are the same everywhere.
KL divergence is widely used as a metrics to measure the similarity between two distributions.">
  <meta property="og:locale" content="en-us">
  
  <meta property="article:published_time" content="2018-05-17T21:00:00-04:00">
  
  <meta property="article:modified_time" content="2018-05-17T21:00:00-04:00">
  

  
  

  <title>Amazing GAN - Wasserstein GAN | Tong Shen</title>

</head>
<body id="top" data-spy="scroll" data-target="#toc" data-offset="71" >

<nav class="navbar navbar-default navbar-fixed-top" id="navbar-main">
  <div class="container">

    
    <div class="navbar-header">
      
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse"
              data-target=".navbar-collapse" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      
      <a class="navbar-brand" href="/">Tong Shen</a>
    </div>

    
    <div class="collapse navbar-collapse">

      
      
      <ul class="nav navbar-nav navbar-right">
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#about">
            
            <span>Home</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#myself">
            
            <span>CV</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#projects">
            
            <span>Projects</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#posts">
            
            <span>Posts</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#publications_selected">
            
            <span>Publications</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#contact">
            
            <span>Contact</span>
            
          </a>
        </li>

        
        
      

      
      </ul>

    </div>
  </div>
</nav>


<article class="article" itemscope itemtype="http://schema.org/Article">

  


  <div class="article-container">
    <h1 itemprop="name">Amazing GAN - Wasserstein GAN</h1>

    

<div class="article-metadata">

  <span class="article-date">
    
    <time datetime="2018-05-17 21:00:00 -0400 EDT" itemprop="datePublished dateModified">
      May 17, 2018
    </time>
  </span>
  <span itemscope itemprop="author publisher" itemtype="http://schema.org/Person">
    <meta itemprop="name" content="Tong Shen">
  </span>

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    3 min read
  </span>
  

  
  

  

  
  
<div class="share-box" aria-hidden="true">
  <ul class="share">
    <li>
      <a class="twitter"
         href="https://twitter.com/intent/tweet?text=Amazing%20GAN%20-%20Wasserstein%20GAN&amp;url=%2fpost%2famazing-gan---wasserstein-gan%2f"
         target="_blank" rel="noopener">
        <i class="fa fa-twitter"></i>
      </a>
    </li>
    <li>
      <a class="facebook"
         href="https://www.facebook.com/sharer.php?u=%2fpost%2famazing-gan---wasserstein-gan%2f"
         target="_blank" rel="noopener">
        <i class="fa fa-facebook"></i>
      </a>
    </li>
    <li>
      <a class="linkedin"
         href="https://www.linkedin.com/shareArticle?mini=true&amp;url=%2fpost%2famazing-gan---wasserstein-gan%2f&amp;title=Amazing%20GAN%20-%20Wasserstein%20GAN"
         target="_blank" rel="noopener">
        <i class="fa fa-linkedin"></i>
      </a>
    </li>
    <li>
      <a class="weibo"
         href="http://service.weibo.com/share/share.php?url=%2fpost%2famazing-gan---wasserstein-gan%2f&amp;title=Amazing%20GAN%20-%20Wasserstein%20GAN"
         target="_blank" rel="noopener">
        <i class="fa fa-weibo"></i>
      </a>
    </li>
    <li>
      <a class="email"
         href="mailto:?subject=Amazing%20GAN%20-%20Wasserstein%20GAN&amp;body=%2fpost%2famazing-gan---wasserstein-gan%2f">
        <i class="fa fa-envelope"></i>
      </a>
    </li>
  </ul>
</div>


  

</div>


    <div class="article-style" itemprop="articleBody">
      

<h2 id="kl-divergence-and-js-divergence">KL divergence and JS divergence</h2>

<p>Before diving into details, let first review two very important metrics to quantify the similarity of two probability distributions:
<a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence" target="_blank">Kullback-Leibler Divergence</a> and <a href="https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence" target="_blank">Jensen-Shannon Divergence</a>.</p>

<ol>
<li><p>Kullback-Leibler Divergence measures the divergence of probability distribution p(x) to q(x):
$$KL(P||Q) = \int_x P(x)log\frac{P(x)}{Q(x)}dx$$
KL(P||Q) achieves its minimum zero when P(x) and Q(x) are the same everywhere.<br />
KL divergence is widely used as a metrics to measure the similarity between two distributions. But according to its formula, its is asymmetric. Also, due to the rapid decreasing of logrithm function, KL divergence put to much weight when P(x) is near zero. This can cause some buggy result in real world measurement.</p></li>

<li><p>Jensen-Shannon Divergence. JS divergence is based on KL divergence and it is symmetric.<br />
$$JS(P||Q) = \frac{1}{2} KL(P||\frac{P+Q}{2}) + \frac{1}{2} KL(Q||\frac{P+Q}{2})$$</p></li>
</ol>

<p>Here is a plot of KL and JS divergence of two normal distributions: N(0, 1) and N(1, 1). Image resource <a href="https://lilianweng.github.io/lil-log/2017/08/20/from-GAN-to-WGAN.html" target="_blank">here</a>
<img src="/img/KL_JS_divergence.png" alt="KL-JS" /></p>

<p>As shown in the plot, JS divergence are symmetric while KL divergence is asymmetric. People believe the success of GANs comes from replacing the traditional maximum likelihood with symmetric similarity measure, JS divergence.</p>

<h2 id="generative-adversarial-networks">Generative Adversarial Networks</h2>

<p>Original GANs consists of two networks:</p>

<ol>
<li>Generator. It receive random samples and synthesize fake images feeding into discriminator. This random sample brings a potential output diversity.<br /></li>
<li>Discriminator. It receive the real dataset images and the fake images from generator. It works as a critic to evaluate the probability of input image coming from dataset and from generator.</li>
</ol>

<p>This is a diagram showing how GAN works:
<img src="/img/GANs.png" alt="GAN" /></p>

<p>In one hand, we want the discriminator&rsquo;s output probability over real data to be higher by maximizing $\mathbb{E}_{x \sim p_{data}(x)}[logD(x)]$; In another hand, we want the discriminator&rsquo;s output probability over fake data from generator to be lower by minimizing $\mathbb{E}_{z \sim p_(z)}[log(1-D(G(z)))]$.</p>

<p>And for the generator, we want it to fool the discriminator by minimizing $\mathbb{E}_{z \sim p_(z)}[log(1-D(G(z)))]$.</p>

<p>So, the overall process of GAN training is obvious:
$$\min_{G} \max_{D} L(G, D) =  [\mathbb{E}_{x \sim p_{data}(x)}[logD(x)] + \mathbb{E}_{z \sim p_(z)}[log(1-D(G(z)))]]$$</p>

<p>Overall, it is a minimax game between generator and discriminator. The main concern in training procedure is keeping the G and D evolving at the same speed.</p>

<h3 id="global-optimal-loss">Global optimal loss</h3>

<p>The global minimum of the training criterion $L(G, D)$ is achieved if and only if
$p_g = p_{data}$. Proof of are in the original <a href="https://arxiv.org/pdf/1406.2661.pdf" target="_blank">GAN paper</a>.<br />
First, we need to find the optimal solution for D when G is fixed. (Sorry, there is a problem in my equation alignment)</p>

<p>$$L(G, D) = \int_x p_{data}(x)logD(x)dx +  \int_z p(z)log(1-D(G(z))dz $$
         $$ = \int_x (p_{data}(x)logD(x) + p_g(x)log(1-D(x)))dx $$</p>

<p>Assume:
$$F(x) = p_{data}(x)logD(x) + p_g(x)log(1-D(x))$$
Take the derivative over $D(x)$:
$$\frac{d f(x)}{dx} = \frac{p_{data}(x)}{D(x)} + \frac{p_g(x)}{1-D(x)} = 0$$
Solve this equation, easily get :
$$D^{\star}(x) = \frac{p_{data}(x)}{p_{data}(x)+p_g(x)}$$</p>

<blockquote>
<p>When system is trained well, $p_{data}(x)$ and $p_g(x)$ should be similar, $D^{\star}(x) = \frac{1}{2}$.</p>
</blockquote>

<p>When discriminator is optimal, the loss function becomes:</p>

<p>$$L(G, D) = \int_x p_{data}(x)logD(x)dx +  \int_z p(z)log(1-D(G(z))dz $$</p>

<p>$$ = \int_x (p_{data}(x)logD(x) + p_g(x)log(1-D(x)))dx $$
$$ = \int_x p_{data}(x)log(\frac{p_{data}(x)}{p_{data}(x)+p_g(x)}) + p_g(x)log(\frac{p_g(x)}{p_{data}(x)+p_g(x)})dx $$
$$ = -log(4) + KL(p_{data} || \frac{p_{data}+p_g}{2}) + KL(p_g || \frac{p_{data}+p_g}{2})$$
$$ = -log(4) + 2* JS(p_{data} || p_g)$$</p>

<p>So, the loss function of GAN quantify the JS divergence of $p_{data}$ and $p_g$. The optimal value is $- log(4)$ when $p_{data} = p_g$.</p>

<h2 id="problem-with-vanilla-gans">Problem with Vanilla GANs</h2>

    </div>

    


<div class="article-tags">
  
  <a class="btn btn-primary btn-outline" href="/tags/wasserstein-gan/">Wasserstein GAN</a>
  
  <a class="btn btn-primary btn-outline" href="/tags/gan/">GAN</a>
  
  <a class="btn btn-primary btn-outline" href="/tags/deep-learning/">deep learning</a>
  
  <a class="btn btn-primary btn-outline" href="/tags/tensorflow/">tensorflow</a>
  
</div>




    
    
    <div class="article-widget">
      <div class="hr-light"></div>
      <h3>Related</h3>
      <ul>
        
        <li><a href="/post/going-deeper-into-batch-normalization/">Going Deeper in Batch Normalization</a></li>
        
        <li><a href="/project/obfuscatedblurred-human-face-reconstruction/">Obfuscated/Blurred Human Face Reconstruction</a></li>
        
      </ul>
    </div>
    

    

    


  </div>
</article>

<footer class="site-footer">
  <div class="container">
    <p class="powered-by">

      &copy; 2018 &middot; 

      Powered by
      
      <a href="https://shen338.github.io/" target="_blank" rel="noopener">Tong Shen</a>. 
	  
	  All rights reserved.

      <span class="pull-right" aria-hidden="true">
        <a href="#" id="back_to_top">
          <span class="button_icon">
            <i class="fa fa-chevron-up fa-2x"></i>
          </span>
        </a>
      </span>

    </p>
  </div>
</footer>


<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <button type="button" class="close btn-large" data-dismiss="modal">&times;</button>
        <h4 class="modal-title">Cite</h4>
      </div>
      <div>
        <pre><code class="modal-body tex"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-primary btn-outline js-copy-cite" href="#" target="_blank">
          <i class="fa fa-copy"></i> Copy
        </a>
        <a class="btn btn-primary btn-outline js-download-cite" href="#" target="_blank">
          <i class="fa fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

    

    
    

    

    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.2.1/jquery.min.js" integrity="sha512-3P8rXCuGJdNZOnUx/03c1jOTnMn3rP63nBip5gOP2qmUh5YAdVAvFZ1E+QLZZbC1rtMrQb+mah3AfYW11RUrWA==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.3/imagesloaded.pkgd.min.js" integrity="sha512-umsR78NN0D23AzgoZ11K7raBD+R6hqKojyBZs1w8WvYlsI+QuKRGBx3LFCwhatzBunCjDuJpDHwxD13sLMbpRA==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha512-iztkobsvnjKfAtTNdHkGVjAYTrrtlC7mGp/54c40wowO7LhURYl3gVzzcEqGl/qKXQltJ2HwMrdLcNUdo+N/RQ==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.4/isotope.pkgd.min.js" integrity="sha512-VDBOIlDbuC4VWxGJNmuFRQ0Li0SKkDpmGyuhAG5LTDLd/dJ/S0WMVxriR2Y+CyPL5gzjpN4f/6iqWVBJlht0tQ==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>
    
    
    <script src="/js/hugo-academic.js"></script>
    

    
    
      
      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js" integrity="sha256-/BfiIkHlHoVihZdc6TFuj7MmJ0TWcWsMXkeDFwhi0zw=" crossorigin="anonymous"></script>
      

      

      

      <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']] } });
    </script>
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML" integrity="sha512-tOav5w1OjvsSJzePRtt2uQPFwBoHt1VZcUq8l8nm5284LEKE9FSJBQryzMBzHxY5P0zRdNqEcpLIRVYFNgu1jw==" crossorigin="anonymous"></script>
    
    

  </body>
</html>

