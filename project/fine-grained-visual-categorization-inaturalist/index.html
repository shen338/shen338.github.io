<!DOCTYPE html>
<html lang="en-us">
<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="theme" content="hugo-academic">
  <meta name="generator" content="Hugo 0.40.3" />
  <meta name="author" content="Tong Shen">

  
  
  
  
    
      
    
  
  <meta name="description" content="Table of Content  Overview Main Difficulties  Data Imbalance Weakly supervised label Data insufficiency  Fine-tuned model on ImageNet pretrained model. Fine-grained Classification  Super-category classification Fine-grained classification in single super-category  Bilinear model Attention model(Class Activation Map)   Reference materials  Overview I run into this challenge at early April. This dataset contains about 0.4 million images spanning over 8142 categories. Without expert knowledge, many of these species are extremely difficult to accurately classify due to their visual similarity.">

  
  <link rel="alternate" hreflang="en-us" href="/project/fine-grained-visual-categorization-inaturalist/">

  


  

  
  
  <meta name="theme-color" content="#0095eb">
  
  
  
  
    
  
  
    
    
      
        <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">
      
    
  
  
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha512-6MXa8B6uaO18Hid6blRMetEIoPqHf7Ux1tnyIQdpt9qI5OACx7C+O3IVTr98vwGnlcg0LOLa02i9Y1HpVhlfiw==" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha512-SfTiTlX6kk+qitfevl/7LibUOeJWlt9rbyDn92a1DqWOw9vWG2MFoays0sgObmWazO5BQPiFucnnEAjpAB+/Sw==" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">
  
  
  
  
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Lato:400,700%7cMerriweather%7cRoboto&#43;Mono%7cPermanent&#43;Marker%7cPrata">
  
  <link rel="stylesheet" href="/styles.css">
  

  

  
  <link rel="alternate" href="/index.xml" type="application/rss+xml" title="Tong Shen">
  <link rel="feed" href="/index.xml" type="application/rss+xml" title="Tong Shen">
  

  <link rel="manifest" href="/site.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="/project/fine-grained-visual-categorization-inaturalist/">

  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="og:site_name" content="Tong Shen">
  <meta property="og:url" content="/project/fine-grained-visual-categorization-inaturalist/">
  <meta property="og:title" content=" Fine-Grained Visual Categorization on iNaturalist dataset | Tong Shen">
  <meta property="og:description" content="Table of Content  Overview Main Difficulties  Data Imbalance Weakly supervised label Data insufficiency  Fine-tuned model on ImageNet pretrained model. Fine-grained Classification  Super-category classification Fine-grained classification in single super-category  Bilinear model Attention model(Class Activation Map)   Reference materials  Overview I run into this challenge at early April. This dataset contains about 0.4 million images spanning over 8142 categories. Without expert knowledge, many of these species are extremely difficult to accurately classify due to their visual similarity.">
  <meta property="og:locale" content="en-us">
  
  <meta property="article:published_time" content="2017-11-27T00:00:00-05:00">
  
  <meta property="article:modified_time" content="2017-11-27T00:00:00-05:00">
  

  
  

  <title> Fine-Grained Visual Categorization on iNaturalist dataset | Tong Shen</title>

</head>
<body id="top" data-spy="scroll" data-target="#toc" data-offset="71" >

<nav class="navbar navbar-default navbar-fixed-top" id="navbar-main">
  <div class="container">

    
    <div class="navbar-header">
      
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse"
              data-target=".navbar-collapse" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      
      <a class="navbar-brand" href="/">Tong Shen</a>
    </div>

    
    <div class="collapse navbar-collapse">

      
      
      <ul class="nav navbar-nav navbar-right">
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#about">
            
            <span>Home</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#myself">
            
            <span>CV</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#projects">
            
            <span>Projects</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#posts">
            
            <span>Posts</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#publications_selected">
            
            <span>Publications</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#contact">
            
            <span>Contact</span>
            
          </a>
        </li>

        
        
      

      
      </ul>

    </div>
  </div>
</nav>


<article class="article article-project" itemscope itemtype="http://schema.org/Article">

  


  <div class="article-container">

    <div class="pub-title">
      <h1 itemprop="name"> Fine-Grained Visual Categorization on iNaturalist dataset</h1>
      <span class="pub-authors" itemprop="author">&nbsp;</span>
      <span class="pull-right">
        
<div class="share-box" aria-hidden="true">
  <ul class="share">
    <li>
      <a class="twitter"
         href="https://twitter.com/intent/tweet?text=%20Fine-Grained%20Visual%20Categorization%20on%20iNaturalist%20dataset&amp;url=%2fproject%2ffine-grained-visual-categorization-inaturalist%2f"
         target="_blank" rel="noopener">
        <i class="fa fa-twitter"></i>
      </a>
    </li>
    <li>
      <a class="facebook"
         href="https://www.facebook.com/sharer.php?u=%2fproject%2ffine-grained-visual-categorization-inaturalist%2f"
         target="_blank" rel="noopener">
        <i class="fa fa-facebook"></i>
      </a>
    </li>
    <li>
      <a class="linkedin"
         href="https://www.linkedin.com/shareArticle?mini=true&amp;url=%2fproject%2ffine-grained-visual-categorization-inaturalist%2f&amp;title=%20Fine-Grained%20Visual%20Categorization%20on%20iNaturalist%20dataset"
         target="_blank" rel="noopener">
        <i class="fa fa-linkedin"></i>
      </a>
    </li>
    <li>
      <a class="weibo"
         href="http://service.weibo.com/share/share.php?url=%2fproject%2ffine-grained-visual-categorization-inaturalist%2f&amp;title=%20Fine-Grained%20Visual%20Categorization%20on%20iNaturalist%20dataset"
         target="_blank" rel="noopener">
        <i class="fa fa-weibo"></i>
      </a>
    </li>
    <li>
      <a class="email"
         href="mailto:?subject=%20Fine-Grained%20Visual%20Categorization%20on%20iNaturalist%20dataset&amp;body=%2fproject%2ffine-grained-visual-categorization-inaturalist%2f">
        <i class="fa fa-envelope"></i>
      </a>
    </li>
  </ul>
</div>


      </span>
    </div>

    

    <div class="article-style" itemprop="articleBody">
      

<h2 id="table-of-content">Table of Content</h2>

<ul>
<li><a href="#overview">Overview</a></li>
<li><a href="#main-difficulties">Main Difficulties</a>

<ul>
<li><a href="#data-imbalance">Data Imbalance</a></li>
<li><a href="#weakly-supervised-label">Weakly supervised label</a></li>
<li><a href="#data-insufficiency">Data insufficiency</a></li>
</ul></li>
<li><a href="#fine-tuned-model-on-imagenet-pretrained-model">Fine-tuned model on ImageNet pretrained model.</a></li>
<li><a href="#fine-grained-classification">Fine-grained Classification</a>

<ul>
<li><a href="#super-category-classification">Super-category classification</a></li>
<li><a href="#fine-grained-classification-in-single-super-category">Fine-grained classification in single super-category</a>

<ul>
<li><a href="#bilinear-model">Bilinear model</a></li>
<li><a href="#attention-model-class-activation-map-">Attention model(Class Activation Map)</a></li>
</ul></li>
</ul></li>
<li><a href="#reference-materials">Reference materials</a></li>
</ul>

<h2 id="overview">Overview</h2>

<p>I run into this challenge at early April. This dataset contains about 0.4 million images spanning over 8142 categories. Without expert knowledge, many of these species are extremely difficult to accurately classify due to their visual similarity. This classification task require us to features a large number of fine-grained
categories over class imbalance.</p>

<h2 id="main-difficulties">Main Difficulties</h2>

<h3 id="data-imbalance">Data Imbalance</h3>

<p>The dataset is quite imbalance, some classes have more than 1000 images, while other classes only have less than 10 images. Here is a table of class image
number distribution.</p>

<table>
<thead>
<tr>
<th>image number range</th>
<th>class number</th>
<th>percentage</th>
</tr>
</thead>

<tbody>
<tr>
<td>0&lt;n&lt;=10</td>
<td>472</td>
<td>5.78%</td>
</tr>

<tr>
<td>10&lt;n&lt;=25</td>
<td>4675</td>
<td>57.4%</td>
</tr>

<tr>
<td>25&lt;n&lt;=50</td>
<td>1599</td>
<td>19.6%</td>
</tr>

<tr>
<td>50&lt;n&lt;=100</td>
<td>554</td>
<td>6.80%</td>
</tr>

<tr>
<td>100&lt;n&lt;=200</td>
<td>399</td>
<td>4.90%</td>
</tr>

<tr>
<td>200&lt;n&lt;=1000</td>
<td>443</td>
<td>5.44%</td>
</tr>
</tbody>
</table>

<p>Over one half classes have image number in range $[10, 25]$, while the other classes are ranged in $[0, 1000]$. Some method or tricks to compensate classs imbalance is necessary here. Otherwise our model would easily get crashed. The majority class image number range is $[10, 25]$, while some other classes can easily get 1000, which can totally destory the training for the majority class.</p>

<p>First, we just abandon the excessive images in class with more than 200 images. Second, oversample the minority class and put more aggressive image augmentation
to these classes. For example, affine transformation like random rotate, crop, shear, flap, blurring like gaussian blur, random dropout and so on. Image augmentation is implemented with Augmentor. Finally, we utilize weighted cross entropy loss function, further compensate the remaining
imbalance effect in our dataset. Here is the code for Multiprocessing Augmentor Pipeline:</p>

<pre><code class="language-python">
# Multiprocessing Augmentor Pipeline
def pipeline():

    P1 = Augmentor.Pipeline()
    P1.flip_left_right(probability=0.3)
    P1.flip_top_bottom(probability=0.05)
    P1.rotate90(probability=0.1)
    P1.rotate_without_crop(probability=0.1, max_left_rotation=15, max_right_rotation=15)
    P1.random_distortion(probability=0.1, grid_width=10, grid_height=10, magnitude=1)
    P1.skew_corner(probability=0.1)
    P1.shear(probability=0.1, max_shear_left=10, max_shear_right=10)
    P1.crop_centre(probability=1.0, percentage_area=0.9)
    P1.crop_random(probability=1.0, percentage_area=0.9)

    return P1


def aug_func(imgs, label, q):

    P = pipeline()
    imgs = np.uint8(imgs)
    g = P.keras_generator_from_array(imgs, label, batch_size=imgs.shape[0]
                                     , image_data_format='WTF')
    i, l = next(g)
    q.put([i, l])


def aug_parallel(imgs, labels):

    queue = Queue()
    cpus = multiprocessing.cpu_count()
    step = int(imgs.shape[0] / cpus)
    processes = []
    for ii in range(cpus):

        p = Process(target=aug_func,
                    args=(imgs[ii * step:(ii + 1) * step, :, :, :],
                          labels[ii * step:(ii + 1) * step],
                          queue))
        processes.append(p)
        p.start()

    rets = []
    labels = []

    for p in processes:
        ret, l = queue.get() 
        rets.append(ret)
        labels.append(l)
    for p in processes:
        p.join()

    if step == 1:
        rets = np.squeeze(np.array(rets))
        labels = np.squeeze(np.array(labels), axis=1)
    else:
        rets = np.concatenate(rets, axis=0)
        labels = np.concatenate(labels, axis=0)

    return rets, labels


</code></pre>

<h3 id="weakly-supervised-label">Weakly supervised label</h3>

<p>After looking into some image samples in the dataset, we find the the image labels are just weakly supervised labels. In other words, usually only
a small area in the image contains the target object. For example, one image is labeled as sunflowers, but the sunflower only appear at one corner,
and the background is grass and woods.</p>

<p>This kind of labeling will apparently increase the difficulty of precise image classification because we have to do something to localize the object in the
image and prevent the background from affecting the classification result. That&rsquo;s the main reason we incorporate attention model afterwards.</p>

<p>Here are some cases of weakly supervised labels:</p>

<table>
<thead>
<tr>
<th>Image</th>
<th>Image</th>
</tr>
</thead>

<tbody>
<tr>
<td><img src="/img/Weak1.jpg" alt="" /></td>
<td><img src="/img/weak2.png" alt="" /></td>
</tr>
</tbody>
</table>

<h3 id="data-insufficiency">Data insufficiency</h3>

<p>Although we have about 450,000 images in total, when averaged to every class, we only have 50 images left. And the majority class image number range is $[10, 25]$.
With so few images, it would be impossible to train a model from scratch. The most popular solution would be adopt a ImageNet pretrained model and fine-tuned on our dataset.</p>

<p>Our first choice is the <a href="https://arxiv.org/abs/1602.07261" target="_blank">Inception Resnet V2</a> network from google. This network is well designed to save parameters and achieve
good performance at the same time. The same with previous Inception module, Inception resnet v2 also implements nx1 and 1xn convolution to replace nxn convolution, also bottleneck architecture to reduce computation. Besides, Inception Resnet V2 introduces residual connection from ResNet architecture to
eases the gradient vanishing problem when training deep neural nets. And the total layer number of the network goes to 467.</p>

<h2 id="fine-tuned-model-on-imagenet-pretrained-model">Fine-tuned model on ImageNet pretrained model.</h2>

<p>The first trail is directly classification over 8142 categories. Here is an architecture diagram of Inception Resnet V2:</p>

<p><img src="/img/InceptionResnetv2.jpg" alt="InceptionResnetv2.jpg" /></p>

<p>We first freeze the CNN feature extraction part, and train our own classifier - the final layers
of the network. Note that we have to training two classifier, one at the end of network, the auxiliary one at the middle. The middle one is used to provide additional
gradient to help the network to avoid gradient vanishing.
After 5 epoches of training, the accurarcy goes up to 40%.</p>

<p>And after that, we use a relatively low learning rate to fine-tune the network to avoid too much modification on the original network. After about 15 epoches, the training accuracy goes up to about 80% and the validation accuracy is about 60%, the top3 accuracy on test set is about 80%. Apparently there are some kinds of overfit, but this is inevitable with so little data. We will further try to freeze the starting layers throughtout the traiing process to add some regulate the expression capacity of the network. Hope that works.</p>

<h2 id="fine-grained-classification">Fine-grained Classification</h2>

<h3 id="super-category-classification">Super-category classification</h3>

<p>After finishing the previous model with InceptionResnetv2, we carefully examined our result. We found that our model often predicted three very similar classes, like three different sunflowers with very subtle difference. This means our model may have a hard time to classify images over similar classes. So, the next step is fine grained classification over these sub species. First, we divide our dataset into 11 super categories, like plants, insects, birds and so on. We consider this problem in a high dimensional space, and treat the image classes as clusters. The cluster distance of classes in the same super-category should be signaficantly smaller than cluster distance of classes in different super-categories. But the our image labelling cannot represent this property. After one-hot encoding, we treat every image class equivalently and assume the distance between them are the same.</p>

<p>According to these analysis, we decided to classify images into their super-categories and do fine-grained classification in every single super-category. In this way, our model would concentrate more on the fine details of similar objects in the same super-category, without wasting resources on the easy cross super-category classification.</p>

<p>Here is a diagram of image space:</p>

<p><img src="/img/Figure_1.png" alt="Diagram of image space" /></p>

<p>It would be more efficient to classify ABC and DE first. In this way, the class distance variance would be much less, we can use relatively less resource to achieve comparable performance to direct classification.</p>

<h3 id="fine-grained-classification-in-single-super-category">Fine-grained classification in single super-category</h3>

<h4 id="bilinear-model">Bilinear model</h4>

<p>The first model is <a href="http://vis-www.cs.umass.edu/bcnn/docs/bcnn_iccv15.pdf" target="_blank">bilinear model</a>. This method considers the interaction between different feature map channels. If the feature map has the size $[H, W, C]$, treat every pixel in feature map as a feature vector, feature vectors has the size $[1,C]$. And the bilinear model takes the outer product of every feature vector and forms a $[H, W, C, C]$ feature map. After taking global pooling, we get a feature map [C, C]. In this process, bilinear model utilizes higher order information of the feature map. According to our test, its performance is pretty good. It achieves nearly 90% accuracy in single super-category classification. But when applying it to bigger super-category, our GPU memory is not enough. The reason is after bilinear operation, the feature map size becomes so big $(C^2)$, so the paremeters in following fully connected layers become overwhelming. For example, if the feature map has 512 channels, and our biggest super-category has more than 3000 classes, the parameters in FC layer is $512^2*3000 = 78 million$, which is more than all the CNN layers combined.</p>

<h4 id="attention-model-class-activation-map">Attention model(Class Activation Map)</h4>

<p>Due to the weakly supervised nature of the dataset (discussed <a href="#weakly-supervised-label">above</a>), we want to first roughly localize the target object in the image before fine-grained classification.
Here attention model comes to the rescue. With attention model, we can determine which part of images matters for the network to make predictions, which should be the object we want to localize. Since we already have the network of super-category classification, its feature map can be utilized to generate class activation map <a href="http://cnnlocalization.csail.mit.edu/Zhou_Learning_Deep_Features_CVPR_2016_paper.pdf" target="_blank">(CAM)</a> to represent the attention level of input images.
Here is a diagram of CAM:</p>

<p><img src="/img/CAM.png" alt="CAM" /></p>

<p>As shown in the diagram, we can use the feature map and fc layer weight to generate the attention map for images with super-category network. Put an image into the network, we can get a feature map $[H, W, C]$ and a prediction. In order to map the global pooling layers
to predictions, the FC layer weight is $W \in R^{C \times N}$, where N is the output class number. If the prediction is $j$, CAM is calculated as follows:</p>

<p>$$CAM = \sum_{k=0}^C F(k)*W_{k,j}$$</p>

<p>where F(k) represent kth feature map channel. Alternatively, we can use top N prediction if top 1 prediction is not confident. Afterwards, we upsample CAM to fit input image size.</p>

<p>After this, we used OSTU algorithm to make the CAM into binary image and extract the biggest contour and generate bounding box via OpenCV. Here are some results without any cherry picking:</p>

<p>First column is original images and generated bounding box; second column is CAM heatmap; third column is the binary image after OSTU algorithm and the biggest contour.</p>

<p><img src="/img/CAM_result.png" alt="drawing" style="width: 500px;"/></p>

<p>The result is pretty amazing. Regardless of the type of objects, like plants, birds, insects, we can accurately locate them.</p>

<h4 id="classification-after-localization">Classification after Localization</h4>

<p>Still in process.</p>

<h2 id="reference-materials">Reference materials</h2>

<ol>
<li><a href="https://arxiv.org/abs/1602.07261" target="_blank">Inception ResNet V2 Paper</a></li>
<li><a href="http://vis-www.cs.umass.edu/bcnn/docs/bcnn_iccv15.pdf" target="_blank">Bilinear Model</a></li>
<li><a href="http://cnnlocalization.csail.mit.edu/Zhou_Learning_Deep_Features_CVPR_2016_paper.pdf" target="_blank">Class Activation Map Paper</a></li>
</ol>

    </div>

    


<div class="article-tags">
  
  <a class="btn btn-primary btn-outline" href="/tags/deep-learning/">deep-learning</a>
  
  <a class="btn btn-primary btn-outline" href="/tags/image-classification/">image classification</a>
  
</div>




    
    
    

    
      
      
      
      

      
      
      
      
    

  </div>
</article>



<footer class="site-footer">
  <div class="container">
    <p class="powered-by">

      &copy; 2018 &middot; 

      Powered by
      
      <a href="https://shen338.github.io/" target="_blank" rel="noopener">Tong Shen</a>. 
	  
	  All rights reserved.

      <span class="pull-right" aria-hidden="true">
        <a href="#" id="back_to_top">
          <span class="button_icon">
            <i class="fa fa-chevron-up fa-2x"></i>
          </span>
        </a>
      </span>

    </p>
  </div>
</footer>


<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <button type="button" class="close btn-large" data-dismiss="modal">&times;</button>
        <h4 class="modal-title">Cite</h4>
      </div>
      <div>
        <pre><code class="modal-body tex"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-primary btn-outline js-copy-cite" href="#" target="_blank">
          <i class="fa fa-copy"></i> Copy
        </a>
        <a class="btn btn-primary btn-outline js-download-cite" href="#" target="_blank">
          <i class="fa fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

    

    
    

    

    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.2.1/jquery.min.js" integrity="sha512-3P8rXCuGJdNZOnUx/03c1jOTnMn3rP63nBip5gOP2qmUh5YAdVAvFZ1E+QLZZbC1rtMrQb+mah3AfYW11RUrWA==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.3/imagesloaded.pkgd.min.js" integrity="sha512-umsR78NN0D23AzgoZ11K7raBD+R6hqKojyBZs1w8WvYlsI+QuKRGBx3LFCwhatzBunCjDuJpDHwxD13sLMbpRA==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha512-iztkobsvnjKfAtTNdHkGVjAYTrrtlC7mGp/54c40wowO7LhURYl3gVzzcEqGl/qKXQltJ2HwMrdLcNUdo+N/RQ==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.4/isotope.pkgd.min.js" integrity="sha512-VDBOIlDbuC4VWxGJNmuFRQ0Li0SKkDpmGyuhAG5LTDLd/dJ/S0WMVxriR2Y+CyPL5gzjpN4f/6iqWVBJlht0tQ==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>
    
    
    <script src="/js/hugo-academic.js"></script>
    

    
    
      
      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js" integrity="sha256-/BfiIkHlHoVihZdc6TFuj7MmJ0TWcWsMXkeDFwhi0zw=" crossorigin="anonymous"></script>
      

      

      

      <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']] } });
    </script>
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML" integrity="sha512-tOav5w1OjvsSJzePRtt2uQPFwBoHt1VZcUq8l8nm5284LEKE9FSJBQryzMBzHxY5P0zRdNqEcpLIRVYFNgu1jw==" crossorigin="anonymous"></script>
    
    

  </body>
</html>

