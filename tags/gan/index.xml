<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>GAN on Tong Shen</title>
    <link>https://shen338.github.io/tags/gan/</link>
    <description>Recent content in GAN on Tong Shen</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Sat, 17 Feb 2018 21:00:00 -0500</lastBuildDate>
    
	<atom:link href="https://shen338.github.io/tags/gan/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Amazing GAN - Wasserstein GAN</title>
      <link>https://shen338.github.io/post/amazing-gan---wasserstein-gan/</link>
      <pubDate>Sat, 17 Feb 2018 21:00:00 -0500</pubDate>
      
      <guid>https://shen338.github.io/post/amazing-gan---wasserstein-gan/</guid>
      <description>Table of Content  KL divergence and JS divergence Generative Adversarial Networks  Global optimal loss  Problem with Vanilla GANs  Gradient Vanishing Mode Collapse  Improved Training of GANs Wasserstein GAN  Earth Mover distance Comparasion between EM distance and KL/JS divergence Lipschitz continuity Modified Algorithm Wasserstein GAN with gradient penalty  Disadvantages of gradient clipping in WGAN Gradient Penalty   Reference Materials   KL divergence and JS divergence Before diving into details, let first review two very important metrics to quantify the similarity of two probability distributions: Kullback-Leibler Divergence and Jensen-Shannon Divergence.</description>
    </item>
    
    <item>
      <title>Obfuscated/Blurred Human Face Reconstruction</title>
      <link>https://shen338.github.io/project/obfuscatedblurred-human-face-reconstruction/</link>
      <pubDate>Mon, 27 Nov 2017 00:00:00 -0500</pubDate>
      
      <guid>https://shen338.github.io/project/obfuscatedblurred-human-face-reconstruction/</guid>
      <description>Table of Contents  Data preparation Super Resolution ResNet Generative Adversarial Network Model  Nowadays, a lot of images on the Internet are intentionally blurred or mosaiced due to various reasons. The main objective of this project is to reconstruct these images, especially heavily blurred ones, to their original high-resolution counterpart. This problem is an ill-posed problems, we need to predict fine-grained details only based on little information on degenerated images.</description>
    </item>
    
  </channel>
</rss>