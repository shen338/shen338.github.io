<!DOCTYPE html>
<html lang="en-us">
<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="theme" content="hugo-academic">
  <meta name="generator" content="Hugo 0.42" />
  <meta name="author" content="Tong Shen">

  
  
  
  
    
      
    
  
  <meta name="description" content="Table of Content  History  AlexNet ResNet Inception Network Series Deformable Parts Model Overfeat  Metrics: mAP  Precision and Recall Average precision Mean Average Precision  R-CNN Families  RCNN  Selective Search Bounding Box Regression Non Maximum Suppression Workflow Speed Bottleneck  Fast-RCNN  ROI pooling Workflow  Faster-RCNN  Region Proposal Network Workflow  Mask-RCNN  ROI Align Key Point Detection   Single Shot Detector  Single Shot MultiBox Detector  Architecture Implementation Details  YOLOv1  Architecture Loss function Workflow Limitation  YOLOv2  Improvements  YOLOv3 Feature Pyramid Network (FPN)  Architecture Feature Pyramid Networks for RPN Feature Pyramid Networks for Faster/Mask R-CNN Experimental Result  Focal Loss and RetinaNet  Class Imbalance RetinaNet Implementation Details Results   Anchor Free Detector  CornerNet Corner and Center Net CenterNet  2d Object Detection 3D object detection and human pose   Summary Reference Materials  It&rsquo;s well known that deep learning has been a real game changer in machine learning, especially in computer vision.">

  
  <link rel="alternate" hreflang="en-us" href="/post/neural-network-based-object-recognition-fast-faster-mask-r-cnn-ssd-and-yolo/">

  


  

  
  
  <meta name="theme-color" content="#0095eb">
  
  
  
  
    
  
  
    
    
      
        <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">
      
    
  
  
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha512-6MXa8B6uaO18Hid6blRMetEIoPqHf7Ux1tnyIQdpt9qI5OACx7C+O3IVTr98vwGnlcg0LOLa02i9Y1HpVhlfiw==" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha512-SfTiTlX6kk+qitfevl/7LibUOeJWlt9rbyDn92a1DqWOw9vWG2MFoays0sgObmWazO5BQPiFucnnEAjpAB+/Sw==" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">
  
  
  
  
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Lato:400,700%7cMerriweather%7cRoboto&#43;Mono%7cPermanent&#43;Marker%7cPrata">
  
  <link rel="stylesheet" href="/styles.css">
  

  

  
  <link rel="alternate" href="/index.xml" type="application/rss+xml" title="Tong Shen">
  <link rel="feed" href="/index.xml" type="application/rss+xml" title="Tong Shen">
  

  <link rel="manifest" href="/site.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="/post/neural-network-based-object-recognition-fast-faster-mask-r-cnn-ssd-and-yolo/">

  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="og:site_name" content="Tong Shen">
  <meta property="og:url" content="/post/neural-network-based-object-recognition-fast-faster-mask-r-cnn-ssd-and-yolo/">
  <meta property="og:title" content="Neural Network based Object Recognition: Fast/Faster/Mask R-CNN, YOLO, RetinaNet to CenterNet | Tong Shen">
  <meta property="og:description" content="Table of Content  History  AlexNet ResNet Inception Network Series Deformable Parts Model Overfeat  Metrics: mAP  Precision and Recall Average precision Mean Average Precision  R-CNN Families  RCNN  Selective Search Bounding Box Regression Non Maximum Suppression Workflow Speed Bottleneck  Fast-RCNN  ROI pooling Workflow  Faster-RCNN  Region Proposal Network Workflow  Mask-RCNN  ROI Align Key Point Detection   Single Shot Detector  Single Shot MultiBox Detector  Architecture Implementation Details  YOLOv1  Architecture Loss function Workflow Limitation  YOLOv2  Improvements  YOLOv3 Feature Pyramid Network (FPN)  Architecture Feature Pyramid Networks for RPN Feature Pyramid Networks for Faster/Mask R-CNN Experimental Result  Focal Loss and RetinaNet  Class Imbalance RetinaNet Implementation Details Results   Anchor Free Detector  CornerNet Corner and Center Net CenterNet  2d Object Detection 3D object detection and human pose   Summary Reference Materials  It&rsquo;s well known that deep learning has been a real game changer in machine learning, especially in computer vision.">
  <meta property="og:locale" content="en-us">
  
  <meta property="article:published_time" content="2018-08-01T21:00:00-05:00">
  
  <meta property="article:modified_time" content="2018-08-01T21:00:00-05:00">
  

  
  

  <title>Neural Network based Object Recognition: Fast/Faster/Mask R-CNN, YOLO, RetinaNet to CenterNet | Tong Shen</title>

</head>
<body id="top" data-spy="scroll" data-target="#toc" data-offset="71" >

<nav class="navbar navbar-default navbar-fixed-top" id="navbar-main">
  <div class="container">

    
    <div class="navbar-header">
      
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse"
              data-target=".navbar-collapse" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      
      <a class="navbar-brand" href="/">Tong Shen</a>
    </div>

    
    <div class="collapse navbar-collapse">

      
      
      <ul class="nav navbar-nav navbar-right">
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#about">
            
            <span>Home</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#myself">
            
            <span>CV</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#posts">
            
            <span>Posts</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#projects">
            
            <span>Projects</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#publications_selected">
            
            <span>Publications</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#contact">
            
            <span>Contact</span>
            
          </a>
        </li>

        
        
      

      
      </ul>

    </div>
  </div>
</nav>


<article class="article" itemscope itemtype="http://schema.org/Article">

  


  <div class="article-container">
    <h1 itemprop="name">Neural Network based Object Recognition: Fast/Faster/Mask R-CNN, YOLO, RetinaNet to CenterNet</h1>

    

<div class="article-metadata">

  <span class="article-date">
    
    <time datetime="2018-08-01 21:00:00 -0500 CDT" itemprop="datePublished dateModified">
      Aug 1, 2018
    </time>
  </span>
  <span itemscope itemprop="author publisher" itemtype="http://schema.org/Person">
    <meta itemprop="name" content="Tong Shen">
  </span>

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    37 min read
  </span>
  

  
  

  

  
  
<div class="share-box" aria-hidden="true">
  <ul class="share">
    <li>
      <a class="twitter"
         href="https://twitter.com/intent/tweet?text=Neural%20Network%20based%20Object%20Recognition%3a%20Fast%2fFaster%2fMask%20R-CNN%2c%20YOLO%2c%20RetinaNet%20to%20CenterNet&amp;url=%2fpost%2fneural-network-based-object-recognition-fast-faster-mask-r-cnn-ssd-and-yolo%2f"
         target="_blank" rel="noopener">
        <i class="fa fa-twitter"></i>
      </a>
    </li>
    <li>
      <a class="facebook"
         href="https://www.facebook.com/sharer.php?u=%2fpost%2fneural-network-based-object-recognition-fast-faster-mask-r-cnn-ssd-and-yolo%2f"
         target="_blank" rel="noopener">
        <i class="fa fa-facebook"></i>
      </a>
    </li>
    <li>
      <a class="linkedin"
         href="https://www.linkedin.com/shareArticle?mini=true&amp;url=%2fpost%2fneural-network-based-object-recognition-fast-faster-mask-r-cnn-ssd-and-yolo%2f&amp;title=Neural%20Network%20based%20Object%20Recognition%3a%20Fast%2fFaster%2fMask%20R-CNN%2c%20YOLO%2c%20RetinaNet%20to%20CenterNet"
         target="_blank" rel="noopener">
        <i class="fa fa-linkedin"></i>
      </a>
    </li>
    <li>
      <a class="weibo"
         href="http://service.weibo.com/share/share.php?url=%2fpost%2fneural-network-based-object-recognition-fast-faster-mask-r-cnn-ssd-and-yolo%2f&amp;title=Neural%20Network%20based%20Object%20Recognition%3a%20Fast%2fFaster%2fMask%20R-CNN%2c%20YOLO%2c%20RetinaNet%20to%20CenterNet"
         target="_blank" rel="noopener">
        <i class="fa fa-weibo"></i>
      </a>
    </li>
    <li>
      <a class="email"
         href="mailto:?subject=Neural%20Network%20based%20Object%20Recognition%3a%20Fast%2fFaster%2fMask%20R-CNN%2c%20YOLO%2c%20RetinaNet%20to%20CenterNet&amp;body=%2fpost%2fneural-network-based-object-recognition-fast-faster-mask-r-cnn-ssd-and-yolo%2f">
        <i class="fa fa-envelope"></i>
      </a>
    </li>
  </ul>
</div>


  

</div>


    <div class="article-style" itemprop="articleBody">
      

<h2 id="table-of-content">Table of Content</h2>

<ul>
<li><a href="#history">History</a>

<ul>
<li><a href="#alexnet">AlexNet</a></li>
<li><a href="#resnet">ResNet</a></li>
<li><a href="#inception-network-series">Inception Network Series</a></li>
<li><a href="#deformable-parts-model">Deformable Parts Model</a></li>
<li><a href="#overfeat">Overfeat</a></li>
</ul></li>
<li><a href="#metrics--map">Metrics: mAP</a>

<ul>
<li><a href="#precision-and-recall">Precision and Recall</a></li>
<li><a href="#average-precision">Average precision</a></li>
<li><a href="#mean-average-precision">Mean Average Precision</a></li>
</ul></li>
<li><a href="#r-cnn-families">R-CNN Families</a>

<ul>
<li><a href="#rcnn">RCNN</a>

<ul>
<li><a href="#selective-search">Selective Search</a></li>
<li><a href="#bounding-box-regression">Bounding Box Regression</a></li>
<li><a href="#non-maximum-suppression">Non Maximum Suppression</a></li>
<li><a href="#workflow">Workflow</a></li>
<li><a href="#speed-bottleneck">Speed Bottleneck</a></li>
</ul></li>
<li><a href="#fast-rcnn">Fast-RCNN</a>

<ul>
<li><a href="#roi-pooling">ROI pooling</a></li>
<li><a href="#workflow-1">Workflow</a></li>
</ul></li>
<li><a href="#faster-rcnn">Faster-RCNN</a>

<ul>
<li><a href="#region-proposal-network">Region Proposal Network</a></li>
<li><a href="#workflow-2">Workflow</a></li>
</ul></li>
<li><a href="#mask-rcnn">Mask-RCNN</a>

<ul>
<li><a href="#roi-align">ROI Align</a></li>
<li><a href="#key-point-detection">Key Point Detection</a></li>
</ul></li>
</ul></li>
<li><a href="#single-shot-detector">Single Shot Detector</a>

<ul>
<li><a href="#single-shot-multibox-detector">Single Shot MultiBox Detector</a>

<ul>
<li><a href="#architecture">Architecture</a></li>
<li><a href="#implementation-details">Implementation Details</a></li>
</ul></li>
<li><a href="#yolov1">YOLOv1</a>

<ul>
<li><a href="#architecture-1">Architecture</a></li>
<li><a href="#loss-function">Loss function</a></li>
<li><a href="#workflow-3">Workflow</a></li>
<li><a href="#limitation">Limitation</a></li>
</ul></li>
<li><a href="#yolov2">YOLOv2</a>

<ul>
<li><a href="#improvements">Improvements</a></li>
</ul></li>
<li><a href="#yolov3">YOLOv3</a></li>
<li><a href="#feature-pyramid-network--fpn-">Feature Pyramid Network (FPN)</a>

<ul>
<li><a href="#architecture-2">Architecture</a></li>
<li><a href="#feature-pyramid-networks-for-rpn">Feature Pyramid Networks for RPN</a></li>
<li><a href="#feature-pyramid-networks-for-faster-mask-r-cnn">Feature Pyramid Networks for Faster/Mask R-CNN</a></li>
<li><a href="#experimental-result">Experimental Result</a></li>
</ul></li>
<li><a href="#focal-loss-and-retinanet">Focal Loss and RetinaNet</a>

<ul>
<li><a href="#class-imbalance">Class Imbalance</a></li>
<li><a href="#retinanet">RetinaNet</a></li>
<li><a href="#implementation-details-1">Implementation Details</a></li>
<li><a href="#results">Results</a></li>
</ul></li>
</ul></li>
<li><a href="#anchor-free-detector">Anchor Free Detector</a>

<ul>
<li><a href="#cornernet">CornerNet</a></li>
<li><a href="#corner-and-center-net">Corner and Center Net</a></li>
<li><a href="#centernet">CenterNet</a>

<ul>
<li><a href="#2d-object-detection">2d Object Detection</a></li>
<li><a href="#3d-object-detection-and-human-pose">3D object detection and human pose</a></li>
</ul></li>
</ul></li>
<li><a href="#summary">Summary</a></li>
<li><a href="#reference-materials">Reference Materials</a></li>
</ul>

<p>It&rsquo;s well known that deep learning has been a real game changer in machine learning, especially in computer vision. Similar to image classification, deep learning/neural network represent the state of the art in modern object recognition.</p>

<h1 id="history">History</h1>

<p>To have a better intuition about the challenges and algorithm evolving, first we will have an overview about the progress of deep learning approach in the last couple of years. This progress also comes with the progress of image classification.</p>

<h3 id="alexnet">AlexNet</h3>

<p><a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" target="_blank">AlexNet</a> famously won the 2012 ImageNet LSVRC-2012 competition by a large margin (15.3% VS 26.2% (second place) error rates). It stands for the revival of neural network as well as deep learning.</p>

<p>The main contribution of AlexNet is as follows:</p>

<ol>
<li>Use ReLU activation function instead of tanh and sigmoid to introduce nonlinearity to the network. Also, it eases the gradient vanishing problem when training deep neural network.</li>
<li>Introduce dropout as a regularization. It works like an ensemble of different networks.</li>
<li>Use two NVIDIA GPUs and CUDA, also parallel architecture to accelerate training.</li>
</ol>

<p>Here is a diagram of famous AlexNet:</p>

<p><img src="/img/AlexNet.png" alt="AlexNet" /></p>

<h3 id="resnet">ResNet</h3>

<p>After AlexNet, the next significant work should be <a href="https://arxiv.org/abs/1512.03385" target="_blank">ResNet</a>.<br />
These networks led to 1st-place winning entries in all five main tracks of the ImageNet and COCO 2015 competitions, which covered image classification, object detection, and semantic segmentation.</p>

<p>The main difference of ResNet is just a skip connection between layers. The author’s hypothesis is that it is easy to optimize the residual mapping function F(x) than to optimize the original, unreferenced mapping H(x). And this method turns out to be very efficient to solve network degradation in deep network.</p>

<p>Here is a diagram of residual blocks (image source: ):</p>

<p><img src="/img/residualblock.png" alt="residual" /></p>

<p>Plus, Resnet uses the bottleneck architecture from Inception network to reduce computation. After $1\times1$ convolution, the feature map channels are greatly reduced in order to save computation for next $3 \times 3$ convolution layer. Another $1\times1$ convolution after that can resume the feature map channels. Amazingly, this operation won&rsquo;t affect the performance. So, this bottleneck architecture is quite popular in modern neural network.</p>

<p><img src="/img/bottleneck.jpg" alt="bottleneck" /></p>

<p>ResNet can be interpreted as ensembles of relatively shallow networks, as described <a href="https://arxiv.org/abs/1605.06431?context=cs" target="_blank">here</a>.</p>

<h3 id="inception-network-series">Inception Network Series</h3>

<p>Inception network series comes from Google. It should be the most well-designed and popular network in image classification community. The architecture becomes very complicated in later version of Inception networks. For example, here is a diagram of InceptionResnetV2, with more than 600 layers:</p>

<p><img src="/img/image00.png" alt="inceptionresnetv2" /></p>

<p>The core of Inception network is the Inception module. The inspiration comes from the idea that you need to make a decision as to what type of convolution you want to make at each layer: $3 \times 3$ or $5 \times 5$? You may need specific image content to decide. How about use them all together? Here come the Inception module:</p>

<p><img src="/img/inception_implement.png" alt="Inceptionmodule" /></p>

<p>It uses all $1 \times 1$, $3 \times 3$, $5 \times 5$ convolution and  altogether and concate their result together. Plus, inception module introduces $N \times 1$ and $1 \times N$ convolution instead of traditional $N \times N $ convolution. This puts constraints on convolution kernel (center symmetric), but can greatly reduces computation. And residual connection, bottleneck architecture also appears in modern Inception network. Here is a diagram of Inception modules in InceptionV4:</p>

<p><img src="/img/inceptionv4.jpeg" alt="v4" /></p>

<h3 id="deformable-parts-model">Deformable Parts Model</h3>

<p><a href="http://vision.stanford.edu/teaching/cs231b_spring1213/slides/dpm-slides-ross-girshick.pdf" target="_blank">Deformable Parts Model</a> was invented by Pedro Felzenszwalb ands Ross Girshick, who becomes the leader of object recognition commumity with his R-CNN families.</p>

<p>The model consists of three major components:</p>

<ol>
<li>A coarse root filter defines a detection window that approximately covers an entire object. A filter specifies weights for a region feature vector.</li>
<li>Multiple part filters that cover smaller parts of the object. Parts filters are learned at twice resolution of the root filter.</li>
<li>A spatial model for scoring the locations of part filters relative to the root.</li>
</ol>

<p>Deformable Part Models model an object as the sum of the geometric deformations of it. So take person model for example:</p>

<p><img src="/img/DPM.jpeg" alt="DPM" /></p>

<p>A root location with high score detects a region with high chances to contain an object, while the locations of the parts with high scores confirm a recognized object hypothesis. The paper adopted latent SVM to model the classifier.</p>

<p><img src="/img/DPMmodel.png" alt="DPMmodel" /></p>

<p>Also, DPM and later CNN models are not two different method to object recognition. Actually, DPM can be unrolled and interpreted as several equivalent CNN layers. See Ross&rsquo;s paper <a href="https://arxiv.org/abs/1409.5403" target="_blank">Deformable Part Models are Convolutional Neural Networks</a></p>

<h3 id="overfeat">Overfeat</h3>

<p><a href="https://arxiv.org/abs/1312.6229" target="_blank">Overfeat</a> is a poineer to incorprate CNN models into object detection, localization and classification tasks.</p>

<h1 id="metrics-map">Metrics: mAP</h1>

<p>mAP is the metric to measure the accuracy of object detectors like Faster R-CNN, SSD, etc. AP is the average of the maximum precisions at different recall values. And mAP is the mean of AP in different classes. This sounds complicated, so let&rsquo;s explain this step by step.</p>

<h3 id="precision-and-recall">Precision and Recall</h3>

<p>According to <a href="https://en.wikipedia.org/wiki/Precision_and_recall#F-measure" target="_blank">wiki</a>, <strong>precision</strong> (also called positive predictive value) is the fraction of relevant instances among the retrieved instances, for example, percentage of your positive predictions are correct.</p>

<p><strong>Recall</strong> (also known as sensitivity) is the fraction of relevant instances that have been retrieved over the total amount of relevant instances. for example, percentage of positive instances in your positive predictions.</p>

<p>Here is an image of precision and recall:</p>

<p><img src="/img/precision-recall.jpg" alt="precision and recall" /></p>

<h3 id="average-precision">Average precision</h3>

<p>First of all, a prediction is consider to be correct if IoU is greater than a threshold, usually 0.5. It is hard to explain AP by words, so let&rsquo;s create an example to make it clear. (<a href="https://medium.com/@jonathan_hui/map-mean-average-precision-for-object-detection-45c121a31173" target="_blank">example source</a>)</p>

<p>For example, if our image has 5 objects. First, we sort the predictions of our detector based on the confidence level. We get correct prediction in 1st, 2nd, 4th, 6th and 10th prediction. So, in the top 4 predictions, we get 3 of them correct. A prediction is consider to be correct if IoU is greater than 0.5. So the precision and the recall for our top 4 predictions is:</p>

<p><strong>Precision</strong> is the proportion of TP = <sup>3</sup>&frasl;<sub>4</sub> = 0.75.</p>

<p><strong>Recall</strong> is the proportion of TP out of the possible positives = <sup>3</sup>&frasl;<sub>5</sub> = 0.6.</p>

<p>Here is a table of different corresponding precision and recall ranks:</p>

<p><img src="/img/precisionrecallrank.png" alt="prrank" /></p>

<p>AP (average precision) is computed as the average of <strong>maximum precision</strong> at different recall levels. Here are two kinds of AP calculation:</p>

<ul>
<li>Before 2010, people get the precision value at 11 recall levels (0.0, 0.1, 0.2 &hellip; 1.0), and average them.</li>
</ul>

<p>$$AP = \frac{1}{11} (AP_r(0.0) + AP_r(0.1) + &hellip; AP_r(1.0))$$</p>

<ul>
<li>After 2010, people calculate the precision value at all different recall levels and average them. Here K is the number of total different recall levels.<br /></li>
</ul>

<p>$$AP = \sum_{i = 0}^K AP_r(i)$$</p>

<p>The $AP_r$ represent the <strong>maximum precision</strong> in certain recall level.</p>

<h3 id="mean-average-precision">Mean Average Precision</h3>

<p>Latest research papers tend to give results for the COCO dataset only. For COCO, AP is the average over multiple IoU. <strong>AP@[.5:.95]</strong> corresponds to the average AP for IoU from 0.5 to 0.95 with a step size of 0.05. AP (which is also called mAP in COCO) averages AP over all class categories.</p>

<h1 id="r-cnn-families">R-CNN Families</h1>

<h2 id="rcnn">RCNN</h2>

<p>The goal of <a href="https://arxiv.org/abs/1311.2524" target="_blank">R-CNN</a> is to take in an image, and correctly identify where are objects (via a bounding box) in the image. The intuition of RCNN is quite simple, just proposing some possible regions, and determine whether they are objects or not. Here is a diagram of RCNN:</p>

<p><img src="/img/RCNN.jpg" alt="" /></p>

<h3 id="selective-search">Selective Search</h3>

<p>R-CNN uses Selective Search method to extract region proposals.</p>

<p>This method uses an over-segmentation method to divide the image into small regions (1k to 2k). Merge the two most probable adjacent areas according to the consolidation rules. This is is a hierarchical grouping algorithm by J.R.R. Uijlings.</p>

<p><img src="/img/grouping.png" alt="grouping" /></p>

<p>Actually the similarity of regions is determined by their color histogram, texture(gradient histogram) and some other aspects. Note that this method tends to merge relatively small regions to avoid a large area gradually &ldquo;eat&rdquo; other small areas. Also, the method tend to merge the regions whose merged result takes large percentage in its bounding box.</p>

<p>Repeat until the entire image merges into one area position. Output all areas that existed once, so-called candidate areas. And generate bounding box for them as region proposals.</p>

<h3 id="bounding-box-regression">Bounding Box Regression</h3>

<p>The Bounding Box Regressors are essential because the initial region proposals might not fully coincide with the region that is indicated by the learned features of the Convolutional Neural Network. It is a kind of a refinement step.</p>

<p>So, based on the features obtained at the end of the final pooling layer, the region proposals are regressed. Note that only when region proposals have large IoU(&gt;0.6) with ground truth, we can treat this transformation as linear model and do regression. The bounding box regression process is quite straight forward in RCNN paper.</p>

<p><img src="/img/bbregression.png" alt="bbregression" /></p>

<p>We directly regress the location $P_x$ and $P_y$, while regress the width $P_w$ and height $P_h$ in log-space which produce better results on the wide range of bounding-box shift.</p>

<h3 id="non-maximum-suppression">Non Maximum Suppression</h3>

<p>In the end, we can have a lot of bounding boxes on an object. We need to determine the best bounding box for this object. So, what NMS does is as follows:</p>

<ol>
<li>First calculate the area of each bounding box, and then sort according to the score</li>
<li>Use the largest bounding box with the score as the selected box, calculate the rest of the bounding box and the current maximum score and the IoU of the box, remove the IoU greater than the set threshold Bounding box.</li>
<li>Then repeat the above process until the candidate bounding box is empty</li>
<li>Delete the selected box with the score less than a certain threshold to get this kind of result</li>
</ol>

<p><img src="/img/NMS.png" alt="" /></p>

<h3 id="workflow">Workflow</h3>

<p>R-CNN is formulated as follows:</p>

<ol>
<li>Apply <a href="#selective-search">selective search</a> to input image, and generate ~2k candidates per image containing target objects with different sizes. Region candidates are warped to have a fixed size as required by CNN.</li>
<li>Start from a pre-train a CNN network on image classification tasks. Continue fine-tuning the CNN on warped proposal regions for K + 1 classes; The additional one class refers to the background (no object of interest). In the fine-tuning stage, we should use a much smaller learning rate and the mini-batch oversamples the positive cases because most proposed regions are just background.</li>
<li>Given every image region, one forward propagation through the CNN generates a feature vector. This feature vector is then consumed by a binary SVM trained for each class independently.
The positive samples are proposed regions with IoU (intersection over union) overlap threshold &gt;= 0.3, and negative samples are irrelevant others.</li>
<li>Use <a href="#bounding-box-regression">bounding box regression</a> to refine the region proposals with CNN feature vector. Also <a href="#non-maximum-suppression">NMS</a> to delete redundant region proposals.</li>
</ol>

<h3 id="speed-bottleneck">Speed Bottleneck</h3>

<ol>
<li>Selective search for proposal is slow.</li>
<li>Calculate feature vectors for every region proposals.</li>
<li>Detection process consists of three steps without any computation sharing.</li>
</ol>

<p>Later work mainly concentrate on the three aspects above to achieve real-time detection.</p>

<h2 id="fast-rcnn">Fast-RCNN</h2>

<p>Fast RCNN mainly deal with the second speed bottleneck of RCNN: Calculate feature vectors for every region proposals.</p>

<p>Instead of extracting CNN feature vectors independently for each region proposal, this model aggregates them into one CNN forward pass over the entire image and the region proposals share this feature matrix. Then the same feature matrix is branched out to be used for learning the object classifier (No SVM but softmax) and the bounding-box regressor.</p>

<p><img src="/img/fastrcnn.png" alt="" /></p>

<h3 id="roi-pooling">ROI pooling</h3>

<p>Indeed, there is nothing magic about ROI pooling. It is used to generate fixed size feature map regardless of the input image(region proposal) size.</p>

<p>For example, the feature size of a projected region is $h \times w$, and we want a fixed feature map with size $H \times W$. ROI pooling will divide original feature map into $H \times W$ grids, approximately every subwindow of size $h/H \times w/W$. Then apply max-pooling in each grid.</p>

<p>Here is a diagram of ROI pooling from cs231n Lecture:</p>

<p><img src="/img/roi-pooling.png" width="600"></p>

<h3 id="workflow-1">Workflow</h3>

<p>Fast R-CNN is formulated as follows:</p>

<ol>
<li>The same with R-CNN, Apply <a href="#selective-search">selective search</a> to input image, and generate ~2k candidates per image containing target objects with different sizes.</li>
<li>Start from a pretrained ImageNet model.

<ul>
<li>Replace the last max pooling layer of the pre-trained CNN with a RoI pooling layer to ensure fixed output size.</li>
<li>Different from R-CNN, only do CNN forward pass once, and directly map region proposals to their corresponding feature map. Shared computation speed up the whole process.</li>
<li>Replace the last fully connected layer and the last softmax layer (K classes) with a fully connected layer and softmax over K + 1 classes.</li>
</ul></li>
<li>Finally the model branches into two output layers:

<ul>
<li>A softmax classifier of K + 1 classes (extra one is the background class), output a discrete probability distribution per RoI.</li>
<li>A bounding-box regression model to refine original RoI for each of K classes.</li>
<li>The overall loss function is a combination of classification loss and bounding box location loss.
<br />
<br /></li>
</ul></li>
</ol>

<h2 id="faster-rcnn">Faster-RCNN</h2>

<p>The intuition of <a href="https://arxiv.org/pdf/1506.01497.pdf" target="_blank">Faster-RCNN</a> is simple: since CNN is so good and efficient, and using selective search is too slow, why not also using CNN to generate region proposals? So, the main difference between faster-RCNN and fast RCNN is the region proposal network (RPN) to generate region proposals with CNN.</p>

<p>Here is an illustratioon of Faster RCNN:</p>

<p><img src="/img/faster-RCNN.png" width="800"></p>

<h3 id="region-proposal-network">Region Proposal Network</h3>

<p>Region proposal network consists of three parts:</p>

<ol>
<li>Feed input image into CNN and get a set of convlutional feature maps on the last convolutional layer.</li>
<li>Then a sliding window is running spatially on these feature maps. The size of sliding window is $3 \times 3$. For each sliding window, a set of 9 anchors are generated which all have the same center (the same with sliding window) but with 3 different aspect ratios (1:2, 1:1, 2:1) and 3 different scales. Note that all these coordinates are computed with respect to the original image. (image <a href="https://www.quora.com/How-does-the-region-proposal-network-RPN-in-Faster-R-CNN-work" target="_blank">source</a>)
<img src="/img/anchor.png" width="600"></li>
<li>Finally, the 3×3 spatial features extracted from convolution feature maps are fed to a smaller network which has two tasks: classification (cls) and regression (reg). The output of regressor determines a a predicted bounding box (x,y,w,h), The classification network produces a probability p indicating whether the the predicted box contains an object (1) or it is from background (0 for no object).
<img src="/img/rpn-end.jpg" width="600"></li>
</ol>

<p>So, the output of rpn is similar to selective search method, which is ~2k region proposals. But the speed of RPN is much faster than selective search method. In this way, the last speed bottleneck of RCNN is addressed. Note that if accuracy is not important, RPN can complete object recognition by its own by modify the classification into N classes.</p>

<h3 id="workflow-2">Workflow</h3>

<p>Except the RPN, the rest of Faster R-CNN is the same with Fast R-CNN. Faster R-CNN is formulated as follows:</p>

<ol>
<li>Start from ImageNet pretrained model, fine-tune the RPN (region proposal network) end-to-end for the region proposal task. Positive samples have IoU (intersection-over-union) &gt; 0.7, while negative samples have IoU &lt; 0.3.</li>
<li>Train a Fast R-CNN object detection model using the proposals generated by the current RPN</li>
<li>Then use the Fast R-CNN network to initialize RPN training. While keeping the shared convolutional layers, only fine-tune the RPN-specific (background/object classification and bounding box regression) layers. At this stage, RPN and the detection network share convolutional layers.</li>
<li>Finally fine-tune the unique layers of Fast R-CNN</li>
</ol>

<h2 id="mask-rcnn">Mask-RCNN</h2>

<p>So far, we’ve seen how CNN features are interesting and effective in object recognition with bounding boxes. Recently, Kaiming He, Ross Girshick and a team of researchers uses CNN features to another important aspect of object recognition: instance segmentation. They name their work <a href="https://arxiv.org/abs/1703.06870" target="_blank">Mask RCNN</a>.</p>

<p>Semantic segmentation need to segment and cluster pixels of different object classes in a single graph; while instance segmentation need to locate every single object and cluster pixels of this object. This image shows the difference of semantic and instance segmentation:</p>

<p><img src="/img/instancesegmentation.png" width="600"></p>

<p>So, overall, instance is more difficult than semantic segmentation. And it is highly related to object detection. For semantic segmentation, most modern frameworks are based on <a href="https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf" target="_blank">Fully Convolutional Neural Networks (FCN)</a>. FCN managed to extract the region of objects in an image but it cannot tell us the exact region of every single object. Here, Mask RCNN combined RCN and Faster RCNN, adding an extra FCN branch to faster RCNN, parallel to localization and classification, to make pixel level prediction and instance segmentation in a single bounding box.</p>

<p><img src="/img/mask-rcnn.png" width="600"></p>

<h3 id="roi-align">ROI Align</h3>

<p>Different to object detection, pixel level prediction need more fine-grained alignment rather than simple ROI pooling. In ROI pooling, the location information is quantified twice:</p>

<ol>
<li>Quantize region proposal coordinates as integer point coordinates</li>
<li>The quantized region is evenly divided into $k \times k$ cells and the boundaries of each cell are quantized.</li>
</ol>

<p><img src="/img/ROIquantize.png" alt="" /></p>

<p>From the diagram above, the ground truth bounding box is $655 \times 655$. And after 5 max pooling operation, the corresponding size on feature map is <sup>655</sup>&frasl;<sub>32</sub> = 20.78 , but ROI pooling quantizes it into $20$. After that, we need to divide the bounding box into $7 \times 7$ cells, the size of each cell is <sup>20</sup>&frasl;<sub>7</sub> = 2.86 , but ROI pooling quantizes it into $2$. So, the largest quantization error of ROI pooling is about $1.5$ pixels on feature map, which is $50$ pixels on input image.</p>

<p>What ROIalign does is removing the quantization in ROI pooling.</p>

<ol>
<li>Do not quantize region proposal coordinates, keep it as float numbers.</li>
<li>Do not quantize $k \times k$ cells boundaries when dividing, keep it as float numbers.</li>
<li>Calculate the value of four fixed points in each cell using bilinear interpolation method, and then perform max pooling for these four points.</li>
</ol>

<p>Here is a figure for ROI align:</p>

<p><img src="/img/ROIAlignDiagram.png" width="600"></p>

<p>And according to the paper, four fixed points in a cell works best, but only use one fixed point in a cell merely hurt the performance. Bilinear interpolation is totally differentiable, so back propogation is also straight forward.</p>

<p>The training process is quite similar with faster RCNN, except for an extra predicting head for segmentation.</p>

<h3 id="key-point-detection">Key Point Detection</h3>

<p>In addition, mask RCNN can handle human pose key point detection. We just need to replace the segmentation mask with one-hot key point mask. In mask RCNN, human pose has 17 key points. The result of Mask RCNN is pretty good:</p>

<p><img src="/img/maskrcnnresult.png" alt="" /></p>

<h1 id="single-shot-detector">Single Shot Detector</h1>

<p>The previous methods of object detection all share one thing in common: they have one part of their network dedicated to providing region proposals followed by a high quality classifier to classify these proposals. These methods are very accurate but come at a big computational cost. In embedded systems, we have to control the computation cost. These method may not be suitable.</p>

<p>Another way of doing object detection is by combining these two tasks into one network. Similar to region proposal network, we use different pre-defined boxes with various shape to capture objects, and predict class scores and bounding box offsets.</p>

<h2 id="single-shot-multibox-detector">Single Shot MultiBox Detector</h2>

<p><a href="https://arxiv.org/abs/1512.02325" target="_blank">Single Shot MultiBox Detector</a> (by C. Szegedy et al.) was released at the end of 2016 and reaches the state-of-the-art performance of object recognition task in real time (58 FPS on a Nvidia Titan X).</p>

<p>The name of SSD describes itself very well. Single Shot mean this method only need to run CNN forward pass once; Multibox means this method uses multiple boxes (different shape, on multiple CNN feature map scales) to detect object; detector means emm&hellip;. Let&rsquo;s dive into the details of SSD:</p>

<h3 id="architecture">Architecture</h3>

<p>Here is a diagram of SSD architecture:(image <a href="https://www.slideshare.net/xavigiro/ssd-single-shot-multibox-detector" target="_blank">source</a>)</p>

<p><img src="/img/SSDarchitecture.png" alt="" /></p>

<p>The main property of SSD is that it uses feature map from different CNN layers with different scale. SSD is built on VGG net and has six feature maps in total, each responsible for a different scale of objects, allowing it to identify objects across a large range of scales.</p>

<p><img src="/img/ssdwork.png" width="600"></p>

<p>The figure above shows how SSD works. In a convolutional fashion, we evaluate a small set
of default boxes (like <a href="#region-proposal-network">RPN anchor boxes</a> of different aspect ratios at each location in several feature maps with
different scales (e.g. 8 $\times$ 8 and 4 $\times$ 4 in b and c). For each default box, we predict
both the shape offsets and the confidences for all object categories $((c_1, c_2, · · · , c_p))$.
At training time, we first match these default boxes to the ground truth boxes. For
example, we have matched two default boxes with the cat and one with the dog, which
are treated as positives and the rest as negatives. Any default box with an IOU of 0.5 or greater with a ground truth box is considered a match.</p>

<p>For each default boxes of each cell, SSD predict two things:</p>

<ol>
<li>Confident scores for all object categories $((c_1, c_2, · · · , c_p))$ (the same to RCNN, an extra background class indicating no object in this box)</li>
<li>An offset vector with 4 entries $(C_x, C_y, C_w, C_h)$ to refine the matched default boxes to ground truth boxes.</li>
</ol>

<p>And the model loss is a weighted sum
between localization loss (e.g. Smooth L1) and confidence loss (e.g. Softmax). In the figure above, only three default get matched. All other default boxes need to predict a background class.</p>

<p>SSD is quite similar to a [RPN]() with multi scale boxes. The default boxes are called prior here. Note that the prior can be customed according to our specific task. For example, if you want to detect pedestrian, you may need default boxes with large height and small width.</p>

<h3 id="implementation-details">Implementation Details</h3>

<ol>
<li>Start from pretrained ImageNet model (VGG), get multiple-scale convolutional feature map.</li>
<li>Hard negative Mining: during training, as most of the bounding boxes will have low IoU, interpreted as negative training examples. We may have disproportionate amount of negative examples. The paper suggests keeping a ratio of negative to positive examples of around 3:1.</li>
<li>Data Augmentation: Sample patches with 0.1, 0.3, 0.5, 0.7, or 0.9 overlap to the ground truth; Random sample patches; 0.5 probability horizonal flip.</li>
</ol>

<h2 id="yolov1">YOLOv1</h2>

<p><a href="https://pjreddie.com/darknet/yolo/" target="_blank">You only look once (YOLO)</a> is a state-of-the-art, real-time object detection system. On a Pascal Titan X it processes images at 30 FPS and has a mAP of 57.9% on COCO test-dev. More importantly, YOLO keeps evolving in recent years.</p>

<h3 id="architecture-1">Architecture</h3>

<p>YOLOv1 is a little bit less precise than R-CNN families, but it is the first work to realize real time detection with CNN. Similar to other single shot detectors, the intuition of YOLOv1 is quite straight forward.</p>

<p><img src="/img/YOLOv1.png" alt="" /></p>

<p>YOLOv1 models detection as a regression problem. It divides the image into an S × S grid and for each grid cell predicts B bounding boxes, confidence for those boxes, and C class probabilities. These predictions are encoded as an S × S × (B × 5 + C) tensor. For example, on PASCAL VOC, S = 7,
B = 2. PASCAL VOC has 20 labelled classes so C = 20. Our final prediction is a 7 × 7 × 30 tensor.</p>

<p><img src="/img/YOLOv1arch.png" width="800"></p>

<p>The figure above is the architecture of YOLOv1. Based on pretrain the convolutional layers on the ImageNet classification
task at half the resolution (224 × 224 input image) and then double the resolution for detection.</p>

<h3 id="loss-function">Loss function</h3>

<p>Here is the multi-part loss function that we want to optimize:</p>

<p><img src="/img/YOLOv1lossfunction.png" width="600"></p>

<p>where:</p>

<ul>
<li>B: Number of bounding boxes (2)</li>
<li>$x_i,y_i,w_i,h_i$:  Bounding box parameters</li>
<li>$C_i$: Some particular class i</li>
<li>S: Grid size (7)</li>
<li>$\bf{1}^{obj}_i$ : If object appear on the cell i, if does not appear it will be zero</li>
<li>$\bf{1}^{obj}_{ij}$ : Bounding box j, from cell i responsible for prediction (i.e. has the highest
IOU of any predictor in that grid cell)</li>
</ul>

<p>And the weight in loss function $\lambda_{coord} = 5$ and $\lambda_{noobj} = 0.5$. Different to bounding box regression, YOLO uses square root instead of logrithm, but they should act in the same way. Note that the loss function <strong>only</strong> penalizes classification
error if an object is present in that grid cell (hence the conditional
class probability discussed earlier). It also <strong>only</strong> penalizes
bounding box coordinate error if that predictor is
“responsible” for the ground truth box (i.e. has the highest
IOU of any predictor in that grid cell). So, the first step of every training phase is <strong>bounding box matching</strong>.</p>

<h3 id="workflow-3">Workflow</h3>

<ol>
<li>Start from a pre-train CNN network on image classification tasks.</li>
<li>Split an image into S x S cells. Each cell is responsible for identifying the object (if any) with its <strong>center</strong> located in this cell. Each cell predicts the location of B bounding boxes and a confidence score, and a probability of object class conditioned on the existence of an object in the bounding box.

<ul>
<li>A bounding box is defined by a tuple of $(C_x, C_y, C_w, C_h)$. x and y are normalized to be the offsets of a cell location; w and h are normalized by the image width and height, and thus between (0, 1].</li>
<li>A confidence score is: probability(containing an object) x IoU(pred, truth).</li>
<li>If the cell contains an object, it predicts a probability of this object belonging to one class $C_i$, i=1,2,…, K. At this stage, the model only predicts one set of class probabilities per cell, regardless of the number of boxes B.<br /></li>
</ul></li>
<li>The final layer of the pre-trained CNN is modified to output a prediction tensor of size S x S x (5B + C).</li>
</ol>

<h3 id="limitation">Limitation</h3>

<p>YOLOv1 also have a lot of limitations:</p>

<ol>
<li>Each grid cell only predicts two boxes and can only have one class. This prevent YOLO from detecting nearby objects.</li>
<li>Compared to SSD, YOLO only uses coarse features. Cannot detect relative small objects.</li>
<li>YOLO has higher localization errors and the recall is lower compared to SSD.</li>
<li>The final FC layers are slow and kind of redundant.</li>
</ol>

<h2 id="yolov2">YOLOv2</h2>

<p>Inspired by faster RCNN and SSD, <a href="https://arxiv.org/pdf/1612.08242.pdf" target="_blank">YOLOv2</a> improves the accuracy significantly while making it faster.</p>

<h3 id="improvements">Improvements</h3>

<p>First, YOLOv2 is <strong>Better</strong>:</p>

<ol>
<li><strong>Batch Normalization</strong>. Batch normalization leads to significant
improvements in convergence while eliminating the
need for other forms of regularization. Improve mAP by 2%. See my another blog for <a href="https://shen338.github.io/post/going-deeper-into-batch-normalization/" target="_blank">batch norm</a></li>
<li><strong>High Resolution Classifier</strong>: we first fine tune the classification network
at the full 448 × 448 resolution for 10 epochs on ImageNet different to original use 224 × 224 network directly for 448 × 448 detection. Improve mAP by 4%.</li>
<li><strong>Convolutional With Anchor Boxes</strong>: We remove the fully connected layers from YOLO and
use anchor boxes like <a href="#faster-rcnn">faster RCNN</a> to predict bounding boxes. No improvement on mAP but 7% increase on recall rate, which makes the model has more room to improve.</li>
<li><strong>Dimension Clusters</strong>: The author run K-means algorithm to determine the anchor box sizes. Shown in the following figure, the authors choose
k = 5 as a good tradeoff between model complexity and
high recall.
<img src="/img/clusterbox.png" width="600"></li>
<li><strong>Direct location prediction</strong>: Instead of predicting offsets we follow the approach of
YOLO and predict location coordinates relative to the location
of the grid cell. (Predict coordinates directly and predict scale using archor boxes). Improve YOLO by 5%. The network predicts 5 coordinates
for each bounding box, $t_x, t_y, t_w, t_h$, and $t_o.$ If the cell is
offset from the top left corner of the image by $(c_x, c_y)$ and
the bounding box prior has width and height $p_w, p_h$, then
the predictions correspond to:
<img src="/img/YOLOv2direct.png" width="600"></li>
<li><strong>Fine-Grained Features</strong>: Similar to faster RCNN and SSD, adding a passthrough layer that brings
features from an earlier layer at 26 × 26 resolution along with original 13 × 13 resolution. Improve mAP about 1%.</li>
<li><strong>Multi-Scale Training</strong>: During training, YOLO takes images of size {320, 352, &hellip;, 608} (with a step of 32 = YOLO downsampling rate). For every 10 batches, YOLOv2 randomly selects another image size to train the model. This acts as data augmentation and forces the network to predict well for different input image dimension and scale.</li>
</ol>

<p>Second, YOLOv2 is <strong>Faster</strong>:</p>

<ol>
<li>GoogleNet. Reduce billions of floating points operations but YOLO’s
custom model gets 88.0% ImageNet compared to 90.0% for
VGG-16.</li>
<li>DarkNet 19: Compare to VGG 19, double the number of channels after every
pooling step. Replace redundant FC layer with global average pooling; use 1 × 1 filters to compress the feature representation
between 3 × 3 convolutions (bottleneck). Requires only 5.58 billion operations and 91.2% top-5 accuracy on ImageNet.</li>
<li>Although SSD500 runs at 45 FPS, a lower resolution version of YOLOv2 with mAP 76.8 (the same as SSD500) runs at 67 FPS, thus showing us the high performance capabilities of YOLOv2 as a result of its design choices.</li>
</ol>

<p>Third, YOLOv2 is <strong>Stronger</strong>:</p>

<p>Hierarchical classification: Without going into details, YOLO combines labels in different datasets to form a tree-like structure WordTree. The children form an is a relationship with its parent like biplane is a plane. But the merged labels are now not mutually exclusive.</p>

<p>YOLO9000 extends YOLO to detect objects over 9000 classes using hierarchical classification with a 9418 node WordTree. It combines samples from COCO and the top 9000 classes from the ImageNet.  ImageNet is a much larger dataset so we balance
the dataset by oversampling COCO so that ImageNet
is only larger by a factor of 4:1. It learns to find objects using the detection data in COCO and to classify these objects with ImageNet samples.</p>

<p>YOLO9000 evaluates its result from the ImageNet object detection dataset which has 200 categories. It shares about 44 categories with COCO. But for another 156 categories, YOLO is never trained how to locate them. But YOLO9000 gets 19.7 mAP overall with 16.0 mAP on those 156 categories. YOLO9000 performs well with new species of animals not found in COCO because their shapes can be generalized easily from their parent classes.</p>

<h2 id="yolov3">YOLOv3</h2>

<h2 id="feature-pyramid-network-fpn">Feature Pyramid Network (FPN)</h2>

<p>In tradition neural network, due to pooling layer, early layers tends to have higher resolution and more spatial information; while higher layers tend to have more semantic information and low resolution. Before FPN, people often use a pyramid of images or features to utilize the advantages of different feature scales (see following figure). But this method are compute and memory intensive. And features close to input image is not so effective to detect objects.</p>

<p><a href="https://arxiv.org/pdf/1612.03144.pdf" target="_blank">Feature Pyramid Network (FPN)</a> extends the idea of feature pyramid, incorprating a topdown architecture with lateral connections to different feature map scales. FPN shows significant improvement as a generic feature extractor
in several applications.</p>

<p><img src="/img/FPN.png" alt="" /></p>

<h3 id="architecture-2">Architecture</h3>

<p>FPN composes of a bottom-up and a top-down pathway. The bottom-up pathway is the usual convolutional network for feature extraction. Close to the end of CNN, the feature maps tend to have more semantic information but low resolution.</p>

<p>FPN also provide top-down path to preserve the semantic information from higher layer to lower, high resolution layer. In this way, we can utilize the advantages of different feature scales. As move up in CNN, the spatial dimension is reduced by <sup>1</sup>&frasl;<sub>2</sub>, so we need to upsample it back in top-down path (using
nearest neighbor upsampling for simplicity). And before every lateral connection, faeture maps undergo a 1×1 convolutional layer to reduce channel dimensions. The top-down path is just elementwise addition from different lateral connection.  Finally,
a 3×3 convolution is appended on each merged map to
generate the final feature map, which is to reduce the aliasing
effect of upsampling. Here is an illustration of FPN:</p>

<p><img src="/img/FPNarch.png" width="600"></p>

<h3 id="feature-pyramid-networks-for-rpn">Feature Pyramid Networks for RPN</h3>

<p>Similar to the idea of <a href="#ii">SSD</a>, RPN with feature pyramid network put anchor boxes in different feature map scales (15 in total). Formally, define the anchors
to have areas of $[32^2, 64^2, 128^2, 256^2, 512^2]$ pixels
on $[P_2, P_3, P_4, P_5, P_6]$ respectively.</p>

<p>According to the authors, the parameters of the heads are shared
across all feature pyramid levels; they have also evaluated the
alternative without sharing parameters and observed similar
accuracy.</p>

<h3 id="feature-pyramid-networks-for-faster-mask-r-cnn">Feature Pyramid Networks for Faster/Mask R-CNN</h3>

<p>To use FPN in Faster/Mask R-CNN, we need
to assign RoIs of different scales to the pyramid levels. We have to assign ROIs to a proper scale and easy to extract features. So, if ROI is bigger, it should use the feature map in higher level and vice versa. Formally, we assign an RoI of
width w and height h (on the input image to the network) to
the level $P_k$ of our feature pyramid by:</p>

<p><img src="/img/FPNformula.png" width="400"></p>

<p>where:</p>

<ul>
<li>$k_0 = 4$, Analogous to the ResNet-based
Faster R-CNN system that uses C4 as the single-scale
feature map</li>
<li>k indicates using $P_k$ from FPN</li>
</ul>

<h3 id="experimental-result">Experimental Result</h3>

<p>FPN for region proposal network:</p>

<p><img src="/img/FPN-RPN.png" width="1000"></p>

<p>FPN for faster RCNN:</p>

<p><img src="/img/FPN-FasterRCNN.png" width="1000"></p>

<p>FPN on COCO dateset:</p>

<p><img src="/img/FPNcoco.png" width="1000"></p>

<p>Top-down pathway plus lateral connections increases 8.0 points over the single-scale RPN
baseline. For small objects, it improves 12.9 points. Recently, FPN has enabled new top results in all tracks
of the COCO competition, including detection, instance
segmentation, and keypoint estimation.</p>

<p>Currently, FPN is a standard baseline of almost every object recognition system.</p>

<h2 id="focal-loss-and-retinanet">Focal Loss and RetinaNet</h2>

<p>One stage detectors are applied over a regular, dense sampling of object locations, scales, and aspect ratios.  Recent work on one-stage detectors, like YOLO and SSD, yielding faster detectors with accuracy within 10-
40% relative to state-of-the-art two-stage methods, like RCNN families.</p>

<p>The work <a href="https://arxiv.org/pdf/1708.02002.pdf" target="_blank">Focal Loss for Dense Object Detection</a> from facebook AI research pushs this envelop: theironestage
object detector that, for the first time, matches the
state-of-the-art, more complex two stage detectors.</p>

<h3 id="class-imbalance">Class Imbalance</h3>

<p>The authors identify class imbalance as the main obstacle hurting the performance of one stage detectors. Unlike two stage detectors, one stage detecors must process a much
larger set of candidate object locations regularly sampled
across an image(~100k), and most of them are easy samples. Detectors can easily classify them as background or object. Although their gradient is not large, their overwhelming amount may influence the performance of detectors. Hilariously, these easy samples are called <strong>&ldquo;inliers&rdquo;</strong> in thie paper. In previous detectors, people use bootstrapping, hard negative mining to address this problem, which is not good enough.</p>

<p>So, here comes focal loss:</p>

<p><img src="/img/focalloss.png" width="600"></p>

<p>The focal loss just modify the cross entropy loss into weight cross entropy. The weight is given by the prediction confidence: $(1 − p_t)^{\gamma}$. In this way, the weight for very confident sample will decrease a lot during training, which perfectly addresses the class imbalance problem discussed above. In experiment, the recommended value of $\gamma$ is 2. Also, you can slightly improve the accuracy by using $\alpha$ balanced form ($\alpha$ is weight on rare class, $\gamma = 2$, $\alpha = -0.25$ works best):</p>

<p>$$FL(p_t) = - \alpha_t (1 - p_t)^{\gamma} log(p_t)$$</p>

<h3 id="retinanet">RetinaNet</h3>

<p>To express the effective of Focal Loss, the authors designed a simple ont stage detector, <strong>RetinaNet</strong>, achieves a COCO test-dev AP of 39.1
while running at 5 fps, surpassing current all single stage and two stage detectors.</p>

<p>Here is a diagram for RetinaNet architecture:</p>

<p><img src="/img/retinanetarch.png" width="800"></p>

<h3 id="implementation-details-1">Implementation Details</h3>

<p>RetinaNet uses FPN backbone. Reason: preliminary experiments using features
from only the final ResNet layer yielded low AP. Addition to original {1:2, 1:1, 2:1} anchor boxes, RetinaNet add anchor boxes wih size {$2^0, 2^{\frac{1}{3}}, 2^{\frac{2}{3}}$}, totally 9 anchor boxes. This brings in more computation but also high recall and accuracy. Anchor boxes with IOU &gt; 0.5 is treated as positive examples, IOU &lt; 0.4 are negative, 0.4 &lt; IOU &lt; 0.5 are ignored. Also, classification and bounding box regression subnets are sharing a common structure, use separate parameters.</p>

<p>Focal loss in RetinaNet are normalized by the number of anchors assigned to a ground-truth box, since the vast majority of anchors are easy
negatives and receive negligible loss values under the focal loss.</p>

<p>Note that RetinaNet uses an <strong>initialization trick</strong> to avoid overwhelming gradient from frequent class. All new conv layers except the final
one in the RetinaNet subnets are initialized with bias b = 0
and a Gaussian weight fill with $\sigma = 0.01$. For the final conv
layer of the classification subnet, we set the bias initialization
to $b = − log((1 − π)/π)$.</p>

<p>In this way, predictions of first iteration would be centered at $\pi$ (0.01 in experiment, not sensitive at all), which is a relatively small number. Since the frequent class is negative, the gradient of frequent, negative sample would be less than positive, rare samples. In this way, we can compensate the gradient advantage of frequent samples and avoid too large loss values in the first iteration of training.</p>

<h3 id="results">Results</h3>

<p><img src="/img/retinanetresult.png" width="1000"></p>

<p>According this figure, RetinaNet achieves top results,
outperforming both one-stage and two-stage models. Plus, compared to existing
one-stage methods, RetinaNet achieves a healthy
5.9 point AP gap (39.1 vs. 33.2) with the closest competitor,
DSSD.</p>

<h1 id="anchor-free-detector">Anchor Free Detector</h1>

<p>Anchor-based object detectors have two main drawbacks:</p>

<ol>
<li>It typically use large amount of anchors, and only a tiny fraction of them are classified as target objects and the majority of them is abandoned in classification and NMS.</li>
<li>It introduces a lot of hyperparameters, like anchor sizes, aspect ratios. Faster-RCNN chooses these parameters in certain fashion, and YOLO use K-means to find most frequent anchor box sizes. This will make the detector only concentrate on certain dataset/scenario, not good for generalization.<br /></li>
</ol>

<p>So, how about anchor-free detectors?</p>

<h2 id="cornernet">CornerNet</h2>

<p><a href="https://arxiv.org/pdf/1808.01244.pdf" target="_blank">CornerNet</a> is the first anchor free detector invented recently. It introduced a novel idea: how about detect the upper-left and bottom-right corner of an object instead of using anchor boxes to fit certain object? Since the two corners can determine a bounding box, detect corner is equivalent to detect boxes directly.</p>

<p><img src="/img/cornetnet.jpg" width="800"></p>

<p>To make this idea work, the authors implemented a key point detection network with a few tricks:</p>

<ol>
<li>The detected corner points have embeddings. The embeddings serve to group a pair of
corners that belong to the same object, the network is
trained to predict similar embeddings for them. And the embedding also contains necessary info to classify this object.</li>
<li>The corners are frequently not lies on the object. So, normal pooling techniques won&rsquo;t be able to find the correct corner coordinates. The authors introduces corner pooling: for upper-left corner,  look horizontally towards the right for the
topmost boundary of the object, and look vertically towards the bottom for the leftmost boundary. This technique is efficient to find all the max values in lower and right (like figure below). But it only concerns about the boundary features on the objects, this may cause problems.</li>
</ol>

<p><img src="/img/cornerpooling.jpg" width="600"></p>

<p>For detailed implementation, the author designed three categories of loss functions:</p>

<ol>
<li>Detect corners. The authors used unnormalized 2D Gaussian $ e^{-\frac{x^{2}+y^{2}}{2 \sigma^{2}}} $to penalize negative/wrong corner locations to allow small error and make the objective differentiable. Also, since the majority of candidate corner location will be negative, focal loss would be helpful for this imbalance.<img src="/img/cornerloss_1.jpg" width="600">
where $p_{cij}$ be the score at location (i, j) for class c
in the predicted heatmaps, and let $y_{cij}$ be the “groundtruth” heatmap augmented with the unnormalized Gaussians. N is the number of objects in an image, and
α and β are the hyper-parameters for focal loss.</li>
<li>The output feature map is typically smaller than original image. Some bounding box location errors may occur in the downsampling process since we are predicting on feature map for bounding boxes in the original image. So, the correction term is necessary:
$$L_{o f f}=\frac{1}{N} \sum_{k=1}^{N} \text { SmoothL1 } \operatorname{Loss}\left(\boldsymbol{o}_{k}, \hat{\boldsymbol{o}}_{k}\right)$$
where $\boldsymbol{o}_{k}$ is the ground truth corner location shift. a location (x, y) in the image is mapped to the location $\left(\left\lfloor\frac{x}{n}\right\rfloor,\left\lfloor\frac{y}{n}\right\rfloor\right)$
in the heatmaps, where n is the downsampling factor.  So, the ground truth corner shift is: $\boldsymbol{o}_{k}=\left(\frac{x_{k}}{n}-\left\lfloor\frac{x_{k}}{n}\right\rfloor, \frac{y_{k}}{n}-\left\lfloor\frac{y_{k}}{n}\right\rfloor\right)$</li>
<li>Group corner pairs. There will be a lot of corner points on the feature map, how todetermine if a pair of the top-left corner and
bottom-right corner is from the same bounding? The authors uses embedding similarity to find paired corners. There will be a &ldquo;pull&rdquo; loss to group corners and a &ldquo;push&rdquo; loss to seperate two corners.
$$L_{p u l l}=\frac{1}{N} \sum_{k=1}^{N}\left[\left(e_{t_{k}}-e_{k}\right)^{2}+\left(e_{b_{k}}-e_{k}\right)^{2}\right]$$
$$L_{p u s h}=\frac{1}{N(N-1)} \sum_{k=1}^{N} \sum_{j=1 \atop j \neq k}^{N} \max \left(0, \Delta-\left|e_{k}-e_{j}\right|\right)$$</li>
</ol>

<p>The overall diagram of corner net:
<img src="/img/cornernet.jpg" width="800"></p>

<p>CornerNet is a great anchor free network, but it&rsquo;s not perfect. Only detecting two corner is not enough, which leads to a lot of false positives, especially for small objects. The next two work will incorporate center point as well to improve.</p>

<h2 id="corner-and-center-net">Corner and Center Net</h2>

<p>Although the paper is named <a href="https://arxiv.org/abs/1904.08189" target="_blank">CenterNet: Keypoint Triplets for Object Detection</a>, I&rsquo;d rather call it Corner and Center Net because the name CenterNet goes to the next work we will talk about.</p>

<p>The authors mainly addressed the missings of CornerNet and achieved 47 mAP on COCO. The idea is simple, just use center point features to filter out the incorrect bounding boxes. After the processing of CornerNet, it selects top-k center keypoints according to their scores and map them into original image to determine if they are in the image  center. If cannot find a feature point at center, the bounding boxes will be abandoned.</p>

<p>One trick used to find the center point is center pooling. The detailed process of center pooling is as follows: the backbone outputs a
feature map, and to determine if a pixel in the feature map
is a center keypoint, we need to find the maximum value
in its both horizontal and vertical directions and add them
together.</p>

<p>Another pooling trick is cascade corner pooling. It first looks along a boundary to find a boundary maximum value like CornerNet did, then looks inside along the location of
the boundary maximum value
to find an internal maximum
value, and finally, add the two maximum values together for a corner.</p>

<p><img src="/img/centerpooling.jpg" width="600"></p>

<p>After finding the center points, most incorrect bounding boxes will be filtered out, which leads to a higher mAP. This method performs bery well but the speed and efficiency is not good enough.</p>

<h2 id="centernet">CenterNet</h2>

<p><a href="https://arxiv.org/pdf/1904.07850.pdf" target="_blank">Object as Points</a> totally abandoned the idea of detect corners and totally transformed object detection and other tasks into key point detection. The method is pretty simple and elegant.</p>

<p>CenterNet represent objects by a single point at the bounding box center.  Other properties, such as object size, dimension, 3D extent, orientation, and pose are then regressed directly from image features at the center location. Images are feeded into backbone network and transformed into a headmap. Peaks in the heatmap corresponds to object centers (may off a little bit, corrected afterwards).  Image features at each peak predict the objects bounding box height, weight and other properties.</p>

<h3 id="2d-object-detection">2d Object Detection</h3>

<p><img src="/img/centernetresult.jpg" width="900"></p>

<p>For the loss functions, the first term is key point detection loss, which is exactly the same compared to CornerNet. They both used 2D Gaussian as soft target and focal loss to handle imbalance.</p>

<p>Second term of loss function, discretization error, is also similar to CornerNet. CenterNet predicts a local offset $\hat{O} \in \mathcal{R} \frac{W}{R} \times \frac{H}{R} \times 2$ for each center point and regress it with L1 loss.</p>

<p>$$L_{o f f}=\frac{1}{N} \sum_{p}\left|\hat{O}_{\tilde{p}}-\left(\frac{p}{R}-\tilde{p}\right)\right|$$</p>

<p>Third loss term is regarded to object size. The network will predict horizaontal and vertical size of objects, $s_{k}=\left(x_{2}^{(k)}-x_{1}^{(k)}, y_{2}^{(k)}-y_{1}^{(k)}\right)$ and regress it with L1 loss.</p>

<p>$$L_{s i z e}=\frac{1}{N} \sum_{k=1}^{N}\left|\hat{S}_{p_{k}}-s_{k}\right|$$</p>

<p>So, the overall loss function is:</p>

<p>$$L_{d e t}=L_{k}+\lambda_{s i z e} L_{s i z e}+\lambda_{o f f} L_{o f f}$$</p>

<p>And the author took $\lambda_{s i z e} = 0.1$ and $\lambda_{o f f} = 1$.</p>

<p>And things becomes straightforward in inference. Just predict key point category, coordinate offset, object size and bounding box is obtained.</p>

<p>CenterNet can do a lot more, like 3D object detection and 17 point human pose estimation.</p>

<h3 id="3d-object-detection-and-human-pose">3D object detection and human pose</h3>

<p>3D object detection requires three additional attributes per
center point: depth, 3D dimension, and orientation for each box. CenterNet add three prediction head for each of them. For depth, CenterNet uses the same form of Eigen, $d=1 / \sigma(\hat{d})-1$, since the long depth prediction should be more error-prone. The depth $\hat{D} \in[0,1]^{\frac{W}{R}} \times \frac{H}{R}$is predicted in an additional channel with two conv layers and regressed using L1 loss. The three dimensions are three scalars, which is predicted in the sample fashion compared to 2d cases.</p>

<p>The orientation is encoded in two bins for regression. An 8-scalar encoding is used to
ease learning. The 8 scalars are divided into two groups,
each for an angular bin. One bin is for angles in $B_1 \in [−7\pi/6, \pi/6]$ and $B_2 \in [-\pi/6, 7\pi/6]$. Within each bin, 2 of
the scalars $b_i ∈ R^2$
are used for softmax classification (if
the orientation falls into to this bin i). And the rest 2 scalars
$a_i ∈ R^2$
are for the sin and cos value of in-bin offset (respect to box center).</p>

<p>So, if the prediction $\hat{\alpha}=\left[\hat{b}_{1}, \hat{a}_{1}, \hat{b}_{2}, \hat{a}_{2}\right]$, the overall loss is:</p>

<p>$$L_{o r i}=\frac{1}{N} \sum_{k=1}^{N} \sum_{i=1}^{2}\left(\operatorname{softmax}\left(\hat{b}_{i}, c_{i}\right)+c_{i}\left|\hat{a}_{i}-a_{i}\right|\right)$$</p>

<p>where $c_{i}=\mathbb{1}\left(\theta \in B_{i}\right)$ and $a_{i}=\left(\sin \left(\theta-m_{i}\right), \cos \left(\theta-m_{i}\right)\right)$ and $m_i$ is box center coordinates.</p>

<p>For human pose, it is inherently a key point detection task. CenterNet predict k=17 human joint heatmaps $\hat{\Phi} \in \mathcal{R}^{\frac{W}{R}} \times \frac{H}{R} \times k$  using standard human pose estimation framework and trained with focal loss and local pixel discretication shift loss.</p>

<p>After that, CenterNet snap initial predictions from backbone to closest detected keypoints from standard human pose estimation. It extracts all
keypoint locations with confidence more than 0.1 for each joint type inside the bounding box and regress these points to its closest detected key points. Basically, CenterNet uses standard human pose estimation as target to train its own simplified human pose estimation prediction head.</p>

<p>Here is the result of human pose estimation and 3D object detection.</p>

<p><img src="/img/centernetresult2.jpg" width="1000"></p>

<h1 id="summary">Summary</h1>

<p>From all the discussion above, we can see the rapid progress of object recognition in recent five years. These models inspired and equipped each other to move forward.</p>

<p>Up to now, object tends to converge to a optimal solution. For example, <strong>anchor boxes</strong> from Faster RCNN, <strong>Boxes in multi scale features</strong> from SSD, <strong>image grid prediction</strong> from YOLO, <strong>non-maximal suppression, bounding box regression</strong> from RCNN, <strong>ROIpooling/align</strong> from Fast/Mask RCNN are widely used and become an essential part of object recognition systems.</p>

<p>Also, various clever innovations really boost the performance of object recognition systems, like recent <strong>RetinaNet with Focal Loss</strong>, <strong>Feature Pyramid network</strong>, becomes the standard component of every state-of-the-art system.</p>

<p>One thing for sure, this optimal solution is definitely not the global optimal. Looking forward to the next stage of object recognition!</p>

<h1 id="reference-materials">Reference Materials</h1>

<ol>
<li><a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks" target="_blank">AlexNet</a></li>
<li><a href="https://arxiv.org/abs/1512.03385" target="_blank">ResNet</a></li>
<li><a href="https://arxiv.org/pdf/1409.4842.pdf" target="_blank">GoogLeNet</a></li>
<li><a href="https://arxiv.org/abs/1512.00567" target="_blank">Inception v3 network</a></li>
<li><a href="https://arxiv.org/abs/1602.07261?context=cs" target="_blank">Inception Resnet v2 and Inception v4</a></li>
<li><a href="http://vision.stanford.edu/teaching/cs231b_spring1213/slides/dpm-slides-ross-girshick.pdf" target="_blank">Deformable Part Models</a></li>
<li><a href="https://arxiv.org/abs/1312.6229" target="_blank">Overfeat</a></li>
<li><a href="https://arxiv.org/abs/1311.2524" target="_blank">R-CNN</a></li>
<li><a href="https://arxiv.org/abs/1504.08083" target="_blank">Fast R-CNN</a></li>
<li><a href="https://arxiv.org/abs/1506.01497" target="_blank">Faster R-CNN</a></li>
<li><a href="https://arxiv.org/abs/1703.06870" target="_blank">Mask R-CNN</a></li>
<li><a href="https://arxiv.org/abs/1512.02325" target="_blank">Single Shot Multibox Detector(SSD)</a></li>
<li><a href="https://arxiv.org/abs/1506.02640" target="_blank">YOLO</a></li>
<li><a href="https://arxiv.org/abs/1612.08242" target="_blank">YOLO9000: Better, Faster, Stronger</a></li>
<li><a href="https://pjreddie.com/media/files/papers/YOLOv3.pdf" target="_blank">YOLOv3</a></li>
<li><a href="https://arxiv.org/abs/1612.03144" target="_blank">Feature Pyramid Network</a></li>
<li><a href="https://arxiv.org/abs/1708.02002" target="_blank">Focal Loss and RetinaNet</a></li>
<li><a href="https://arxiv.org/pdf/1808.01244.pdf" target="_blank">CornerNet: Detecting Objects as Paired Keypoints</a></li>
</ol>

    </div>

    


<div class="article-tags">
  
  <a class="btn btn-primary btn-outline" href="/tags/review/">review</a>
  
  <a class="btn btn-primary btn-outline" href="/tags/deep-learning/">deep learning</a>
  
  <a class="btn btn-primary btn-outline" href="/tags/tensorflow/">tensorflow</a>
  
  <a class="btn btn-primary btn-outline" href="/tags/object-recognition/">object recognition</a>
  
</div>




    
    
    <div class="article-widget">
      <div class="hr-light"></div>
      <h3>Related</h3>
      <ul>
        
        <li><a href="/post/capsnet-and-dynamic-routing/">CapsNet and dynamic routing</a></li>
        
        <li><a href="/post/nlp-basics---word2vec/">NLP basics - Word2vec: Skip-gram, CBOW, GloVe</a></li>
        
        <li><a href="/post/going-deeper-into-batch-normalization/">Going Deeper in Batch Normalization</a></li>
        
        <li><a href="/post/amazing-gan---wasserstein-gan/">Amazing GAN - Wasserstein GAN</a></li>
        
      </ul>
    </div>
    

    

    


  </div>
</article>

<footer class="site-footer">
  <div class="container">
    <p class="powered-by">

      &copy; 2018 &middot; 

      Powered by
      
      <a href="https://shen338.github.io/" target="_blank" rel="noopener">Tong Shen</a>. 
	  
	  All rights reserved.

      <span class="pull-right" aria-hidden="true">
        <a href="#" id="back_to_top">
          <span class="button_icon">
            <i class="fa fa-chevron-up fa-2x"></i>
          </span>
        </a>
      </span>

    </p>
  </div>
</footer>


<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <button type="button" class="close btn-large" data-dismiss="modal">&times;</button>
        <h4 class="modal-title">Cite</h4>
      </div>
      <div>
        <pre><code class="modal-body tex"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-primary btn-outline js-copy-cite" href="#" target="_blank">
          <i class="fa fa-copy"></i> Copy
        </a>
        <a class="btn btn-primary btn-outline js-download-cite" href="#" target="_blank">
          <i class="fa fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

    

    
    

    

    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.2.1/jquery.min.js" integrity="sha512-3P8rXCuGJdNZOnUx/03c1jOTnMn3rP63nBip5gOP2qmUh5YAdVAvFZ1E+QLZZbC1rtMrQb+mah3AfYW11RUrWA==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.3/imagesloaded.pkgd.min.js" integrity="sha512-umsR78NN0D23AzgoZ11K7raBD+R6hqKojyBZs1w8WvYlsI+QuKRGBx3LFCwhatzBunCjDuJpDHwxD13sLMbpRA==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha512-iztkobsvnjKfAtTNdHkGVjAYTrrtlC7mGp/54c40wowO7LhURYl3gVzzcEqGl/qKXQltJ2HwMrdLcNUdo+N/RQ==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.4/isotope.pkgd.min.js" integrity="sha512-VDBOIlDbuC4VWxGJNmuFRQ0Li0SKkDpmGyuhAG5LTDLd/dJ/S0WMVxriR2Y+CyPL5gzjpN4f/6iqWVBJlht0tQ==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>
    
    
    <script src="/js/hugo-academic.js"></script>
    

    
    
      
      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js" integrity="sha256-/BfiIkHlHoVihZdc6TFuj7MmJ0TWcWsMXkeDFwhi0zw=" crossorigin="anonymous"></script>
      

      

      

      <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']] } });
    </script>
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML" integrity="sha512-tOav5w1OjvsSJzePRtt2uQPFwBoHt1VZcUq8l8nm5284LEKE9FSJBQryzMBzHxY5P0zRdNqEcpLIRVYFNgu1jw==" crossorigin="anonymous"></script>
    
    

  </body>
</html>

