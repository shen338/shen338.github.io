<!DOCTYPE html>
<html lang="en-us">
<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="theme" content="hugo-academic">
  <meta name="generator" content="Hugo 0.40.3" />
  <meta name="author" content="Tong Shen">

  
  
  
  
    
      
    
  
  <meta name="description" content="Table of Contents  Data preparation Super Resolution ResNet Generative Adversarial Network Model  Nowadays, a lot of images on the Internet are intentionally blurred or mosaiced due to various reasons. The main objective of this project is to reconstruct these images, especially heavily blurred ones, to their original high-resolution counterpart. This problem is an ill-posed problems, we need to predict fine-grained details only based on little information on degenerated images.">

  
  <link rel="alternate" hreflang="en-us" href="/project/obfuscatedblurred-human-face-reconstruction/">

  


  

  
  
  <meta name="theme-color" content="#0095eb">
  
  
  
  
    
  
  
    
    
      
        <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">
      
    
  
  
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha512-6MXa8B6uaO18Hid6blRMetEIoPqHf7Ux1tnyIQdpt9qI5OACx7C+O3IVTr98vwGnlcg0LOLa02i9Y1HpVhlfiw==" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha512-SfTiTlX6kk+qitfevl/7LibUOeJWlt9rbyDn92a1DqWOw9vWG2MFoays0sgObmWazO5BQPiFucnnEAjpAB+/Sw==" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">
  
  
  
  
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Lato:400,700%7cMerriweather%7cRoboto&#43;Mono%7cPermanent&#43;Marker%7cPrata">
  
  <link rel="stylesheet" href="/styles.css">
  

  

  
  <link rel="alternate" href="/index.xml" type="application/rss+xml" title="Tong Shen">
  <link rel="feed" href="/index.xml" type="application/rss+xml" title="Tong Shen">
  

  <link rel="manifest" href="/site.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="/project/obfuscatedblurred-human-face-reconstruction/">

  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="og:site_name" content="Tong Shen">
  <meta property="og:url" content="/project/obfuscatedblurred-human-face-reconstruction/">
  <meta property="og:title" content="Obfuscated/Blurred Human Face Reconstruction | Tong Shen">
  <meta property="og:description" content="Table of Contents  Data preparation Super Resolution ResNet Generative Adversarial Network Model  Nowadays, a lot of images on the Internet are intentionally blurred or mosaiced due to various reasons. The main objective of this project is to reconstruct these images, especially heavily blurred ones, to their original high-resolution counterpart. This problem is an ill-posed problems, we need to predict fine-grained details only based on little information on degenerated images."><meta property="og:image" content="/img/face_header.png">
  <meta property="og:locale" content="en-us">
  
  <meta property="article:published_time" content="2017-11-27T00:00:00-05:00">
  
  <meta property="article:modified_time" content="2017-11-27T00:00:00-05:00">
  

  
  

  <title>Obfuscated/Blurred Human Face Reconstruction | Tong Shen</title>

</head>
<body id="top" data-spy="scroll" data-target="#toc" data-offset="71" >

<nav class="navbar navbar-default navbar-fixed-top" id="navbar-main">
  <div class="container">

    
    <div class="navbar-header">
      
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse"
              data-target=".navbar-collapse" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      
      <a class="navbar-brand" href="/">Tong Shen</a>
    </div>

    
    <div class="collapse navbar-collapse">

      
      
      <ul class="nav navbar-nav navbar-right">
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#about">
            
            <span>Home</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#myself">
            
            <span>CV</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#projects">
            
            <span>Projects</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#posts">
            
            <span>Posts</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#publications_selected">
            
            <span>Publications</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#contact">
            
            <span>Contact</span>
            
          </a>
        </li>

        
        
      

      
      </ul>

    </div>
  </div>
</nav>


<article class="article article-project" itemscope itemtype="http://schema.org/Article">

  
<div class="article-header">
  
  
    <img src="/img/face_header.png" class="article-banner" itemprop="image">
  

  <span class="article-header-caption">ðŸ˜„</span>
</div>



  <div class="article-container">

    <div class="pub-title">
      <h1 itemprop="name">Obfuscated/Blurred Human Face Reconstruction</h1>
      <span class="pub-authors" itemprop="author">&nbsp;</span>
      <span class="pull-right">
        
<div class="share-box" aria-hidden="true">
  <ul class="share">
    <li>
      <a class="twitter"
         href="https://twitter.com/intent/tweet?text=Obfuscated%2fBlurred%20Human%20Face%20Reconstruction&amp;url=%2fproject%2fobfuscatedblurred-human-face-reconstruction%2f"
         target="_blank" rel="noopener">
        <i class="fa fa-twitter"></i>
      </a>
    </li>
    <li>
      <a class="facebook"
         href="https://www.facebook.com/sharer.php?u=%2fproject%2fobfuscatedblurred-human-face-reconstruction%2f"
         target="_blank" rel="noopener">
        <i class="fa fa-facebook"></i>
      </a>
    </li>
    <li>
      <a class="linkedin"
         href="https://www.linkedin.com/shareArticle?mini=true&amp;url=%2fproject%2fobfuscatedblurred-human-face-reconstruction%2f&amp;title=Obfuscated%2fBlurred%20Human%20Face%20Reconstruction"
         target="_blank" rel="noopener">
        <i class="fa fa-linkedin"></i>
      </a>
    </li>
    <li>
      <a class="weibo"
         href="http://service.weibo.com/share/share.php?url=%2fproject%2fobfuscatedblurred-human-face-reconstruction%2f&amp;title=Obfuscated%2fBlurred%20Human%20Face%20Reconstruction"
         target="_blank" rel="noopener">
        <i class="fa fa-weibo"></i>
      </a>
    </li>
    <li>
      <a class="email"
         href="mailto:?subject=Obfuscated%2fBlurred%20Human%20Face%20Reconstruction&amp;body=%2fproject%2fobfuscatedblurred-human-face-reconstruction%2f">
        <i class="fa fa-envelope"></i>
      </a>
    </li>
  </ul>
</div>


      </span>
    </div>

    

    <div class="article-style" itemprop="articleBody">
      

<h1 id="table-of-contents">Table of Contents</h1>

<ol>
<li><a href="#data-preparation">Data preparation</a></li>
<li><a href="#super-resolution-resnet">Super Resolution ResNet</a></li>
<li><a href="#generative-adversarial-network-model">Generative Adversarial Network Model</a></li>
</ol>

<p>Nowadays, a lot of images on the Internet are intentionally blurred or mosaiced due to various reasons. The main objective of this project is to
reconstruct these images, especially heavily blurred ones, to their original high-resolution counterpart. This problem is an ill-posed problems, we need to
predict fine-grained details only based on little information on degenerated images. Exploring and enforcing strong prior information
about the high-resolution image are necessary to guarantee the stability of
this reconstruction process. Many traditional example-based methods have been devoted to resolving this problem via probabilistic
graphical model, neighbor embedding, sparse coding, linear or nonlinear regression, and random forest.<br />
Our approach is utilizing deep networks for image reconstruction to learn the mapping between low and high-resolution image pairs, automatically take the prior
information into account. Check out the code <a href="https://github.com/shen338/Obfuscated-Face-Reconstruction" target="_blank">here</a>.</p>

<p>Our experiment consists of three steps.</p>

<h2 id="data-preparation">Data preparation</h2>

<p>In 2017, the most popular dataset about human face is the <a href="http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html" target="_blank">CelebA dataset</a> from CUHK, which has
 10,177 number of identities, 202,599 number of face images.<br />
 What we do first is detecting and cropping human face from the image and resize it into 128x128 in convenience. We use OpenCV&rsquo;s module named &ldquo;haarcascade_frontalface&rdquo;
 to detect faces and use bicubic interpolation to resize images. The code are as below:</p>

<pre><code class="language-python">facedata = &quot;haarcascade_frontalface_default.xml&quot;
cascade = cv2.CascadeClassifier(facedata)
img = cv2.imread(image_directory)
minisize = (img.shape[1], img.shape[0])
miniframe = cv2.resize(img, minisize)

faces = cascade.detectMultiScale(miniframe)
if(len(faces) == 0): continue
x, y, w, h = [v for v in faces[0]]  # only need the first detected face image

img_raw = img[y:y + h, x:x + w]
img_raw = cv2.resize(img_raw, (128, 128), interpolation=cv2.INTER_LANCZOS4)
</code></pre>

<p>Then we manually downsample these high-resolution images using gaussian and average pooling. After write both low and high-resolution image pairs into binary files
using TFRecord for multi-threading read in the future. Code:</p>

<pre><code class="language-python">import tensorflow as tf
import numpy as np
import cv2
import glob
import random
import sys


def load_image(addr):
    # read an image and resize to (224, 224)
    # cv2 load images as BGR, convert it to RGB
    img = cv2.imread(addr)
    #img = cv2.resize(img, (128, 128), interpolation=cv2.INTER_CUBIC)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    img = img.astype(np.uint8)
    return img

def _int64_feature(value):
  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))

def _bytes_feature(value):
  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))


shuffle_data = True
image_path_small = './origin/small/*.png'
address_small = glob.glob(image_path_small)
print(len(address_small))
image_path_origin = './origin/origin/*.png'
address_origin = glob.glob(image_path_origin)

if shuffle_data:
    c = list(zip(address_small, address_origin))
    random.shuffle(c)
    address_small,  address_origin= zip(*c)

train_filename = 'train_espcn.tfrecords'

# create new TFrecord file
writer = tf.python_io.TFRecordWriter(train_filename)

for i in range(len(address_small)):

    if not i % 1000:
        print('Train data: {}/{}'.format(i, len(address_small)))
        sys.stdout.flush()

    img_small = load_image(address_small[i])
    img_origin = load_image(address_origin[i])

    feature = {'train/image_small': _bytes_feature(tf.compat.as_bytes(img_small.tostring())),
               'train/image_origin': _bytes_feature(tf.compat.as_bytes(img_origin.tostring()))}

    # Create an example protocol buffer
    example = tf.train.Example(features=tf.train.Features(feature=feature))

    # Serialize to string and write on the file
    writer.write(example.SerializeToString())

writer.close()
</code></pre>

<h2 id="super-resolution-resnet">Super Resolution ResNet</h2>

<p>My first trail was using convolution neural network to do end-to-end super-resolution. The intuition is quite simple,
just feed the low resolution images to the network, do downsampling and upsampling, in the end, use
high resolution images as ground truth to train this network. In this way, the neural network will learn how to map from
low-resolution image directly to high-resolution images. One thing worth to mention, In that time, I didn&rsquo;t have a good GPU,
so I have to so every I can to save computational resources.
We replaced the upsampling layers with Pixel Shuffle layers, which directly map the feature map into
high resolution output instead of doing transposed convolution to upsample the feature map.</p>

<p>Next, we use skip connection from <a href="https://arxiv.org/abs/1512.03385" target="_blank">ResNet paper</a> to enhance our model&rsquo;s  expression capacity. We put 15 residual
block module in our network before Pixel Shuffle layers. The enhancing is quite obvious in the result comparasion.
We also use pretrained VGG net to build the preceptual loss. Instead of using
vanilla MSE loss to compare the network output and ground truth, we feed the network output and ground truth
into ImageNet pretrained VGG net and compare their feature map difference using MSE. Using direct MSE loss function also works here, but it may
not agree with human observers, because human visual system is not sentitive to color and edgee. While perceptual loss simulates human visual system, making the
loss value more related to human percetion. The architecture of our network is shown as follows:
<img src="https://raw.githubusercontent.com/shen338/Obfuscated-Face-Reconstruction/master/SRResNet_model.PNG" alt="SRResnet" /></p>

<p>This model works well for 4 times mosaiced/blurred images, even though there is still some smoothing effect in human face detailes. I think our model even makes
these celebrities more beautiful/handsome ðŸ˜„. After smoothing out their wrinkles and little flaws, they all look younger than before! The result is shown as follows:</p>

<p><img src="https://raw.githubusercontent.com/shen338/Obfuscated-Face-Reconstruction/master/result/SRResNet_result.PNG" alt="SRResnet_result" /></p>

<h2 id="generative-adversarial-network-model">Generative Adversarial Network Model</h2>

<p>Although our ResNet model works well in 4 times mosaiced/blurred images, it terribly failed in more than 8 times mosaiced/blurred images. Maybe there is not enough
prior information for our network to reconstruct high-resolution images. To deal with this problem, we incorporate GAN model into our project, and expect GANs can
generate the fine details to the reconstruction process and improve our result. We brought this idea from <a href="https://arxiv.org/abs/1609.04802" target="_blank">SRGAN paper</a>.</p>

<p>Different from original GAN model to generate unique images based on image dataset, instead of feeding random noise to generator, we feed low-resolution
image to the generator and expect it to produce high-resolution images. And the discriminator is still responsible to determine whether the image is from
generator or from dataset. And the training loss also consist of two parts, the super-resolution(SR) loss, and the GAN loss. The SR loss is the perceptual loss (mentioned above)
between ground truth high-resolution image and the generator output, while GAN loss is the loss function for the generator and discriminator combined.</p>

<p>First, we tried vanilla deep convolution GAN(DCGAN) on our GAN model. After a few days of hyperparamete tunning, we find this model is not that stable.
The whole model can easily crash due to a small shift of a single hyperparameter. But there are better alternatives for DCGAN: the <a href="https://arxiv.org/abs/1701.07875" target="_blank">Wasserstein GAN</a>.
Although still have imperfections, WGAN almost totally solve issues like training instability, failure to converge or model collapse.</p>

<p>Here is the algorithm of WGAN:
<img src="/img/WGAN.jpg" alt="WGAN" /></p>

<p>Just minor modification from original GAN algorithm, how amazing is that! In Wasserstein GAN paper, the author thoroughly analyzed the weakness and holes on
original GANs. For details of the mathematics and implementation about WGAN, see my <a href="https://shen338.github.io/post/amazing-gan---wasserstein-gan/" target="_blank">another blog</a>. Here I will only list the modification from original GAN to WGAN:</p>

<ol>
<li>Remove the sigmoid function at the end of Discriminator<br /></li>
<li>Remove the log function in generator and discriminator loss function<br /></li>
<li>clip the gradient norm into an interval $[-c, c]$<br /></li>
<li>Use optimizers without momentum term, like RMSprop, not Adam.</li>
</ol>

<p>The core code are as follows:</p>

<pre><code class="language-python">def train():

    image_lr = tf.placeholder(dtype=tf.float32, shape=(None, 16, 16, 3), name='lr')
    image_hr = tf.placeholder(dtype=tf.float32, shape=(None, 128, 128, 3), name='hr')

    net = WGAN(gamma)

    gen = net.generator(image_lr, bottleneck_num=2)

    real_score = net.discrimintor(gen)
    fake_score = net.discrimintor(image_hr, reuse=True)

    with tf.name_scope('SR_loss'):

        residual = image_hr - gen
        square = tf.abs(residual)
        SR_loss = tf.reduce_mean(square)

        tf.summary.scalar('SR_loss', SR_loss)

    with tf.name_scope('gan_loss'):

        D_loss = tf.reduce_mean(fake_score) - tf.reduce_mean(real_score)

        G_loss = -tf.reduce_mean(fake_score)

        tf.summary.scalar('G_loss', G_loss)
        tf.summary.scalar('D_loss', D_loss)

        G_overall_loss = gan_ratio*G_loss + SR_loss 

    # get variable from G and D
    var_g = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'generator')
    var_d = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'discriminator')

    with tf.name_scope('optim'):

        optim_g = tf.train.RMSPropOptimizer(learning_rate=LEARNING_RATE)\
            .minimize(G_overall_loss, var_list=var_g)
        optim_d = tf.train.RMSPropOptimizer(learning_rate=LEARNING_RATE) \
            .minimize(-D_loss, var_list=var_d)

    clip_D = [p.assign(tf.clip_by_value(p, -0.01, 0.01)) for p in var_d]

    # set up logging for tensorboard
    writer = tf.summary.FileWriter(filewriter_path)
    writer.add_graph(tf.get_default_graph())
    summaries = tf.summary.merge_all()
	
	config = tf.ConfigProto()
    config.gpu_options.allow_growth = True
    with tf.Session() as sess:

        init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())
        sess.run(init_op)

        steps, start_average, end_average = 0, 0, 0
        start_time = time.clock()

        coord = tf.train.Coordinator()
        threads = tf.train.start_queue_runners(coord=coord)

        for ii in range(NUM_EPOCHS):

            batch_average = 0
            batch_num = int(np.floor(192794 / BATCH_SIZE / 6.0))

            for jj in range(batch_num):

                g_ops = [optim_g, G_loss, summaries]
                d_ops = [optim_d, D_loss]

                for kk in range(critic):

                    steps += 1
                    img_lr, img_hr = load_batch_date()
                    img_lr = (img_lr.astype(np.float32) - 127.5) / 127.5
                    img_hr = (img_hr.astype(np.float32) - 127.5) / 127.5

                    _, loss_d = sess.run(d_ops, feed_dict=
                                  {image_lr: img_lr, image_hr: img_hr})

                    sess.run(clip_D)

                steps += 1
                img_lr, img_hr = sess.run([images, labels])
                img_lr = (img_lr.astype(np.float32) - 127.5) / 127.5
                img_hr = (img_hr.astype(np.float32) - 127.5) / 127.5

                _, loss_g, summary = sess.run(g_ops,
                                feed_dict={image_lr: img_lr, image_hr: img_hr})

                # update W_loss and Kt

                writer.add_summary(summary, steps)
                batch_average += loss_g

                if (steps % 100 == 0):
                    print('step: {:d}, G_loss: {:.9f}, D_loss: {:.9f}'.format(steps, loss_g, loss_d))
                    print('time:', time.clock())

            batch_average = float(batch_average) / batch_num

            duration = time.time() - start_time
            print('Epoch: {}, step: {:d}, loss: {:.9f}, '
                  '({:.3f} sec/epoch)'.format(ii, steps, batch_average, duration))

            start_time = time.time()
            net.save(sess, saver, checkpoint_path, steps)
        coord.request_stop()

        # Wait for threads to stop
        coord.join(threads)
        sess.close()
</code></pre>

<p>Also, we implement WGAN-GP method, instead of directly clipping gradient, it put penalty on gradient&rsquo;s L2 norm. This method is even better
compared to original Wasserstein GAN.</p>

<p>And the result is pretty good compared to SRResNet model:
<img src="https://raw.githubusercontent.com/shen338/Obfuscated-Face-Reconstruction/master/result/SRGAN_result.PNG" alt="SRGAN_result" /><br />
The smoothing effect is quite significant in this case. But the SRGAN result is totally acceptable considering our model is reconstruct the
high-resolution image only using <sup>1</sup>&frasl;<sub>64</sub> of original information. I believe our result should be better with a faster GPU and deep network. I finished this
project on a Nivida Quadro K1000 GPU, which takes 6 hours to run one epoch.</p>

<p>There are always some failure case using GAN model:
<img src="https://raw.githubusercontent.com/shen338/Obfuscated-Face-Reconstruction/master/result/failure_case.PNG" alt="failure case" />
Some of them are funny and some of them are just scary&hellip;</p>

<p>Reference Materials:</p>

<ol>
<li><a href="http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html" target="_blank">CelebA Dataset from CUHK</a></li>
<li><a href="https://arxiv.org/pdf/1501.00092.pdf" target="_blank">Image Super-Resolution Using Deep Convolutional Networks</a></li>
<li><a href="https://arxiv.org/abs/1701.07875" target="_blank">Wasserstein GAN</a></li>
<li><a href="https://lilianweng.github.io/lil-log/2017/08/20/from-GAN-to-WGAN.html" target="_blank">A great blog on GAN and WGAN</a></li>
</ol>

    </div>

    


<div class="article-tags">
  
  <a class="btn btn-primary btn-outline" href="/tags/deep-learning/">deep-learning</a>
  
  <a class="btn btn-primary btn-outline" href="/tags/gan/">GAN</a>
  
</div>




    
    
    

    
      
      
      
      

      
      
      
      
    

  </div>
</article>



<footer class="site-footer">
  <div class="container">
    <p class="powered-by">

      &copy; 2018 &middot; 

      Powered by
      
      <a href="https://shen338.github.io/" target="_blank" rel="noopener">Tong Shen</a>. 
	  
	  All rights reserved.

      <span class="pull-right" aria-hidden="true">
        <a href="#" id="back_to_top">
          <span class="button_icon">
            <i class="fa fa-chevron-up fa-2x"></i>
          </span>
        </a>
      </span>

    </p>
  </div>
</footer>


<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <button type="button" class="close btn-large" data-dismiss="modal">&times;</button>
        <h4 class="modal-title">Cite</h4>
      </div>
      <div>
        <pre><code class="modal-body tex"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-primary btn-outline js-copy-cite" href="#" target="_blank">
          <i class="fa fa-copy"></i> Copy
        </a>
        <a class="btn btn-primary btn-outline js-download-cite" href="#" target="_blank">
          <i class="fa fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

    

    
    

    

    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.2.1/jquery.min.js" integrity="sha512-3P8rXCuGJdNZOnUx/03c1jOTnMn3rP63nBip5gOP2qmUh5YAdVAvFZ1E+QLZZbC1rtMrQb+mah3AfYW11RUrWA==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.3/imagesloaded.pkgd.min.js" integrity="sha512-umsR78NN0D23AzgoZ11K7raBD+R6hqKojyBZs1w8WvYlsI+QuKRGBx3LFCwhatzBunCjDuJpDHwxD13sLMbpRA==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha512-iztkobsvnjKfAtTNdHkGVjAYTrrtlC7mGp/54c40wowO7LhURYl3gVzzcEqGl/qKXQltJ2HwMrdLcNUdo+N/RQ==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.4/isotope.pkgd.min.js" integrity="sha512-VDBOIlDbuC4VWxGJNmuFRQ0Li0SKkDpmGyuhAG5LTDLd/dJ/S0WMVxriR2Y+CyPL5gzjpN4f/6iqWVBJlht0tQ==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>
    
    
    <script src="/js/hugo-academic.js"></script>
    

    
    
      
      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js" integrity="sha256-/BfiIkHlHoVihZdc6TFuj7MmJ0TWcWsMXkeDFwhi0zw=" crossorigin="anonymous"></script>
      

      

      

      <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']] } });
    </script>
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML" integrity="sha512-tOav5w1OjvsSJzePRtt2uQPFwBoHt1VZcUq8l8nm5284LEKE9FSJBQryzMBzHxY5P0zRdNqEcpLIRVYFNgu1jw==" crossorigin="anonymous"></script>
    
    

  </body>
</html>

