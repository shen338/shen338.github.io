<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Tong Shen on Tong Shen</title>
    <link>/</link>
    <description>Recent content in Tong Shen on Tong Shen</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Sun, 15 Oct 2017 00:00:00 -0400</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Spatial Transform Network</title>
      <link>/post/spatial-transform-network/</link>
      <pubDate>Sat, 19 May 2018 21:00:00 -0400</pubDate>
      
      <guid>/post/spatial-transform-network/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Amazing GAN - Wasserstein GAN</title>
      <link>/post/amazing-gan---wasserstein-gan/</link>
      <pubDate>Thu, 17 May 2018 21:00:00 -0400</pubDate>
      
      <guid>/post/amazing-gan---wasserstein-gan/</guid>
      <description>

&lt;h1 id=&#34;table-of-contents&#34;&gt;Table of Contents&lt;/h1&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;#example&#34;&gt;Example&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#example2&#34;&gt;Example2&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#third-example&#34;&gt;Third Example&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;example&#34;&gt;Example&lt;/h2&gt;

&lt;p&gt;.
.
.
.
.
.
.&lt;/p&gt;

&lt;h2 id=&#34;example2&#34;&gt;Example2&lt;/h2&gt;

&lt;p&gt;.
.&lt;/p&gt;

&lt;p&gt;.
..
.
..
.
.
..
.
..
.
.
..
..&lt;/p&gt;

&lt;h2 id=&#34;third-example&#34;&gt;Third Example&lt;/h2&gt;
</description>
    </item>
    
    <item>
      <title>Going Deeper in Batch Normalization</title>
      <link>/post/going-deeper-into-batch-normalization/</link>
      <pubDate>Thu, 17 May 2018 21:00:00 -0400</pubDate>
      
      <guid>/post/going-deeper-into-batch-normalization/</guid>
      <description>

&lt;p&gt;This article will thoroughly explain batch normalization in a simple way.
I wrote this article after getting failed an interview because of detailed batchnorm related question.
I will start with why we need it, how it works, then how to fuse it into conv layer, and finally how to implement it in tensorflow.&lt;/p&gt;

&lt;p&gt;Here is the original paper about batch normalization on Arxiv:&lt;br /&gt;
&lt;a href=&#34;https://arxiv.org/abs/1502.03167&#34; target=&#34;_blank&#34;&gt;Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;interpretation-and-advantage-of-batch-norm&#34;&gt;Interpretation and Advantage of Batch Norm&lt;/h3&gt;

&lt;p&gt;Of course, batch norm is used to normalize the input for certain layer. We can think it in this way: if some of our input image have a scale between 0-1 while others are
between 1-1000. It is better to normalize them before training. We can apply the same idea to the input of every layer input.
There are several advantages to use batch norm:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;span class=&#34;markup-quote&#34;&gt;Batch norm can reduce the covariance shift&lt;/span&gt;. For example, we train a model to classify cat and flowers.
And the training data of cat are all black cats. In this way, the model won&amp;rsquo;t work because it can only
classify the distribution of black cat and flowers. What batch norm does is to reduce this kind of error and make the
input shift, like reduce the difference between black cat and other cats. And the same thing also applies to
every layer in the neural network. Batch norm can reduce the shift around of previous output and make
the training of next layers easier.&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;markup-quote&#34;&gt;Batch norm can remove the linear interactions in output&lt;/span&gt;. In this way, linear layers would be useless, because they cano only have effect on
linear component.  In a deep neural network with nonlinearactivation functions, the lower layers can perform nonlinear transformations of the data, so they remain useful.
Batch normalization acts to standardize only the mean and variance of each unit in order to stabilize learning, but it allows therelationships
between units and the nonlinear statistics of a single unit to change.&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;markup-quote&#34;&gt;Batch normalization can greatly speed up training process&lt;/span&gt;. Batch normalization accelerates training by requiring less iterations to
converge to a given loss value. This can be done by using higher learning rates, but with smaller learning rates you can still see an improvement.&lt;br /&gt;
Batch normalization also makes the optimization problem &amp;ldquo;easier&amp;rdquo;, as minimizing the covariate shift avoid lots of plateaus where the loss stagnates
or decreases slowly. It can still happen but it is much less frequent.&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;markup-quote&#34;&gt;Batch norm also has some regularization effect&lt;/span&gt;. Every mini-batch is a biased sample from the total dataset.
When doing batch norm, we will subtract mean and divide it by variance. This can also be treated as add
noise to data. Similar to regularization techniques like dropout, network can gain some regularization
from this. But this effect is quite minor.&lt;br /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;algorithm-and-implementation&#34;&gt;Algorithm and implementation&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;Here is the algorithm diagram batch norm.
&lt;img src=&#34;/img/batch_norm_fp.png&#34; alt=&#34;Batch norm algorithm&#34; /&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Nothing fancy but extremely practical algorithm. One thing has to mention, &lt;span class=&#34;markup-quote&#34;&gt;the learnable variables
$\gamma$ and $\beta$. The deep learning book gives clear explaination about this&lt;/span&gt;. Normalizing the mean and deviation of a unit can
reduce the expressive power of a neural network. In this way, it is common to multiply the normalized result with $\gamma$ and add $\beta$. For exmaple,
if we have sigmoid activation afterwards, the network may don&amp;rsquo;t want the output lies in the near linear part of sigmoid. With $\gamma$ and $\beta$,
the network has the freedom to shift whatever it wants.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;This is a new parametrization can represent the same family of functions of the input as the old parametrization, but the new parametrization
 has different learning dynamics. In the old parametrization, the mean of H was determined by a complicated interaction between the parameters
 in the layers below H. In the new parametrization, the mean of $y=\gamma x + \beta$ is determined solely by $\gamma$. The new parametrization
 is much easier to learn with gradient descent.    &amp;ndash; Deep Learning Book&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;span class=&#34;markup-quote&#34;&gt;At test time, we need the mean and variance directly. So, the method is using an exponentially weighted average across mini-batches. &lt;/span&gt;
We have $ x_1, x_2, &amp;hellip; ,x_i $ outputs from different mini-batches. What we do is put expotential
weight on previous processed mini-batches. The calculation is quite simple:
$$Mean_{running} = \mu * Mean_{running} + (1.0 - \mu) * Mean_{sample}$$
$$Var_{running} = \mu * Var_{running} + (1.0 - \mu) * Var_{sample}$$
And we use running mean and var to calculate batchnorm.&lt;br /&gt;
Alternatively, we can first calculate the total mean and variance of total test dataset. But
this exponential weighted method are more popular in practice.&lt;/p&gt;

&lt;p&gt;And last but not least, the code for forward and backward pass(from my cs231n homework):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def batchnorm_forward(x, gamma, beta, bn_param):
    &amp;quot;&amp;quot;&amp;quot;
    Forward pass for batch normalization.
    During training the sample mean and (uncorrected) sample variance are
    computed from minibatch statistics and used to normalize the incoming data.
    During training we also keep an exponentially decaying running mean of the
    mean and variance of each feature, and these averages are used to normalize
    data at test-time.
    At each timestep we update the running averages for mean and variance using
    an exponential decay based on the momentum parameter:
    running_mean = momentum * running_mean + (1 - momentum) * sample_mean
    running_var = momentum * running_var + (1 - momentum) * sample_var
    Note that the batch normalization paper suggests a different test-time
    behavior: they compute sample mean and variance for each feature using a
    large number of training images rather than using a running average. For
    this implementation we have chosen to use running averages instead since
    they do not require an additional estimation step; the torch7
    implementation of batch normalization also uses running averages.
    Input:
    - x: Data of shape (N, D)
    - gamma: Scale parameter of shape (D,)
    - beta: Shift paremeter of shape (D,)
    - bn_param: Dictionary with the following keys:
      - mode: &#39;train&#39; or &#39;test&#39;; required
      - eps: Constant for numeric stability
      - momentum: Constant for running mean / variance.
      - running_mean: Array of shape (D,) giving running mean of features
      - running_var Array of shape (D,) giving running variance of features
    Returns a tuple of:
    - out: of shape (N, D)
    - cache: A tuple of values needed in the backward pass
    &amp;quot;&amp;quot;&amp;quot;
    mode = bn_param[&#39;mode&#39;]
    eps = bn_param.get(&#39;eps&#39;, 1e-5)
    momentum = bn_param.get(&#39;momentum&#39;, 0.9)

    N, D = x.shape
    running_mean = bn_param.get(&#39;running_mean&#39;, np.zeros(D, dtype=x.dtype))
    running_var = bn_param.get(&#39;running_var&#39;, np.zeros(D, dtype=x.dtype))

    out, cache = None, None
    if mode == &#39;train&#39;:
       
        sample_mean = np.mean(x, axis=0)
        sample_var = np.var(x, axis=0)
        x_stand = (x - sample_mean.T) / np.sqrt(sample_var.T + eps)

        out = x_stand * gamma + beta

        running_mean = momentum * running_mean + (1.0 - momentum) * sample_mean
        running_var = momentum * running_var + (1.0 - momentum) * sample_var

        cache = (sample_mean, sample_var, x_stand, x, gamma, beta, eps)

       
    elif mode == &#39;test&#39;:
        

        x_stand = (x - running_mean) / np.sqrt(running_var)
        out = x_stand * gamma + beta

        
    else:
        raise ValueError(&#39;Invalid forward batchnorm mode &amp;quot;%s&amp;quot;&#39; % mode)

    # Store the updated running means back into bn_param
    bn_param[&#39;running_mean&#39;] = running_mean
    bn_param[&#39;running_var&#39;] = running_var

    return out, cache

&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def batchnorm_backward(dout, cache):
    &amp;quot;&amp;quot;&amp;quot;
    Backward pass for batch normalization.
    For this implementation, you should write out a computation graph for
    batch normalization on paper and propagate gradients backward through
    intermediate nodes.
    Inputs:
    - dout: Upstream derivatives, of shape (N, D)
    - cache: Variable of intermediates from batchnorm_forward.
    Returns a tuple of:
    - dx: Gradient with respect to inputs x, of shape (N, D)
    - dgamma: Gradient with respect to scale parameter gamma, of shape (D,)
    - dbeta: Gradient with respect to shift parameter beta, of shape (D,)
    &amp;quot;&amp;quot;&amp;quot;
    dx, dgamma, dbeta = None, None, None
    

    sample_mean, sample_var, x_stand, x, gamma, beta, eps = cache
    N, D = dout.shape

    dbeta = np.sum(dout, axis=0)
    dgamma = np.sum(x_stand * dout, axis=0)
    dx = (1. / N) * gamma * (sample_var + eps)**(-1. / 2.) * (
         N * dout - np.sum(dout, axis=0) - (x - sample_mean) * (
         sample_var + eps)**(-1.0) * np.sum(dout * (x - sample_mean), axis=0))


    return dx, dgamma, dbeta

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;During training, the moving_mean and moving_variance need to be updated.
By default the update ops are placed in tf.GraphKeys.UPDATE_OPS, so they need to be added as a dependency to the train_op.
So, the template is:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;batchnorm = tf.layers.batch_normalization(x, training=training)
update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)
with tf.control_dependencies(update_ops):
    train_op = optimizer.minimize(loss)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And also tensorflow official evaluate function (classification model zoo):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, first_clone_scope)
with tf.control_dependencies([update_op]):
      train_tensor = tf.identity(total_loss, name=&#39;train_op&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;improvements-and-alternatives&#34;&gt;Improvements and Alternatives&lt;/h3&gt;

&lt;h4 id=&#34;batch-norm-fused-with-convolution&#34;&gt;Batch norm fused with Convolution&lt;/h4&gt;

&lt;p&gt;This is the question makes me fail the interview. Actually, there is no magic stuff about fused batch normalization. Just mathematically
calculate two layers together and treat them as one layer in forward and backward pass:&lt;/p&gt;

&lt;p&gt;Conv:
$$convout(N_i,C_{out})=bias(C_{out})+\sum_{k=1}^{C} weight(C_{out},k)*input(N_i,k)$$
Batch Norm:
$$bnout(N_i,C_{out})=(convout(N_i,C_{out})-\mu)/\sqrt{((\epsilon + \sigma)^2)}$$
$$bnout(N_i,C_{out})=\gamma*bnout(N_i,C_{out})+\beta$$
Here, $convout(N_i,C_{out})$ means the $N_ith$ sample in the $C_{out}$ channel. Same notation applies to input and bnout.
$weight(C_{out},k)$ is the conv kernel corresponding to $C_{out}$. And $\epsilon, \sigma, \mu, \gamma, \beta$ are the same as above.&lt;br /&gt;
After fusion, the total calculation becomes:&lt;/p&gt;

&lt;p&gt;$$out(N_i,C_{out})=\gamma*(bias(C_{out})+\sum_{k=1}^{C} weight(C_{out},k)*input(N_i,k)/\sqrt{((\epsilon + \sigma)^2)}+\beta$$
In this way, the weight and bias of fused conv layer is:&lt;/p&gt;

&lt;p&gt;$$bias = \gamma*(bias(C_{out})$$
$$weight = weight(C_{out},k)/\sqrt{((\epsilon + \sigma)^2)}+\beta$$&lt;/p&gt;

&lt;p&gt;We can use the fused bias and weight in previous conv layers. We can drop the intermediate result between conv and batch norm using this method,
which can save up to 50% memory and a minor increase of training time.&lt;/p&gt;

&lt;h4 id=&#34;layer-normalization&#34;&gt;Layer normalization&lt;/h4&gt;

&lt;p&gt;Just understand from its name, layer normalization. Instead of using a batch of data to produce $\mu and \sigma$ at every location,
It uses all the neuron activations in one layer to produce $\mu and \sigma$.
This method is especially useful when not using mini-batch like RNN, where batch norm cannot be used. But its performance in convs layers are not as good
as batch norm.&lt;/p&gt;

&lt;h4 id=&#34;instance-normalization&#34;&gt;Instance Normalization&lt;/h4&gt;

&lt;h4 id=&#34;group-normalization&#34;&gt;Group Normalization&lt;/h4&gt;

&lt;h4 id=&#34;other-normalization-techniques&#34;&gt;Other normalization techniques*&lt;/h4&gt;

&lt;p&gt;Recurrent Batch Normalization (BN) (Cooijmans, 2016; also proposed concurrently by Qianli Liao &amp;amp; Tomaso Poggio, but tested on Recurrent ConvNets,
instead of RNN/LSTM): Same as batch normalization. Use different normalization statistics for each time step. You need to store a set of mean and
standard deviation for each time step.&lt;/p&gt;

&lt;p&gt;Batch Normalized Recurrent Neural Networks (Laurent, 2015): batch normalization is only applied between the input and hidden state, but not between
hidden states. i.e., normalization is not applied over time.&lt;/p&gt;

&lt;p&gt;Streaming Normalization (Liao et al. 2016) : it summarizes existing normalizations and overcomes most issues mentioned above. It works well with
ConvNets, recurrent learning and online learning (i.e., small mini-batch or one sample at a time):&lt;/p&gt;

&lt;p&gt;Weight Normalization (Salimans and Kingma 2016): whenever a weight is used, it is divided by its L2 norm first, such that the resulting weight has
L2 norm 1. That is, output y=x*(w/|w|), where x and w denote the input and weight respectively. A scalar scaling factor g is then multiplied to the
output y=y*g. But in my experience g seems not essential for performance (also downstream learnable layers can learn this anyway).&lt;/p&gt;

&lt;p&gt;Cosine Normalization (Luo et al. 2017): weight normalization is very similar to cosine normalization, where the same L2 normalization is applied
to both weight and input: y=(x/|x|)*(w/|w|). Again, manual or automatic differentiation can compute appropriate gradients of x and w.&lt;/p&gt;

&lt;p&gt;Here are some great reference materials:&lt;br /&gt;
1. &lt;a href=&#34;http://www.deeplearningbook.org/contents/optimization.html&#34; target=&#34;_blank&#34;&gt;Deep Learning Book, Chapter 8.7.1&lt;/a&gt;&lt;br /&gt;
2. &lt;a href=&#34;https://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow&#34; target=&#34;_blank&#34;&gt;Stackoverflow: How could I use Batch Normalization in TensorFlow?&lt;/a&gt;&lt;br /&gt;
3. &lt;a href=&#34;http://sifaka.cs.uiuc.edu/jiang4/domain_adaptation/survey/node8.html&#34; target=&#34;_blank&#34;&gt;Explaination on Covariance Shift&lt;/a&gt;&lt;br /&gt;
4. &lt;a href=&#34;https://www.tensorflow.org/api_docs/python/tf/nn/batch_normalization&#34; target=&#34;_blank&#34;&gt;Tensorflow batch normalization docs&lt;/a&gt;&lt;br /&gt;
5. &lt;a href=&#34;https://datascience.stackexchange.com/questions/12956/paper-whats-the-difference-between-layer-normalization-recurrent-batch-normal&#34; target=&#34;_blank&#34;&gt;Various Normalization Techniques in Deep Learning&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Microscale two-dimensional (2D) temperature mapping by ratiometric fluorescence imaging under orthogonal excitations</title>
      <link>/publication/temperature_mapping/</link>
      <pubDate>Thu, 15 Feb 2018 00:00:00 -0500</pubDate>
      
      <guid>/publication/temperature_mapping/</guid>
      <description></description>
    </item>
    
    <item>
      <title> Fine-Grained Visual Categorization on iNaturalist dataset</title>
      <link>/project/fine-grained-visual-categorization-inaturalist/</link>
      <pubDate>Mon, 27 Nov 2017 00:00:00 -0500</pubDate>
      
      <guid>/project/fine-grained-visual-categorization-inaturalist/</guid>
      <description>

&lt;h2 id=&#34;table-of-content&#34;&gt;Table of Content&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#overview&#34;&gt;Overview&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#main-difficulties&#34;&gt;Main Difficulties&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#data-imbalance&#34;&gt;Data Imbalance&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#weakly-supervised-label&#34;&gt;Weakly supervised label&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#data-insufficiency&#34;&gt;Data insufficiency&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#fine-tuned-model-on-imagenet-pretrained-model&#34;&gt;Fine-tuned model on ImageNet pretrained model.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#fine-grained-classification&#34;&gt;Fine-grained Classification&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#super-category-classification&#34;&gt;Super-category classification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#fine-grained-classification-in-single-super-category&#34;&gt;Fine-grained classification in single super-category&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#bilinear-model&#34;&gt;Bilinear model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#attention-model-class-activation-map-&#34;&gt;Attention model(Class Activation Map)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#reference-materials&#34;&gt;Reference materials&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;

&lt;p&gt;I run into this challenge at early April. This dataset contains about 0.4 million images spanning over 8142 categories. Without expert knowledge, many of these species are extremely difficult to accurately classify due to their visual similarity. This classification task require us to features a large number of fine-grained
categories over class imbalance.&lt;/p&gt;

&lt;h2 id=&#34;main-difficulties&#34;&gt;Main Difficulties&lt;/h2&gt;

&lt;h3 id=&#34;data-imbalance&#34;&gt;Data Imbalance&lt;/h3&gt;

&lt;p&gt;The dataset is quite imbalance, some classes have more than 1000 images, while other classes only have less than 10 images. Here is a table of class image
number distribution.&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;image number range&lt;/th&gt;
&lt;th&gt;class number&lt;/th&gt;
&lt;th&gt;percentage&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;0&amp;lt;n&amp;lt;=10&lt;/td&gt;
&lt;td&gt;472&lt;/td&gt;
&lt;td&gt;5.78%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;10&amp;lt;n&amp;lt;=25&lt;/td&gt;
&lt;td&gt;4675&lt;/td&gt;
&lt;td&gt;57.4%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;25&amp;lt;n&amp;lt;=50&lt;/td&gt;
&lt;td&gt;1599&lt;/td&gt;
&lt;td&gt;19.6%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;50&amp;lt;n&amp;lt;=100&lt;/td&gt;
&lt;td&gt;554&lt;/td&gt;
&lt;td&gt;6.80%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;100&amp;lt;n&amp;lt;=200&lt;/td&gt;
&lt;td&gt;399&lt;/td&gt;
&lt;td&gt;4.90%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;200&amp;lt;n&amp;lt;=1000&lt;/td&gt;
&lt;td&gt;443&lt;/td&gt;
&lt;td&gt;5.44%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Over one half classes have image number in range $[10, 25]$, while the other classes are ranged in $[0, 1000]$. Some method or tricks to compensate classs imbalance is necessary here. Otherwise our model would easily get crashed. The majority class image number range is $[10, 25]$, while some other classes can easily get 1000, which can totally destory the training for the majority class.&lt;/p&gt;

&lt;p&gt;First, we just abandon the excessive images in class with more than 200 images. Second, oversample the minority class and put more aggressive image augmentation
to these classes. For example, affine transformation like random rotate, crop, shear, flap, blurring like gaussian blur, random dropout and so on. Image augmentation is implemented with Augmentor. Finally, we utilize weighted cross entropy loss function, further compensate the remaining
imbalance effect in our dataset. Here is the code for Multiprocessing Augmentor Pipeline:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
# Multiprocessing Augmentor Pipeline
def pipeline():

    P1 = Augmentor.Pipeline()
    P1.flip_left_right(probability=0.3)
    P1.flip_top_bottom(probability=0.05)
    P1.rotate90(probability=0.1)
    P1.rotate_without_crop(probability=0.1, max_left_rotation=15, max_right_rotation=15)
    P1.random_distortion(probability=0.1, grid_width=10, grid_height=10, magnitude=1)
    P1.skew_corner(probability=0.1)
    P1.shear(probability=0.1, max_shear_left=10, max_shear_right=10)
    P1.crop_centre(probability=1.0, percentage_area=0.9)
    P1.crop_random(probability=1.0, percentage_area=0.9)

    return P1


def aug_func(imgs, label, q):

    P = pipeline()
    imgs = np.uint8(imgs)
    g = P.keras_generator_from_array(imgs, label, batch_size=imgs.shape[0]
                                     , image_data_format=&#39;WTF&#39;)
    i, l = next(g)
    q.put([i, l])


def aug_parallel(imgs, labels):

    queue = Queue()
    cpus = multiprocessing.cpu_count()
    step = int(imgs.shape[0] / cpus)
    processes = []
    for ii in range(cpus):

        p = Process(target=aug_func,
                    args=(imgs[ii * step:(ii + 1) * step, :, :, :],
                          labels[ii * step:(ii + 1) * step],
                          queue))
        processes.append(p)
        p.start()

    rets = []
    labels = []

    for p in processes:
        ret, l = queue.get() 
        rets.append(ret)
        labels.append(l)
    for p in processes:
        p.join()

    if step == 1:
        rets = np.squeeze(np.array(rets))
        labels = np.squeeze(np.array(labels), axis=1)
    else:
        rets = np.concatenate(rets, axis=0)
        labels = np.concatenate(labels, axis=0)

    return rets, labels


&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;weakly-supervised-label&#34;&gt;Weakly supervised label&lt;/h3&gt;

&lt;p&gt;After looking into some image samples in the dataset, we find the the image labels are just weakly supervised labels. In other words, usually only
a small area in the image contains the target object. For example, one image is labeled as sunflowers, but the sunflower only appear at one corner,
and the background is grass and woods.&lt;/p&gt;

&lt;p&gt;This kind of labeling will apparently increase the difficulty of precise image classification because we have to do something to localize the object in the
image and prevent the background from affecting the classification result. That&amp;rsquo;s the main reason we incorporate attention model afterwards.&lt;/p&gt;

&lt;p&gt;Here are some cases of weakly supervised labels:&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Image&lt;/th&gt;
&lt;th&gt;Image&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;img src=&#34;/img/Weak1.jpg&#34; alt=&#34;&#34; /&gt;&lt;/td&gt;
&lt;td&gt;&lt;img src=&#34;/img/weak2.png&#34; alt=&#34;&#34; /&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&#34;data-insufficiency&#34;&gt;Data insufficiency&lt;/h3&gt;

&lt;p&gt;Although we have about 450,000 images in total, when averaged to every class, we only have 50 images left. And the majority class image number range is $[10, 25]$.
With so few images, it would be impossible to train a model from scratch. The most popular solution would be adopt a ImageNet pretrained model and fine-tuned on our dataset.&lt;/p&gt;

&lt;p&gt;Our first choice is the &lt;a href=&#34;https://arxiv.org/abs/1602.07261&#34; target=&#34;_blank&#34;&gt;Inception Resnet V2&lt;/a&gt; network from google. This network is well designed to save parameters and achieve
good performance at the same time. The same with previous Inception module, Inception resnet v2 also implements nx1 and 1xn convolution to replace nxn convolution, also bottleneck architecture to reduce computation. Besides, Inception Resnet V2 introduces residual connection from ResNet architecture to
eases the gradient vanishing problem when training deep neural nets. And the total layer number of the network goes to 467.&lt;/p&gt;

&lt;h2 id=&#34;fine-tuned-model-on-imagenet-pretrained-model&#34;&gt;Fine-tuned model on ImageNet pretrained model.&lt;/h2&gt;

&lt;p&gt;The first trail is directly classification over 8142 categories. Here is an architecture diagram of Inception Resnet V2:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/InceptionResnetv2.jpg&#34; alt=&#34;InceptionResnetv2.jpg&#34; /&gt;&lt;/p&gt;

&lt;p&gt;We first freeze the CNN feature extraction part, and train our own classifier - the final layers
of the network. Note that we have to training two classifier, one at the end of network, the auxiliary one at the middle. The middle one is used to provide additional
gradient to help the network to avoid gradient vanishing.
After 5 epoches of training, the accurarcy goes up to 40%.&lt;/p&gt;

&lt;p&gt;And after that, we use a relatively low learning rate to fine-tune the network to avoid too much modification on the original network. After about 15 epoches, the training accuracy goes up to about 80% and the validation accuracy is about 60%, the top3 accuracy on test set is about 80%. Apparently there are some kinds of overfit, but this is inevitable with so little data. We will further try to freeze the starting layers throughtout the traiing process to add some regulate the expression capacity of the network. Hope that works.&lt;/p&gt;

&lt;h2 id=&#34;fine-grained-classification&#34;&gt;Fine-grained Classification&lt;/h2&gt;

&lt;h3 id=&#34;super-category-classification&#34;&gt;Super-category classification&lt;/h3&gt;

&lt;p&gt;After finishing the previous model with InceptionResnetv2, we carefully examined our result. We found that our model often predicted three very similar classes, like three different sunflowers with very subtle difference. This means our model may have a hard time to classify images over similar classes. So, the next step is fine grained classification over these sub species. First, we divide our dataset into 11 super categories, like plants, insects, birds and so on. We consider this problem in a high dimensional space, and treat the image classes as clusters. The cluster distance of classes in the same super-category should be signaficantly smaller than cluster distance of classes in different super-categories. But the our image labelling cannot represent this property. After one-hot encoding, we treat every image class equivalently and assume the distance between them are the same.&lt;/p&gt;

&lt;p&gt;According to these analysis, we decided to classify images into their super-categories and do fine-grained classification in every single super-category. In this way, our model would concentrate more on the fine details of similar objects in the same super-category, without wasting resources on the easy cross super-category classification.&lt;/p&gt;

&lt;p&gt;Here is a diagram of image space:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/Figure_1.png&#34; alt=&#34;Diagram of image space&#34; /&gt;&lt;/p&gt;

&lt;p&gt;It would be more efficient to classify ABC and DE first. In this way, the class distance variance would be much less, we can use relatively less resource to achieve comparable performance to direct classification.&lt;/p&gt;

&lt;h3 id=&#34;fine-grained-classification-in-single-super-category&#34;&gt;Fine-grained classification in single super-category&lt;/h3&gt;

&lt;h4 id=&#34;bilinear-model&#34;&gt;Bilinear model&lt;/h4&gt;

&lt;p&gt;The first model is &lt;a href=&#34;http://vis-www.cs.umass.edu/bcnn/docs/bcnn_iccv15.pdf&#34; target=&#34;_blank&#34;&gt;bilinear model&lt;/a&gt;. This method considers the interaction between different feature map channels. If the feature map has the size $[H, W, C]$, treat every pixel in feature map as a feature vector, feature vectors has the size $[1,C]$. And the bilinear model takes the outer product of every feature vector and forms a $[H, W, C, C]$ feature map. After taking global pooling, we get a feature map [C, C]. In this process, bilinear model utilizes higher order information of the feature map. According to our test, its performance is pretty good. It achieves nearly 90% accuracy in single super-category classification. But when applying it to bigger super-category, our GPU memory is not enough. The reason is after bilinear operation, the feature map size becomes so big $(C^2)$, so the paremeters in following fully connected layers become overwhelming. For example, if the feature map has 512 channels, and our biggest super-category has more than 3000 classes, the parameters in FC layer is $512^2*3000 = 78 million$, which is more than all the CNN layers combined.&lt;/p&gt;

&lt;h4 id=&#34;attention-model-class-activation-map&#34;&gt;Attention model(Class Activation Map)&lt;/h4&gt;

&lt;p&gt;Due to the weakly supervised nature of the dataset (discussed &lt;a href=&#34;#weakly-supervised-label&#34;&gt;above&lt;/a&gt;), we want to first roughly localize the target object in the image before fine-grained classification.
Here attention model comes to the rescue. With attention model, we can determine which part of images matters for the network to make predictions, which should be the object we want to localize. Since we already have the network of super-category classification, its feature map can be utilized to generate class activation map &lt;a href=&#34;http://cnnlocalization.csail.mit.edu/Zhou_Learning_Deep_Features_CVPR_2016_paper.pdf&#34; target=&#34;_blank&#34;&gt;(CAM)&lt;/a&gt; to represent the attention level of input images.
Here is a diagram of CAM:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/CAM.png&#34; alt=&#34;CAM&#34; /&gt;&lt;/p&gt;

&lt;p&gt;As shown in the diagram, we can use the feature map and fc layer weight to generate the attention map for images with super-category network. Put an image into the network, we can get a feature map $[H, W, C]$ and a prediction. In order to map the global pooling layers
to predictions, the FC layer weight is $W \in R^{C \times N}$, where N is the output class number. If the prediction is $j$, CAM is calculated as follows:&lt;/p&gt;

&lt;p&gt;$$CAM = \sum_{k=0}^C F(k)*W_{k,j}$$&lt;/p&gt;

&lt;p&gt;where F(k) represent kth feature map channel. Alternatively, we can use top N prediction if top 1 prediction is not confident. Afterwards, we upsample CAM to fit input image size.&lt;/p&gt;

&lt;p&gt;After this, we used OSTU algorithm to make the CAM into binary image and extract the biggest contour and generate bounding box via OpenCV. Here are some results:&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Input Image&lt;/th&gt;
&lt;th&gt;CAM heatmap&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;img src=&#34;/img/Weak1.jpg&#34; alt=&#34;&#34; /&gt;&lt;/td&gt;
&lt;td&gt;&lt;img src=&#34;/img/weak2.png&#34; alt=&#34;&#34; /&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&#34;reference-materials&#34;&gt;Reference materials&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1602.07261&#34; target=&#34;_blank&#34;&gt;Inception ResNet V2 Paper&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://vis-www.cs.umass.edu/bcnn/docs/bcnn_iccv15.pdf&#34; target=&#34;_blank&#34;&gt;Bilinear Model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://cnnlocalization.csail.mit.edu/Zhou_Learning_Deep_Features_CVPR_2016_paper.pdf&#34; target=&#34;_blank&#34;&gt;Class Activation Map Paper&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Obfuscated/Blurred Human Face Reconstruction</title>
      <link>/project/obfuscatedblurred-human-face-reconstruction/</link>
      <pubDate>Mon, 27 Nov 2017 00:00:00 -0500</pubDate>
      
      <guid>/project/obfuscatedblurred-human-face-reconstruction/</guid>
      <description>

&lt;h1 id=&#34;table-of-contents&#34;&gt;Table of Contents&lt;/h1&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;#data-preparation&#34;&gt;Data preparation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#super-resolution-resnet&#34;&gt;Super Resolution ResNet&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#generative-adversarial-network-model&#34;&gt;Generative Adversarial Network Model&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Nowadays, a lot of images on the Internet are intentionally blurred or mosaiced due to various reasons. The main objective of this project is to
reconstruct these images, especially heavily blurred ones, to their original high-resolution counterpart. This problem is an ill-posed problems, we need to
predict fine-grained details only based on little information on degenerated images. Exploring and enforcing strong prior information
about the high-resolution image are necessary to guarantee the stability of
this reconstruction process. Many traditional example-based methods have been devoted to resolving this problem via probabilistic
graphical model, neighbor embedding, sparse coding, linear or nonlinear regression, and random forest.&lt;br /&gt;
Our approach is utilizing deep networks for image reconstruction to learn the mapping between low and high-resolution image pairs, automatically take the prior
information into account. Check out the code &lt;a href=&#34;https://github.com/shen338/Obfuscated-Face-Reconstruction&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Our experiment consists of three steps.&lt;/p&gt;

&lt;h2 id=&#34;data-preparation&#34;&gt;Data preparation&lt;/h2&gt;

&lt;p&gt;In 2017, the most popular dataset about human face is the &lt;a href=&#34;http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html&#34; target=&#34;_blank&#34;&gt;CelebA dataset&lt;/a&gt; from CUHK, which has
 10,177 number of identities, 202,599 number of face images.&lt;br /&gt;
 What we do first is detecting and cropping human face from the image and resize it into 128x128 in convenience. We use OpenCV&amp;rsquo;s module named &amp;ldquo;haarcascade_frontalface&amp;rdquo;
 to detect faces and use bicubic interpolation to resize images. The code are as below:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;facedata = &amp;quot;haarcascade_frontalface_default.xml&amp;quot;
cascade = cv2.CascadeClassifier(facedata)
img = cv2.imread(image_directory)
minisize = (img.shape[1], img.shape[0])
miniframe = cv2.resize(img, minisize)

faces = cascade.detectMultiScale(miniframe)
if(len(faces) == 0): continue
x, y, w, h = [v for v in faces[0]]  # only need the first detected face image

img_raw = img[y:y + h, x:x + w]
img_raw = cv2.resize(img_raw, (128, 128), interpolation=cv2.INTER_LANCZOS4)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then we manually downsample these high-resolution images using gaussian and average pooling. After write both low and high-resolution image pairs into binary files
using TFRecord for multi-threading read in the future. Code:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import tensorflow as tf
import numpy as np
import cv2
import glob
import random
import sys


def load_image(addr):
    # read an image and resize to (224, 224)
    # cv2 load images as BGR, convert it to RGB
    img = cv2.imread(addr)
    #img = cv2.resize(img, (128, 128), interpolation=cv2.INTER_CUBIC)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    img = img.astype(np.uint8)
    return img

def _int64_feature(value):
  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))

def _bytes_feature(value):
  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))


shuffle_data = True
image_path_small = &#39;./origin/small/*.png&#39;
address_small = glob.glob(image_path_small)
print(len(address_small))
image_path_origin = &#39;./origin/origin/*.png&#39;
address_origin = glob.glob(image_path_origin)

if shuffle_data:
    c = list(zip(address_small, address_origin))
    random.shuffle(c)
    address_small,  address_origin= zip(*c)

train_filename = &#39;train_espcn.tfrecords&#39;

# create new TFrecord file
writer = tf.python_io.TFRecordWriter(train_filename)

for i in range(len(address_small)):

    if not i % 1000:
        print(&#39;Train data: {}/{}&#39;.format(i, len(address_small)))
        sys.stdout.flush()

    img_small = load_image(address_small[i])
    img_origin = load_image(address_origin[i])

    feature = {&#39;train/image_small&#39;: _bytes_feature(tf.compat.as_bytes(img_small.tostring())),
               &#39;train/image_origin&#39;: _bytes_feature(tf.compat.as_bytes(img_origin.tostring()))}

    # Create an example protocol buffer
    example = tf.train.Example(features=tf.train.Features(feature=feature))

    # Serialize to string and write on the file
    writer.write(example.SerializeToString())

writer.close()
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;super-resolution-resnet&#34;&gt;Super Resolution ResNet&lt;/h2&gt;

&lt;p&gt;My first trail was using convolution neural network to do end-to-end super-resolution. The intuition is quite simple,
just feed the low resolution images to the network, do downsampling and upsampling, in the end, use
high resolution images as ground truth to train this network. In this way, the neural network will learn how to map from
low-resolution image directly to high-resolution images. One thing worth to mention, In that time, I didn&amp;rsquo;t have a good GPU,
so I have to so every I can to save computational resources.
We replaced the upsampling layers with Pixel Shuffle layers, which directly map the feature map into
high resolution output instead of doing transposed convolution to upsample the feature map.&lt;/p&gt;

&lt;p&gt;Next, we use skip connection from &lt;a href=&#34;https://arxiv.org/abs/1512.03385&#34; target=&#34;_blank&#34;&gt;ResNet paper&lt;/a&gt; to enhance our model&amp;rsquo;s  expression capacity. We put 15 residual
block module in our network before Pixel Shuffle layers. The enhancing is quite obvious in the result comparasion.
We also use pretrained VGG net to build the preceptual loss. Instead of using
vanilla MSE loss to compare the network output and ground truth, we feed the network output and ground truth
into ImageNet pretrained VGG net and compare their feature map difference using MSE. Using direct MSE loss function also works here, but it may
not agree with human observers, because human visual system is not sentitive to color and edgee. While perceptual loss simulates human visual system, making the
loss value more related to human percetion. The architecture of our network is shown as follows:
&lt;img src=&#34;https://raw.githubusercontent.com/shen338/Obfuscated-Face-Reconstruction/master/SRResNet_model.PNG&#34; alt=&#34;SRResnet&#34; /&gt;&lt;/p&gt;

&lt;p&gt;This model works well for 4 times mosaiced/blurred images, even though there is still some smoothing effect in human face detailes. I think our model even makes
these celebrities more beautiful/handsome 😄. After smoothing out their wrinkles and little flaws, they all look younger than before! The result is shown as follows:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/shen338/Obfuscated-Face-Reconstruction/master/result/SRResNet_result.PNG&#34; alt=&#34;SRResnet_result&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;generative-adversarial-network-model&#34;&gt;Generative Adversarial Network Model&lt;/h2&gt;

&lt;p&gt;Although our ResNet model works well in 4 times mosaiced/blurred images, it terribly failed in more than 8 times mosaiced/blurred images. Maybe there is not enough
prior information for our network to reconstruct high-resolution images. To deal with this problem, we incorporate GAN model into our project, and expect GANs can
generate the fine details to the reconstruction process and improve our result. We brought this idea from &lt;a href=&#34;https://arxiv.org/abs/1609.04802&#34; target=&#34;_blank&#34;&gt;SRGAN paper&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Different from original GAN model to generate unique images based on image dataset, instead of feeding random noise to generator, we feed low-resolution
image to the generator and expect it to produce high-resolution images. And the discriminator is still responsible to determine whether the image is from
generator or from dataset. And the training loss also consist of two parts, the super-resolution(SR) loss, and the GAN loss. The SR loss is the perceptual loss (mentioned above)
between ground truth high-resolution image and the generator output, while GAN loss is the loss function for the generator and discriminator combined.&lt;/p&gt;

&lt;p&gt;First, we tried vanilla deep convolution GAN(DCGAN) on our GAN model. After a few days of hyperparamete tunning, we find this model is not that stable.
The whole model can easily crash due to a small shift of a single hyperparameter. But there are better alternatives for DCGAN: the &lt;a href=&#34;https://arxiv.org/abs/1701.07875&#34; target=&#34;_blank&#34;&gt;Wasserstein GAN&lt;/a&gt;.
Although still have imperfections, WGAN almost totally solve issues like training instability, failure to converge or model collapse.&lt;/p&gt;

&lt;p&gt;Here is the algorithm of WGAN:
&lt;img src=&#34;/img/WGAN.jpg&#34; alt=&#34;WGAN&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Just minor modification from original GAN algorithm, how amazing is that! In Wasserstein GAN paper, the author thoroughly analyzed the weakness and holes on
original GANs. For details of the mathematics and implementation about WGAN, see my another blog. Here I will only list the modification from original GAN to WGAN:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Remove the sigmoid function at the end of Discriminator&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Remove the log function in generator and discriminator loss function&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;clip the gradient norm into an interval $[-c, c]$&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Use optimizers without momentum term, like RMSprop, not Adam.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The core code are as follows:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def train():

    image_lr = tf.placeholder(dtype=tf.float32, shape=(None, 16, 16, 3), name=&#39;lr&#39;)
    image_hr = tf.placeholder(dtype=tf.float32, shape=(None, 128, 128, 3), name=&#39;hr&#39;)

    net = WGAN(gamma)

    gen = net.generator(image_lr, bottleneck_num=2)

    real_score = net.discrimintor(gen)
    fake_score = net.discrimintor(image_hr, reuse=True)

    with tf.name_scope(&#39;SR_loss&#39;):

        residual = image_hr - gen
        square = tf.abs(residual)
        SR_loss = tf.reduce_mean(square)

        tf.summary.scalar(&#39;SR_loss&#39;, SR_loss)

    with tf.name_scope(&#39;gan_loss&#39;):

        D_loss = tf.reduce_mean(fake_score) - tf.reduce_mean(real_score)

        G_loss = -tf.reduce_mean(fake_score)

        tf.summary.scalar(&#39;G_loss&#39;, G_loss)
        tf.summary.scalar(&#39;D_loss&#39;, D_loss)

        G_overall_loss = gan_ratio*G_loss + SR_loss 

    # get variable from G and D
    var_g = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, &#39;generator&#39;)
    var_d = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, &#39;discriminator&#39;)

    with tf.name_scope(&#39;optim&#39;):

        optim_g = tf.train.RMSPropOptimizer(learning_rate=LEARNING_RATE)\
            .minimize(G_overall_loss, var_list=var_g)
        optim_d = tf.train.RMSPropOptimizer(learning_rate=LEARNING_RATE) \
            .minimize(-D_loss, var_list=var_d)

    clip_D = [p.assign(tf.clip_by_value(p, -0.01, 0.01)) for p in var_d]

    # set up logging for tensorboard
    writer = tf.summary.FileWriter(filewriter_path)
    writer.add_graph(tf.get_default_graph())
    summaries = tf.summary.merge_all()
	
	config = tf.ConfigProto()
    config.gpu_options.allow_growth = True
    with tf.Session() as sess:

        init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())
        sess.run(init_op)

        steps, start_average, end_average = 0, 0, 0
        start_time = time.clock()

        coord = tf.train.Coordinator()
        threads = tf.train.start_queue_runners(coord=coord)

        for ii in range(NUM_EPOCHS):

            batch_average = 0
            batch_num = int(np.floor(192794 / BATCH_SIZE / 6.0))

            for jj in range(batch_num):

                g_ops = [optim_g, G_loss, summaries]
                d_ops = [optim_d, D_loss]

                for kk in range(critic):

                    steps += 1
                    img_lr, img_hr = load_batch_date()
                    img_lr = (img_lr.astype(np.float32) - 127.5) / 127.5
                    img_hr = (img_hr.astype(np.float32) - 127.5) / 127.5

                    _, loss_d = sess.run(d_ops, feed_dict=
                                  {image_lr: img_lr, image_hr: img_hr})

                    sess.run(clip_D)

                steps += 1
                img_lr, img_hr = sess.run([images, labels])
                img_lr = (img_lr.astype(np.float32) - 127.5) / 127.5
                img_hr = (img_hr.astype(np.float32) - 127.5) / 127.5

                _, loss_g, summary = sess.run(g_ops,
                                feed_dict={image_lr: img_lr, image_hr: img_hr})

                # update W_loss and Kt

                writer.add_summary(summary, steps)
                batch_average += loss_g

                if (steps % 100 == 0):
                    print(&#39;step: {:d}, G_loss: {:.9f}, D_loss: {:.9f}&#39;.format(steps, loss_g, loss_d))
                    print(&#39;time:&#39;, time.clock())

            batch_average = float(batch_average) / batch_num

            duration = time.time() - start_time
            print(&#39;Epoch: {}, step: {:d}, loss: {:.9f}, &#39;
                  &#39;({:.3f} sec/epoch)&#39;.format(ii, steps, batch_average, duration))

            start_time = time.time()
            net.save(sess, saver, checkpoint_path, steps)
        coord.request_stop()

        # Wait for threads to stop
        coord.join(threads)
        sess.close()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Also, we implement WGAN-GP method, instead of directly clipping gradient, it put penalty on gradient&amp;rsquo;s L2 norm. This method is even better
compared to original Wasserstein GAN.&lt;/p&gt;

&lt;p&gt;And the result is pretty good compared to SRResNet model:
&lt;img src=&#34;https://raw.githubusercontent.com/shen338/Obfuscated-Face-Reconstruction/master/result/SRGAN_result.PNG&#34; alt=&#34;SRGAN_result&#34; /&gt;&lt;br /&gt;
The smoothing effect is quite significant in this case. But the SRGAN result is totally acceptable considering our model is reconstruct the
high-resolution image only using &lt;sup&gt;1&lt;/sup&gt;&amp;frasl;&lt;sub&gt;64&lt;/sub&gt; of original information. I believe our result should be better with a faster GPU and deep network. I finished this
project on a Nivida Quadro K1000 GPU, which takes 6 hours to run one epoch.&lt;/p&gt;

&lt;p&gt;There are always some failure case using GAN model:
&lt;img src=&#34;https://raw.githubusercontent.com/shen338/Obfuscated-Face-Reconstruction/master/result/failure_case.PNG&#34; alt=&#34;failure case&#34; /&gt;
Some of them are funny and some of them are just scary&amp;hellip;&lt;/p&gt;

&lt;p&gt;Reference Materials:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html&#34; target=&#34;_blank&#34;&gt;CelebA Dataset from CUHK&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1501.00092.pdf&#34; target=&#34;_blank&#34;&gt;Image Super-Resolution Using Deep Convolutional Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1701.07875&#34; target=&#34;_blank&#34;&gt;Wasserstein GAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://lilianweng.github.io/lil-log/2017/08/20/from-GAN-to-WGAN.html&#34; target=&#34;_blank&#34;&gt;A great blog on GAN and WGAN&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Subwavelength focusing by binary multi-annular plates: design theory and experiment</title>
      <link>/publication/map/</link>
      <pubDate>Sun, 15 Feb 2015 00:00:00 -0500</pubDate>
      
      <guid>/publication/map/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
