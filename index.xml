<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Tong Shen on Tong Shen</title>
    <link>/</link>
    <description>Recent content in Tong Shen on Tong Shen</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Sun, 15 Oct 2017 00:00:00 -0400</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Going Deeper in Batch Normalization</title>
      <link>/post/going-deeper-into-batch-normalization/</link>
      <pubDate>Thu, 17 May 2018 21:00:00 -0400</pubDate>
      
      <guid>/post/going-deeper-into-batch-normalization/</guid>
      <description>

&lt;h3 id=&#34;table-of-contents-optimization&#34;&gt;Table of contents/optimization&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#interpretation-and-advantage-of-batch-norm&#34;&gt;Interpretation and Advantage of Batch Norm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#algorithm-and-implementation&#34;&gt;Algorithm and implementation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#improvements-and-alternatives&#34;&gt;Improvements and Alternatives&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#batch-norm-fused-with-convolution&#34;&gt;Batch norm fused with Convolution&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#layer-normalization&#34;&gt;Layer normalization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#instance-normalization&#34;&gt;Instance Normalization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#group-normalization&#34;&gt;Group Normalization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#other-normalization-techniques-&#34;&gt;Other normalization techniques*&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#reference-materials-&#34;&gt;Reference Materials:&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This article will thoroughly explain batch normalization in a simple way.
I wrote this article after getting failed an interview because of detailed batchnorm related question.
I will start with why we need it, how it works, then how to fuse it into conv layer, and finally how to implement it in tensorflow.&lt;/p&gt;

&lt;p&gt;Here is the original paper about batch normalization on Arxiv:&lt;br /&gt;
&lt;a href=&#34;https://arxiv.org/abs/1502.03167&#34; target=&#34;_blank&#34;&gt;Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;interpretation-and-advantage-of-batch-norm&#34;&gt;Interpretation and Advantage of Batch Norm&lt;/h3&gt;

&lt;p&gt;Of course, batch norm is used to normalize the input for certain layer. We can think it in this way: if some of our input image have a scale between 0-1 while others are
between 1-1000. It is better to normalize them before training. We can apply the same idea to the input of every layer input.
There are several advantages to use batch norm:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;span class=&#34;markup-quote&#34;&gt;Batch norm can reduce the covariance shift&lt;/span&gt;. For example, we train a model to classify cat and flowers.
And the training data of cat are all black cats. In this way, the model won&amp;rsquo;t work because it can only
classify the distribution of black cat and flowers. What batch norm does is to reduce this kind of error and make the
input shift, like reduce the difference between black cat and other cats. And the same thing also applies to
every layer in the neural network. Batch norm can reduce the shift around of previous output and make
the training of next layers easier.&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;markup-quote&#34;&gt;Batch norm can remove the linear interactions in output&lt;/span&gt;. In this way, linear layers would be useless, because they cano only have effect on
linear component.  In a deep neural network with nonlinearactivation functions, the lower layers can perform nonlinear transformations of the data, so they remain useful.
Batch normalization acts to standardize only the mean and variance of each unit in order to stabilize learning, but it allows therelationships
between units and the nonlinear statistics of a single unit to change.&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;markup-quote&#34;&gt;Batch normalization can greatly speed up training process&lt;/span&gt;. Batch normalization accelerates training by requiring less iterations to
converge to a given loss value. This can be done by using higher learning rates, but with smaller learning rates you can still see an improvement.&lt;br /&gt;
Batch normalization also makes the optimization problem &amp;ldquo;easier&amp;rdquo;, as minimizing the covariate shift avoid lots of plateaus where the loss stagnates
or decreases slowly. It can still happen but it is much less frequent.&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;markup-quote&#34;&gt;Batch norm also has some regularization effect&lt;/span&gt;. Every mini-batch is a biased sample from the total dataset.
When doing batch norm, we will subtract mean and divide it by variance. This can also be treated as add
noise to data. Similar to regularization techniques like dropout, network can gain some regularization
from this. But this effect is quite minor.&lt;br /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;algorithm-and-implementation&#34;&gt;Algorithm and implementation&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;Here is the algorithm diagram batch norm.
&lt;img src=&#34;/img/batch_norm_fp.png&#34; alt=&#34;Batch norm algorithm&#34; /&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Nothing fancy but extremely practical algorithm. One thing has to mention, &lt;span class=&#34;markup-quote&#34;&gt;the learnable variables
$\gamma$ and $\beta$. The deep learning book gives clear explaination about this&lt;/span&gt;. Normalizing the mean and deviation of a unit can
reduce the expressive power of a neural network. In this way, it is common to multiply the normalized result with $\gamma$ and add $\beta$. For exmaple,
if we have sigmoid activation afterwards, the network may don&amp;rsquo;t want the output lies in the near linear part of sigmoid. With $\gamma$ and $\beta$,
the network has the freedom to shift whatever it wants. Another thing to note is that batch norm in CNN is different. Instead of calculate mean and variance in size $[H, W, C]$, it also averages over H and W, and the mean and variance has C dimensions, because CNN weight are shared cross H and W.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;This is a new parametrization can represent the same family of functions of the input as the old parametrization, but the new parametrization
 has different learning dynamics. In the old parametrization, the mean of H was determined by a complicated interaction between the parameters
 in the layers below H. In the new parametrization, the mean of $y=\gamma x + \beta$ is determined solely by $\gamma$. The new parametrization
 is much easier to learn with gradient descent.    &amp;ndash; Deep Learning Book&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;span class=&#34;markup-quote&#34;&gt;At test time, we need the mean and variance directly. So, the method is using an exponentially weighted average across mini-batches. &lt;/span&gt;
We have $ x_1, x_2, &amp;hellip; ,x_i $ outputs from different mini-batches. What we do is put expotential
weight on previous processed mini-batches. The calculation is quite simple:
$$Mean_{running} = \mu * Mean_{running} + (1.0 - \mu) * Mean_{sample}$$
$$Var_{running} = \mu * Var_{running} + (1.0 - \mu) * Var_{sample}$$
And we use running mean and var to calculate batchnorm.&lt;br /&gt;
Alternatively, we can first calculate the total mean and variance of total test dataset. But
this exponential weighted method are more popular in practice.&lt;/p&gt;

&lt;p&gt;And last but not least, the code for forward and backward pass(from my cs231n homework):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def batchnorm_forward(x, gamma, beta, bn_param):
    &amp;quot;&amp;quot;&amp;quot;
    Forward pass for batch normalization.
    During training the sample mean and (uncorrected) sample variance are
    computed from minibatch statistics and used to normalize the incoming data.
    During training we also keep an exponentially decaying running mean of the
    mean and variance of each feature, and these averages are used to normalize
    data at test-time.
    At each timestep we update the running averages for mean and variance using
    an exponential decay based on the momentum parameter:
    running_mean = momentum * running_mean + (1 - momentum) * sample_mean
    running_var = momentum * running_var + (1 - momentum) * sample_var
    Note that the batch normalization paper suggests a different test-time
    behavior: they compute sample mean and variance for each feature using a
    large number of training images rather than using a running average. For
    this implementation we have chosen to use running averages instead since
    they do not require an additional estimation step; the torch7
    implementation of batch normalization also uses running averages.
    Input:
    - x: Data of shape (N, D)
    - gamma: Scale parameter of shape (D,)
    - beta: Shift paremeter of shape (D,)
    - bn_param: Dictionary with the following keys:
      - mode: &#39;train&#39; or &#39;test&#39;; required
      - eps: Constant for numeric stability
      - momentum: Constant for running mean / variance.
      - running_mean: Array of shape (D,) giving running mean of features
      - running_var Array of shape (D,) giving running variance of features
    Returns a tuple of:
    - out: of shape (N, D)
    - cache: A tuple of values needed in the backward pass
    &amp;quot;&amp;quot;&amp;quot;
    mode = bn_param[&#39;mode&#39;]
    eps = bn_param.get(&#39;eps&#39;, 1e-5)
    momentum = bn_param.get(&#39;momentum&#39;, 0.9)

    N, D = x.shape
    running_mean = bn_param.get(&#39;running_mean&#39;, np.zeros(D, dtype=x.dtype))
    running_var = bn_param.get(&#39;running_var&#39;, np.zeros(D, dtype=x.dtype))

    out, cache = None, None
    if mode == &#39;train&#39;:
       
        sample_mean = np.mean(x, axis=0)
        sample_var = np.var(x, axis=0)
        x_stand = (x - sample_mean.T) / np.sqrt(sample_var.T + eps)

        out = x_stand * gamma + beta

        running_mean = momentum * running_mean + (1.0 - momentum) * sample_mean
        running_var = momentum * running_var + (1.0 - momentum) * sample_var

        cache = (sample_mean, sample_var, x_stand, x, gamma, beta, eps)

       
    elif mode == &#39;test&#39;:
        

        x_stand = (x - running_mean) / np.sqrt(running_var)
        out = x_stand * gamma + beta

        
    else:
        raise ValueError(&#39;Invalid forward batchnorm mode &amp;quot;%s&amp;quot;&#39; % mode)

    # Store the updated running means back into bn_param
    bn_param[&#39;running_mean&#39;] = running_mean
    bn_param[&#39;running_var&#39;] = running_var

    return out, cache

&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def batchnorm_backward(dout, cache):
    &amp;quot;&amp;quot;&amp;quot;
    Backward pass for batch normalization.
    For this implementation, you should write out a computation graph for
    batch normalization on paper and propagate gradients backward through
    intermediate nodes.
    Inputs:
    - dout: Upstream derivatives, of shape (N, D)
    - cache: Variable of intermediates from batchnorm_forward.
    Returns a tuple of:
    - dx: Gradient with respect to inputs x, of shape (N, D)
    - dgamma: Gradient with respect to scale parameter gamma, of shape (D,)
    - dbeta: Gradient with respect to shift parameter beta, of shape (D,)
    &amp;quot;&amp;quot;&amp;quot;
    dx, dgamma, dbeta = None, None, None
    

    sample_mean, sample_var, x_stand, x, gamma, beta, eps = cache
    N, D = dout.shape

    dbeta = np.sum(dout, axis=0)
    dgamma = np.sum(x_stand * dout, axis=0)
    dx = (1. / N) * gamma * (sample_var + eps)**(-1. / 2.) * (
         N * dout - np.sum(dout, axis=0) - (x - sample_mean) * (
         sample_var + eps)**(-1.0) * np.sum(dout * (x - sample_mean), axis=0))


    return dx, dgamma, dbeta

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;During training, the moving_mean and moving_variance need to be updated.
By default the update ops are placed in tf.GraphKeys.UPDATE_OPS, so they need to be added as a dependency to the train_op.
So, the template is:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;batchnorm = tf.layers.batch_normalization(x, training=training)
update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)
with tf.control_dependencies(update_ops):
    train_op = optimizer.minimize(loss)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And also tensorflow official evaluate function (classification model zoo):&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, first_clone_scope)
with tf.control_dependencies([update_op]):
      train_tensor = tf.identity(total_loss, name=&#39;train_op&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;improvements-and-alternatives&#34;&gt;Improvements and Alternatives&lt;/h3&gt;

&lt;h4 id=&#34;batch-norm-fused-with-convolution&#34;&gt;Batch norm fused with Convolution&lt;/h4&gt;

&lt;p&gt;This is the question makes me fail the interview. Actually, there is no magic stuff about fused batch normalization. Just mathematically
calculate two layers together and treat them as one layer in forward and backward pass:&lt;/p&gt;

&lt;p&gt;Conv:
$$convout(N_i,C_{out})=bias(C_{out})+\sum_{k=1}^{C} weight(C_{out},k)*input(N_i,k)$$
Batch Norm:
$$bnout(N_i,C_{out})=(convout(N_i,C_{out})-\mu)/\sqrt{((\epsilon + \sigma)^2)}$$
$$bnout(N_i,C_{out})=\gamma*bnout(N_i,C_{out})+\beta$$
Here, $convout(N_i,C_{out})$ means the $N_ith$ sample in the $C_{out}$ channel. Same notation applies to input and bnout.
$weight(C_{out},k)$ is the conv kernel corresponding to $C_{out}$. And $\epsilon, \sigma, \mu, \gamma, \beta$ are the same as above.&lt;br /&gt;
After fusion, the total calculation becomes:&lt;/p&gt;

&lt;p&gt;$$out(N_i,C_{out})=\gamma*(bias(C_{out})+\sum_{k=1}^{C} weight(C_{out},k)*input(N_i,k)/\sqrt{((\epsilon + \sigma)^2)}+\beta$$
In this way, the weight and bias of fused conv layer is:&lt;/p&gt;

&lt;p&gt;$$bias = \gamma*(bias(C_{out})$$
$$weight = weight(C_{out},k)/\sqrt{((\epsilon + \sigma)^2)}+\beta$$&lt;/p&gt;

&lt;p&gt;We can use the fused bias and weight in previous conv layers. We can drop the intermediate result between conv and batch norm using this method,
which can save up to 50% memory and a minor increase of training time.&lt;/p&gt;

&lt;h4 id=&#34;layer-normalization&#34;&gt;Layer normalization&lt;/h4&gt;

&lt;p&gt;Just understand from its name, layer normalization. Instead of using a batch of data to produce $\mu $ and $\sigma$ at every location,
It uses all the neuron activations in one layer to produce $\mu$ and $\sigma$.
This method is especially useful when not using mini-batch like RNN, where batch norm cannot be used. But its performance in convs layers are not as good
as batch norm.&lt;/p&gt;

&lt;h4 id=&#34;instance-normalization&#34;&gt;Instance Normalization&lt;/h4&gt;

&lt;p&gt;Just simply replace all batch normalization layers with instance normalization layers. Batch normalization normalizes using the information from the whole batch, while instance normalization normalizes each feature map on its own.&lt;br /&gt;
Formula comparasion:&lt;br /&gt;
&lt;img src=&#34;/img/instancenorm.jpg&#34; alt=&#34;instance norm&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;group-normalization&#34;&gt;Group Normalization&lt;/h4&gt;

&lt;p&gt;While batch-norm demonstrates it effectiveness in a variety of fields including computer vision, natural language processing, speech processing, robotics, etc., batch-norm&amp;rsquo;s performance substantially decrease when the training batch size become smaller, which limits the gain of utilizing batch-norm in a task requiring small batches constrained by memory consumption.&lt;/p&gt;

&lt;p&gt;Instead of normalizing along the batch dimension, GN divides the channels into groups and computes within each group the mean and variance. Therefore, GN&amp;rsquo;s computation is independent of batch sizes, and so does its accuracy.&lt;/p&gt;

&lt;p&gt;Here is a diagram for all these Group techniques:&lt;br /&gt;
&lt;img src=&#34;/img/GN.jpg&#34; alt=&#34;GN&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The difference is obvious. Batch norm uses data in the same batch to calculate $\mu $ and $\sigma$; Layer norm uses all the neurons in single layer; Instance norm uses all in data in the same batch and channel; Group norm uses only a small piece of batch.&lt;/p&gt;

&lt;h4 id=&#34;other-normalization-techniques&#34;&gt;Other normalization techniques*&lt;/h4&gt;

&lt;p&gt;Recurrent Batch Normalization (BN) (Cooijmans, 2016; also proposed concurrently by Qianli Liao &amp;amp; Tomaso Poggio, but tested on Recurrent ConvNets,
instead of RNN/LSTM): Same as batch normalization. Use different normalization statistics for each time step. You need to store a set of mean and
standard deviation for each time step.&lt;/p&gt;

&lt;p&gt;Batch Normalized Recurrent Neural Networks (Laurent, 2015): batch normalization is only applied between the input and hidden state, but not between
hidden states. i.e., normalization is not applied over time.&lt;/p&gt;

&lt;p&gt;Streaming Normalization (Liao et al. 2016) : it summarizes existing normalizations and overcomes most issues mentioned above. It works well with
ConvNets, recurrent learning and online learning (i.e., small mini-batch or one sample at a time):&lt;/p&gt;

&lt;p&gt;Weight Normalization (Salimans and Kingma 2016): whenever a weight is used, it is divided by its L2 norm first, such that the resulting weight has
L2 norm 1. That is, output y=x*(w/|w|), where x and w denote the input and weight respectively. A scalar scaling factor g is then multiplied to the
output y=y*g. But in my experience g seems not essential for performance (also downstream learnable layers can learn this anyway).&lt;/p&gt;

&lt;p&gt;Cosine Normalization (Luo et al. 2017): weight normalization is very similar to cosine normalization, where the same L2 normalization is applied
to both weight and input: y=(x/|x|)*(w/|w|). Again, manual or automatic differentiation can compute appropriate gradients of x and w.&lt;/p&gt;

&lt;h3 id=&#34;reference-materials&#34;&gt;Reference Materials:&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;http://www.deeplearningbook.org/contents/optimization.html&#34; target=&#34;_blank&#34;&gt;Deep Learning Book, Chapter 8.7.1&lt;/a&gt;&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow&#34; target=&#34;_blank&#34;&gt;Stackoverflow: How could I use Batch Normalization in TensorFlow?&lt;/a&gt;&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://sifaka.cs.uiuc.edu/jiang4/domain_adaptation/survey/node8.html&#34; target=&#34;_blank&#34;&gt;Explaination on Covariance Shift&lt;/a&gt;&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.tensorflow.org/api_docs/python/tf/nn/batch_normalization&#34; target=&#34;_blank&#34;&gt;Tensorflow batch normalization docs&lt;/a&gt;&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://datascience.stackexchange.com/questions/12956/paper-whats-the-difference-between-layer-normalization-recurrent-batch-normal&#34; target=&#34;_blank&#34;&gt;Various Normalization Techniques in Deep Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1607.08022.pdf&#34; target=&#34;_blank&#34;&gt;Instance Normalization&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Amazing GAN - Wasserstein GAN</title>
      <link>/post/amazing-gan---wasserstein-gan/</link>
      <pubDate>Sat, 17 Feb 2018 21:00:00 -0500</pubDate>
      
      <guid>/post/amazing-gan---wasserstein-gan/</guid>
      <description>

&lt;h2 id=&#34;table-of-content&#34;&gt;Table of Content&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#kl-divergence-and-js-divergence&#34;&gt;KL divergence and JS divergence&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#generative-adversarial-networks&#34;&gt;Generative Adversarial Networks&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#global-optimal-loss&#34;&gt;Global optimal loss&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#problem-with-vanilla-gans&#34;&gt;Problem with Vanilla GANs&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#gradient-vanishing&#34;&gt;Gradient Vanishing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#mode-collapse&#34;&gt;Mode Collapse&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#improved-training-of-gans&#34;&gt;Improved Training of GANs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#wasserstein-gan&#34;&gt;Wasserstein GAN&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#earth-mover-distance&#34;&gt;Earth Mover distance&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#comparasion-between-em-distance-and-kl-js-divergence&#34;&gt;Comparasion between EM distance and KL/JS divergence&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#lipschitz-continuity&#34;&gt;Lipschitz continuity&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#modified-algorithm&#34;&gt;Modified Algorithm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#wasserstein-gan-with-gradient-penalty&#34;&gt;Wasserstein GAN with gradient penalty&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#disadvantages-of-gradient-clipping-in-wgan&#34;&gt;Disadvantages of gradient clipping in WGAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#gradient-penalty&#34;&gt;Gradient Penalty&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#reference-materials&#34;&gt;Reference Materials&lt;/a&gt;
&lt;br /&gt;
&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;kl-divergence-and-js-divergence&#34;&gt;KL divergence and JS divergence&lt;/h2&gt;

&lt;p&gt;Before diving into details, let first review two very important metrics to quantify the similarity of two probability distributions:
&lt;a href=&#34;https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence&#34; target=&#34;_blank&#34;&gt;Kullback-Leibler Divergence&lt;/a&gt; and &lt;a href=&#34;https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence&#34; target=&#34;_blank&#34;&gt;Jensen-Shannon Divergence&lt;/a&gt;.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Kullback-Leibler Divergence measures the divergence of probability distribution p(x) to q(x):
$$KL(P||Q) = \int_x P(x)log\frac{P(x)}{Q(x)}dx$$
KL(P||Q) achieves its minimum zero when P(x) and Q(x) are the same everywhere.&lt;br /&gt;
KL divergence is widely used as a metrics to measure the similarity between two distributions. But according to its formula, its is asymmetric. Also, due to the rapid decreasing of logrithm function, KL divergence put to much weight when P(x) is near zero. This can cause some buggy result in real world measurement.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Jensen-Shannon Divergence. JS divergence is based on KL divergence and it is symmetric.&lt;br /&gt;
$$JS(P||Q) = \frac{1}{2} KL(P||\frac{P+Q}{2}) + \frac{1}{2} KL(Q||\frac{P+Q}{2})$$&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Here is a plot of KL and JS divergence of two normal distributions: N(0, 1) and N(1, 1). Image resource &lt;a href=&#34;https://lilianweng.github.io/lil-log/2017/08/20/from-GAN-to-WGAN.html&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;
&lt;img src=&#34;/img/KL_JS_divergence.png&#34; alt=&#34;KL-JS&#34; /&gt;&lt;/p&gt;

&lt;p&gt;As shown in the plot, JS divergence are symmetric while KL divergence is asymmetric. People believe the success of GANs comes from replacing the traditional maximum likelihood with symmetric similarity measure, JS divergence.&lt;/p&gt;

&lt;h2 id=&#34;generative-adversarial-networks&#34;&gt;Generative Adversarial Networks&lt;/h2&gt;

&lt;p&gt;Original GANs consists of two networks:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Generator. It receive random samples and synthesize fake images feeding into discriminator. This random sample brings a potential output diversity.&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Discriminator. It receive the real dataset images and the fake images from generator. It works as a critic to evaluate the probability of input image coming from dataset and from generator.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This is a diagram showing how GAN works:
&lt;img src=&#34;/img/GANs.png&#34; alt=&#34;GAN&#34; /&gt;&lt;/p&gt;

&lt;p&gt;In one hand, we want the discriminator&amp;rsquo;s output probability over real data to be higher by maximizing $\mathbb{E}_{x \sim p_{data}(x)}[logD(x)]$; In another hand, we want the discriminator&amp;rsquo;s output probability over fake data from generator to be lower by minimizing $\mathbb{E}_{z \sim p_(z)}[log(1-D(G(z)))]$.&lt;/p&gt;

&lt;p&gt;And for the generator, we want it to fool the discriminator by minimizing $\mathbb{E}_{z \sim p_(z)}[log(1-D(G(z)))]$.&lt;/p&gt;

&lt;p&gt;So, the overall process of GAN training is obvious:
$$\min_{G} \max_{D} L(G, D) =  [\mathbb{E}_{x \sim p_{data}(x)}[logD(x)] + \mathbb{E}_{z \sim p_(z)}[log(1-D(G(z)))]]$$&lt;/p&gt;

&lt;p&gt;Overall, it is a minimax game between generator and discriminator. The main concern in training procedure is keeping the G and D evolving at the same speed.&lt;/p&gt;

&lt;h3 id=&#34;global-optimal-loss&#34;&gt;Global optimal loss&lt;/h3&gt;

&lt;p&gt;The global minimum of the training criterion $L(G, D)$ is achieved if and only if
$p_g = p_{data}$. Proof of are in the original &lt;a href=&#34;https://arxiv.org/pdf/1406.2661.pdf&#34; target=&#34;_blank&#34;&gt;GAN paper&lt;/a&gt;.&lt;br /&gt;
First, we need to find the optimal solution for D when G is fixed. (Sorry, there is a problem in my equation alignment)&lt;/p&gt;

&lt;p&gt;$$L(G, D) = \int_x p_{data}(x)logD(x)dx +  \int_z p(z)log(1-D(G(z))dz $$
         $$ = \int_x (p_{data}(x)logD(x) + p_g(x)log(1-D(x)))dx $$&lt;/p&gt;

&lt;p&gt;Assume:
$$F(x) = p_{data}(x)logD(x) + p_g(x)log(1-D(x))$$
Take the derivative over $D(x)$:
$$\frac{d f(x)}{dx} = \frac{p_{data}(x)}{D(x)} + \frac{p_g(x)}{1-D(x)} = 0$$
Solve this equation, easily get :
$$D^{\star}(x) = \frac{p_{data}(x)}{p_{data}(x)+p_g(x)}$$&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;When system is trained well, $p_{data}(x)$ and $p_g(x)$ should be similar, $D^{\star}(x) = \frac{1}{2}$.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;When discriminator is optimal, the loss function becomes:&lt;/p&gt;

&lt;p&gt;$$L(G, D) = \int_x p_{data}(x)logD(x)dx +  \int_z p(z)log(1-D(G(z))dz $$&lt;/p&gt;

&lt;p&gt;$$ = \int_x (p_{data}(x)logD(x) + p_g(x)log(1-D(x)))dx $$
$$ = \int_x p_{data}(x)log(\frac{p_{data}(x)}{p_{data}(x)+p_g(x)}) + p_g(x)log(\frac{p_g(x)}{p_{data}(x)+p_g(x)})dx $$
$$ = -log(4) + KL(p_{data} || \frac{p_{data}+p_g}{2}) + KL(p_g || \frac{p_{data}+p_g}{2})$$
$$ = -log(4) + 2* JS(p_{data} || p_g)$$&lt;/p&gt;

&lt;p&gt;So, the loss function of GAN quantify the JS divergence of $p_{data}$ and $p_g$. The optimal value is $- log(4)$ when $p_{data} = p_g$.&lt;/p&gt;

&lt;h2 id=&#34;problem-with-vanilla-gans&#34;&gt;Problem with Vanilla GANs&lt;/h2&gt;

&lt;h3 id=&#34;gradient-vanishing&#34;&gt;Gradient Vanishing&lt;/h3&gt;

&lt;p&gt;The loss function for training G is: $\mathbb{E}_{z \sim p(z)}[log(1-D(G(z)))]$. But in the early stage of training, discriminator can be very confident in detecting results from G, $D(G(z))$ is always 0. In this way, the gradient to update G vanishes.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/GAN_gradient_vanishing.png&#34; alt=&#34;gradient vanishing&#34; /&gt;&lt;/p&gt;

&lt;p&gt;With the generator fixed we
train a discriminator from scratch and measure the gradients with the original cost function. We see
the gradient norms decay quickly, in the best case 5 orders of magnitude after 4000 discriminator
iterations. Note the logarithmic scale.&lt;/p&gt;

&lt;p&gt;We can use an alternative loss function for G: $\mathbb{E}_{z \sim p(z)}[-log(D(G(z)))]$. Instead of minimizing, let G maximizing the logprobability of the discriminator being mistaken. It is heuristically motivated that generator can still
learn even when discriminator successfully rejects
all generator samples, but not theoretically
guaranteed.&lt;/p&gt;

&lt;p&gt;But this will result in gradient unstable issue because the nature of logrithm function.&lt;/p&gt;

&lt;p&gt;So, the training of GAN faces a dilemma:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;If discriminator is trained badly, it fails to provide correct gradient to update generator.&lt;/li&gt;
&lt;li&gt;If discriminator is trained well, it will be too confident and give near 0 score to generator result, which kills the gradient
in the generator.&lt;br /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;To sum up, the difficulty to train a GAN is how to keep G and D in the same pace. This is quite hard to control in practice without a metrics to quantify it.&lt;/p&gt;

&lt;h3 id=&#34;mode-collapse&#34;&gt;Mode Collapse&lt;/h3&gt;

&lt;p&gt;Mode collapse is when the generator generates a limited diversity of samples, or even the same sample, regardless of the input. The main reason also comes from the nature of loss function.  It
is not equally treated when G generates a unreal
sample and when G fails to generate real sample.&lt;/p&gt;

&lt;p&gt;Without any guidance to ensure the diversity of generator, G only care about how to fool discriminator. Once get a good sample that successfully fools discriminator, it will produce this kind of samples as many as possible to optimize the loss function. When discriminator finally realized the mistake during its training, the generator can easily find another perfect example to fool the discriminator and produce a lot of similar samples. This becomes an endless circle between G and D updates. And the loss function value in this process  will have unnecessary oscillations.&lt;/p&gt;

&lt;p&gt;One method to compensate this is putting regularization on the diversity of generator, forcing it to produce various samples. In practice, this method is still not good enough.&lt;/p&gt;

&lt;p&gt;Here is some result of GAN mode collapse in LSUN dataset:
&lt;img src=&#34;/img/mode_collapse.png&#34; alt=&#34;mode collapse&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;improved-training-of-gans&#34;&gt;Improved Training of GANs&lt;/h2&gt;

&lt;p&gt;The following improvement are proposed to help stabilize and improve the training of GANs. These comes from this paper:
&lt;a href=&#34;http://papers.nips.cc/paper/6125-improved-techniques-for-training-gans.pdf&#34; target=&#34;_blank&#34;&gt;Improved Techniques for Training GANs&lt;/a&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Feature Matching&lt;br /&gt;
Feature matching addresses the instability of GANs by specifying a new objective for the generator
that prevents it from overtraining on the current discriminator. Specifically, we train the generator to match the expected value of the
features on an intermediate layer of the discriminator.&lt;br /&gt;
Our new objective for the generator is defined as: $| \mathbb{E}_{x \sim p_r} f(x) - \mathbb{E}_{z \sim p_z(z)}f(G(z)) |_2^2 $, where $f(x)$ denote activations on an intermediate layer of the discriminator.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Mini-batch Discrimination&lt;br /&gt;
The concept of minibatch discrimination is quite general: any discriminator model that looks
at multiple examples in combination, rather than in isolation, could potentially help avoid collapse
of the generator.&lt;br /&gt;
In one minibatch, we approximate the closeness between every pair of samples, $c(x_i, x_j)$, and get the overall status of one data point by summing up how close it is to other samples in the same batch, $o(x_i) = \sum_{j} c(x_i, x_j)$. Then $o(x_i)$ is explicitly added to the input of the model.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Historical averaging&lt;br /&gt;
When applying this technique, we modify each player&amp;rsquo;s cost to include a term $||\theta - \frac{1}{t} \sum_{i=1}^t \theta[i]||^2$
where $\theta[i]$ is the value of the parameters at past time i. The historical average of the parameters can
be updated in an online fashion so this learning rule scales well to long time series.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;One-sided label smoothing&lt;br /&gt;
Replaces the 0 and 1 targets for a classifier with smoothed values, like .9 or .1, and was
recently shown to reduce the vulnerability of neural networks to adversarial examples&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Virtual batch normalization&lt;br /&gt;
Each example x is normalized based on
the statistics collected on a reference batch of examples that are chosen once and fixed at the start
of training. The reference batch is normalized using only its own statistics.&lt;br /&gt;
VBN is
computationally expensive because it requires running forward propagation on two minibatches of
data, so we use it only in the generator network.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;wasserstein-gan&#34;&gt;Wasserstein GAN&lt;/h2&gt;

&lt;h3 id=&#34;earth-mover-distance&#34;&gt;Earth Mover distance&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Earth_mover%27s_distance&#34; target=&#34;_blank&#34;&gt;Earth Mover distance(EM distance, Wasserstein distance)&lt;/a&gt; is another metrics on the similarity:
$$ W(p_r, p_g) = \inf_{\gamma \sim \Pi(p_r, p_g)} \mathbb{E}_{(x, y) \sim \gamma}[| x-y |] $$&lt;/p&gt;

&lt;p&gt;Looks like a very complex formula, but actually quite simple. Intuitively, $\gamma(x, y)$ indicates how much &amp;lsquo;mass&amp;rsquo;
must be transported from x to y in order to transform the distributions $p_g$
into the distribution $p_r$. The EM distance then is the &amp;lsquo;cost&amp;rsquo; of the optimal
transport plan. PS. Notice it is the expection over $| x-y |$, the total movement should be:&lt;/p&gt;

&lt;p&gt;$$ \sum_{x, y} \gamma(x, y) | x-y | = \mathbb{E}_{x, y \sim \gamma} | x-y | $$&lt;/p&gt;

&lt;p&gt;For example, we get two distributions:&lt;/p&gt;

&lt;p&gt;$$p_r : p_r(0) = \frac{1}{4}, p_r(1) = \frac{1}{4}, p_r(2) = \frac{1}{2}$$&lt;/p&gt;

&lt;p&gt;And:&lt;/p&gt;

&lt;p&gt;$$p_g : p_g(0) = \frac{1}{2}, p_g(1) = \frac{1}{2}, p_g(2) = 0$$&lt;/p&gt;

&lt;p&gt;The optimal plan to move from $p_r$ to $p_g$ should be move $\frac{1}{4}$ from $p_r(2)$ to $p_r(1)$ and $p_r(0)$. So, the EM distance of this two distribution is $\frac{1}{4} * |2-0| + \frac{1}{4} * |2-1| = \frac{3}{4}$.&lt;/p&gt;

&lt;h3 id=&#34;comparasion-between-em-distance-and-kl-js-divergence&#34;&gt;Comparasion between EM distance and KL/JS divergence&lt;/h3&gt;

&lt;p&gt;The example from Wasserstein GAN paper is pretty good. Let $ Z \sim U[0, 1] $ is a uniform distribution on unit interval.
Let $P_0$ be the distribution of $(0, Z) \in R^2 $
(0 on the x-axis and
the random variable Z on the y-axis), uniform on a straight vertical line passing
through the origin. Now let $g_{\theta}(z) = (\theta, z)$ with $\theta$ a single real parameter. It is easy
to see that in this case when $\theta \neq 0$, (if $\theta = 0$, all these are zero):&lt;/p&gt;

&lt;p&gt;$$EM(P_0, P_{\theta}) = |\theta| $$
$$JS(P_0, P_{\theta}) = log2 $$
$$KL(P_0, P_{\theta}) = \infty $$&lt;/p&gt;

&lt;p&gt;KL gives us inifity when two distributions are non-overlapped. And this situation is quite normal in high dimension space. The value of JS has sudden jump, not differentiable at $\theta=0$. Only Wasserstein metric provides a smooth measure, which is super helpful to provide stable gradient to in training.&lt;/p&gt;

&lt;h3 id=&#34;lipschitz-continuity&#34;&gt;Lipschitz continuity&lt;/h3&gt;

&lt;p&gt;The definition of Lipschitz continuity is:
A real-valued function $f: \mathbb{R} \rightarrow \mathbb{R}$ is called $K$-Lipschitz continuous if there exists a real constant $K \geq 0$ such that, for all $x_1, x_2 \in \mathbb{R}$,&lt;/p&gt;

&lt;p&gt;$$\lvert f(x_1) - f(x_2) \rvert \leq K \lvert x_1 - x_2 \rvert$$&lt;/p&gt;

&lt;p&gt;if a function is differentiable everywhere, it&amp;rsquo;s derivative should be bounded in $[-K, K]$.&lt;/p&gt;

&lt;p&gt;The infimum in the earth mover distance is highly intractable. Also, it would be impossible to search all the cases to move one probability distribution to another. Here the Kantorovich-Rubinstein duality tells us that:&lt;/p&gt;

&lt;p&gt;$$ W(p_r, p_g) = \frac{1}{K} \sup_{| f |_L \leq K} \mathbb{E}_{x \sim p_r}[f(x)] - \mathbb{E}_{x \sim p_g}[f(x)] $$&lt;/p&gt;

&lt;p&gt;where the supremum is over all the 1-Lipschitz functions. If you want to know more about Kantorovich-Rubinstein duality, see this awesome &lt;a href=&#34;https://vincentherrmann.github.io/blog/wasserstein/&#34; target=&#34;_blank&#34;&gt;blog&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Suppose this function $f$ comes from a family of K-Lipschitz continuous functions, ${ f_w }$, parameterized by $w$. The above equation becomes:&lt;/p&gt;

&lt;p&gt;$$ W(p_r, p_g) = \max_{w \in W} \mathbb{E}_{x \sim p_r}[f_w(x)] - \mathbb{E}_{z \sim p_r(z)}[f_w(g_\theta(z))] $$&lt;/p&gt;

&lt;p&gt;In the modified Wasserstein-GAN, the &amp;ldquo;discriminator&amp;rdquo; model is used to learn $w$ to find a good $f_w$ and the loss function is equivalent to measure the Wasserstein distance between $p_r$ and $p_g$.&lt;/p&gt;

&lt;p&gt;In this perspective, the discriminator can be treated as a K-Lipschitz function to measure the Wasserstein distance between the distribution of $p_r$ and $p_g$. Since the Wasserstein distance is a smooth measure, the gradient should be stable and make $p_r$ closer to $p_g$.&lt;/p&gt;

&lt;h3 id=&#34;modified-algorithm&#34;&gt;Modified Algorithm&lt;/h3&gt;

&lt;p&gt;The next thing to care about is how to keep the discriminator function satisfying K-Lipschitz continuity. Wasserstein GAN adopts the most simple method, clipping the gradient into a small interval ($[-0.01, 0.01]$), and make the parameter $w$ lies in a compact space and  $f_w$ will preserve its Lipschitz continuity. The modified algorithm is as follows:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/GAN_algorithm.png&#34; alt=&#34;WGAN&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Compared to original GANs, Wasserstein GAN takes these changes:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Remove the sigmoid function at the end of Discriminator&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Remove the log function in generator and discriminator loss function&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Clip the gradient norm into an interval $[-c, c]$&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Empirically use optimizers without momentum term, like RMSprop, not Adam.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The first two modification comes from a brand-new loss function derived from Wasserstein distance. It is not a probability score anymore, so no sigmoid and logrithm is needed. And the last change comes from author&amp;rsquo;s experience, it should be a practical suggestion for training.&lt;/p&gt;

&lt;h3 id=&#34;wasserstein-gan-with-gradient-penalty&#34;&gt;Wasserstein GAN with gradient penalty&lt;/h3&gt;

&lt;p&gt;In the original Wasserstein GAN paper, the author admitted gradient clipping is a terrible idea enforce a Lipschitz constraint:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Weight clipping is a clearly terrible way to enforce a Lipschitz constraint. If the
clipping parameter is large, then it can take a long time for any weights to reach
their limit, thereby making it harder to train the critic till optimality. If the clipping
is small, this can easily lead to vanishing gradients when the number of layers is
big, or batch normalization is not used.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;So, this paper &lt;a href=&#34;https://arxiv.org/pdf/1704.00028.pdf&#34; target=&#34;_blank&#34;&gt;Improved Training of Wasserstein GANs&lt;/a&gt; proposed a new Wasserstein GAN with gradient penalty to replace gradient clipping.&lt;/p&gt;

&lt;h4 id=&#34;disadvantages-of-gradient-clipping-in-wgan&#34;&gt;Disadvantages of gradient clipping in WGAN&lt;/h4&gt;

&lt;p&gt;The author found weight clipping in WGAN leads to optimization difficulties, and that even when optimization
succeeds the resulting critic can have a pathological value surface.&lt;/p&gt;

&lt;p&gt;Their experiments use the specific form of weight constraint like hard clipping of the magnitude
of each weight, L2 norm clipping, weight normalization,
as well as soft constraints (L1 and L2 weight decay) and found that they exhibit similar problems.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Capacity underuse&lt;br /&gt;
Under a weight-clipping constraint, the authors observe that neural
network architectures try to attain their maximum gradient norm k end up learning extremely
simple functions. Also, the critic trained with weight clipping ignores higher moments of the data distribution
and instead models very simple approximations to the optimal functions.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Gradient vanishing and exploding
Without careful tuning of the clipping threshold c, the network can easily stuck in gradient vanishing and exploding.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&#34;gradient-penalty&#34;&gt;Gradient Penalty&lt;/h4&gt;

&lt;p&gt;The authors provide an alternative way to enforce the Lipschitz constraint. First, they proved that a differentiable function
is 1-Lipschtiz if and only if it has gradients with norm at most 1 everywhere. So, gradient penalty term is add to directly
constrain the gradient norm of the critic&amp;rsquo;s output with respect to its input. The new loss function is:
$$L(G, D) = \mathbb{E}_{x \sim p_r}[f_w(x)] - \mathbb{E}_{z \sim p_r(z)}[f_w(g_\theta(z))] + \lambda \mathbb{E}_{\hat x \sim \mathbb{P}_{\hat x}}[(||\nabla{\hat{x}}D(\hat x)||_2 - 1)^2]$$&lt;/p&gt;

&lt;p&gt;So, they use a soft version constraint, putting penalty on gradient norm for random samples $\hat x \sim \mathbb{P}_{\hat x}$, where $\mathbb{P}_{\hat x}$ is drawn from a straight line between pairs of points sampled from the data distribution $\mathbb{P}_r$ and the generator distribution $\mathbb{P}_g$. Note the hyperparameter $\lambda = 10$&lt;/p&gt;

&lt;p&gt;The main reason they do this is that enforcing the unit gradient norm constraint everywhere is intractable, enforcing it only along these straight lines seems sufficient and experimentally results in good performance.&lt;/p&gt;

&lt;p&gt;Also, the batch norm layer can make this penalty invalid, so WGAN-GP simply remove the batch norm layers. And WGAN-GP takes two side penalty. Instead of forcing the gradient norm smaller than 1, it encourage gradient norm closer to 1 according to their proof. In practice, this works slightly better. Here is the algorithm of WGAN-GP:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/WGANGP.jpg&#34; alt=&#34;WGAN-GP&#34; /&gt;&lt;/p&gt;

&lt;p&gt;And the implementation on tensorflow is quite simple with tf.gradient function.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt; 
 with tf.name_scope(&#39;gan_loss&#39;):

    D_loss = tf.reduce_mean(fake_score) - tf.reduce_mean(real_score)

    G_loss = -tf.reduce_mean(fake_score)

    def interpolate(a, b):
        shape = tf.concat((tf.shape(a)[0:1], tf.tile([1], [a.shape.ndims - 1])), axis=0)
        alpha = tf.random_uniform(shape=shape, minval=0., maxval=1.)
        inter = a + alpha * (b - a)
        inter.set_shape(a.get_shape().as_list())
        return inter

    gp_sample = interpolate(gen, image_hr)

    gp_gradient = tf.gradients(net.discrimintor(gp_sample, reuse=True), gp_sample)

    grad_norm = tf.sqrt(tf.reduce_sum(tf.square(gp_gradient[0]), reduction_indices=[-1]))

    gp_loss = tf.reduce_mean(tf.square(grad_norm-1.))

    D_overall_loss = D_loss + gp_rate*gp_loss

    tf.summary.scalar(&#39;G_loss&#39;, (G_loss))
    tf.summary.scalar(&#39;D_loss&#39;, (D_loss))
    tf.summary.scalar(&#39;GP_loss&#39;, gp_loss)

    G_overall_loss = gan_ratio*G_loss + SR_loss 
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;reference-materials&#34;&gt;Reference Materials&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1406.2661&#34; target=&#34;_blank&#34;&gt;Original Paper of GAN: Generative Adversarial Nets&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1611.02163.pdf&#34; target=&#34;_blank&#34;&gt;Unrolled Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1701.04862.pdf&#34; target=&#34;_blank&#34;&gt;Towards Principled Methods for Training Generative Adversarial Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1701.07875.pdf&#34; target=&#34;_blank&#34;&gt;Wasserstein GAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://papers.nips.cc/paper/6125-improved-techniques-for-training-gans.pdf&#34; target=&#34;_blank&#34;&gt;Improved Techniques for Training GANs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://lilianweng.github.io/lil-log/2017/08/20/from-GAN-to-WGAN.html&#34; target=&#34;_blank&#34;&gt;Another great blog of WGAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1704.00028.pdf&#34; target=&#34;_blank&#34;&gt;WGAN-GP: Improved Training of Wasserstein GANs&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Microscale two-dimensional (2D) temperature mapping by ratiometric fluorescence imaging under orthogonal excitations</title>
      <link>/publication/temperature_mapping/</link>
      <pubDate>Thu, 15 Feb 2018 00:00:00 -0500</pubDate>
      
      <guid>/publication/temperature_mapping/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Neural Network based Object Recognition: Fast/Faster/Mask R-CNN, SSD, and YOLO</title>
      <link>/post/neural-network-based-object-recognition-fast-faster-mask-r-cnn-ssd-and-yolo/</link>
      <pubDate>Mon, 01 Jan 2018 21:00:00 -0500</pubDate>
      
      <guid>/post/neural-network-based-object-recognition-fast-faster-mask-r-cnn-ssd-and-yolo/</guid>
      <description>

&lt;h2 id=&#34;table-of-content&#34;&gt;Table of Content&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#history&#34;&gt;History&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#alexnet&#34;&gt;AlexNet&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#resnet&#34;&gt;ResNet&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#inception-network-series&#34;&gt;Inception Network Series&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#deformable-parts-model&#34;&gt;Deformable Parts Model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#overfeat&#34;&gt;Overfeat&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#metrics--map&#34;&gt;Metrics: mAP&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#precision-and-recall&#34;&gt;Precision and Recall&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#average-precision--ap-&#34;&gt;Average precision (AP)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#mean-average-precision--map-&#34;&gt;mean average precision (mAP)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#r-cnn-families&#34;&gt;R-CNN Families&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#rcnn&#34;&gt;RCNN&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#selective-search&#34;&gt;Selective Search&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#bounding-box-regression&#34;&gt;Bounding Box Regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#non-maximum-suppression&#34;&gt;Non Maximum Suppression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#workflow&#34;&gt;Workflow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#speed-bottleneck&#34;&gt;Speed Bottleneck&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#fast-rcnn&#34;&gt;Fast-RCNN&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#roi-pooling&#34;&gt;ROI pooling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#workflow-1&#34;&gt;Workflow&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#faster-rcnn&#34;&gt;Faster-RCNN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#mask-rcnn&#34;&gt;Mask-RCNN&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#single-shot-detector&#34;&gt;Single Shot Detector&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#single-shot-multibox-detector-ssd-&#34;&gt;Single Shot MultiBox Detector(SSD)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#you-only-look-once-yolo-&#34;&gt;You Only Look Once(YOLO)&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#yolov1&#34;&gt;YOLOv1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#yolov2&#34;&gt;YOLOv2&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#yolov3&#34;&gt;YOLOv3&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#summary&#34;&gt;Summary&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#reference-materials&#34;&gt;Reference Materials&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It&amp;rsquo;s well known that deep learning has been a real game changer in machine learning, especially in computer vision. Similar to image classification, deep learning/neural network represent the state of the art in modern object recognition.&lt;/p&gt;

&lt;h1 id=&#34;history&#34;&gt;History&lt;/h1&gt;

&lt;p&gt;To have a better intuition about the challenges and algorithm evolving, first we will have an overview about the progress of deep learning approach in the last couple of years. This progress also comes with the progress of image classification.&lt;/p&gt;

&lt;h3 id=&#34;alexnet&#34;&gt;AlexNet&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf&#34; target=&#34;_blank&#34;&gt;AlexNet&lt;/a&gt; famously won the 2012 ImageNet LSVRC-2012 competition by a large margin (15.3% VS 26.2% (second place) error rates). It stands for the revival of neural network as well as deep learning.&lt;/p&gt;

&lt;p&gt;The main contribution of AlexNet is as follows:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Use ReLU activation function instead of tanh and sigmoid to introduce nonlinearity to the network. Also, it eases the gradient vanishing problem when training deep neural network.&lt;/li&gt;
&lt;li&gt;Introduce dropout as a regularization. It works like an ensemble of different networks.&lt;/li&gt;
&lt;li&gt;Use two NVIDIA GPUs and CUDA, also parallel architecture to accelerate training.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Here is a diagram of famous AlexNet:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/AlexNet.png&#34; alt=&#34;AlexNet&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;resnet&#34;&gt;ResNet&lt;/h3&gt;

&lt;p&gt;After AlexNet, the next significant work should be &lt;a href=&#34;https://arxiv.org/abs/1512.03385&#34; target=&#34;_blank&#34;&gt;ResNet&lt;/a&gt;.&lt;br /&gt;
These networks led to 1st-place winning entries in all five main tracks of the ImageNet and COCO 2015 competitions, which covered image classification, object detection, and semantic segmentation.&lt;/p&gt;

&lt;p&gt;The main difference of ResNet is just a skip connection between layers. The author’s hypothesis is that it is easy to optimize the residual mapping function F(x) than to optimize the original, unreferenced mapping H(x). And this method turns out to be very efficient to solve network degradation in deep network.&lt;/p&gt;

&lt;p&gt;Here is a diagram of residual blocks (image source: ):&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/residualblock.png&#34; alt=&#34;residual&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Plus, Resnet uses the bottleneck architecture from Inception network to reduce computation. After $1\times1$ convolution, the feature map channels are greatly reduced in order to save computation for next $3 \times 3$ convolution layer. Another $1\times1$ convolution after that can resume the feature map channels. Amazingly, this operation won&amp;rsquo;t affect the performance. So, this bottleneck architecture is quite popular in modern neural network.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/bottleneck.jpg&#34; alt=&#34;bottleneck&#34; /&gt;&lt;/p&gt;

&lt;p&gt;ResNet can be interpreted as ensembles of relatively shallow networks, as described &lt;a href=&#34;https://arxiv.org/abs/1605.06431?context=cs&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;inception-network-series&#34;&gt;Inception Network Series&lt;/h3&gt;

&lt;p&gt;Inception network series comes from Google. It should be the most well-designed and popular network in image classification community. The architecture becomes very complicated in later version of Inception networks. For example, here is a diagram of InceptionResnetV2, with more than 600 layers:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/image00.png&#34; alt=&#34;inceptionresnetv2&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The core of Inception network is the Inception module. The inspiration comes from the idea that you need to make a decision as to what type of convolution you want to make at each layer: $3 \times 3$ or $5 \times 5$? You may need specific image content to decide. How about use them all together? Here come the Inception module:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/inception_implement.png&#34; alt=&#34;Inceptionmodule&#34; /&gt;&lt;/p&gt;

&lt;p&gt;It uses all $1 \times 1$, $3 \times 3$, $5 \times 5$ convolution and  altogether and concate their result together. Plus, inception module introduces $N \times 1$ and $1 \times N$ convolution instead of traditional $N \times N $ convolution. This puts constraints on convolution kernel (center symmetric), but can greatly reduces computation. And residual connection, bottleneck architecture also appears in modern Inception network. Here is a diagram of Inception modules in InceptionV4:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/inceptionv4.jpeg&#34; alt=&#34;v4&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;deformable-parts-model&#34;&gt;Deformable Parts Model&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;http://vision.stanford.edu/teaching/cs231b_spring1213/slides/dpm-slides-ross-girshick.pdf&#34; target=&#34;_blank&#34;&gt;Deformable Parts Model (DPM)&lt;/a&gt; was invented by Pedro Felzenszwalb ands Ross Girshick, who becomes the leader of object recognition commumity with his R-CNN families.&lt;/p&gt;

&lt;p&gt;The model consists of three major components:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;A coarse root filter defines a detection window that approximately covers an entire object. A filter specifies weights for a region feature vector.&lt;/li&gt;
&lt;li&gt;Multiple part filters that cover smaller parts of the object. Parts filters are learned at twice resolution of the root filter.&lt;/li&gt;
&lt;li&gt;A spatial model for scoring the locations of part filters relative to the root.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Deformable Part Models model an object as the sum of the geometric deformations of it. So take person model for example:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/DPM.jpeg&#34; alt=&#34;DPM&#34; /&gt;&lt;/p&gt;

&lt;p&gt;A root location with high score detects a region with high chances to contain an object, while the locations of the parts with high scores confirm a recognized object hypothesis. The paper adopted latent SVM to model the classifier.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/DPMmodel.png&#34; alt=&#34;DPMmodel&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Also, DPM and later CNN models are not two different method to object recognition. Actually, DPM can be unrolled and interpreted as several equivalent CNN layers. See Ross&amp;rsquo;s paper &lt;a href=&#34;https://arxiv.org/abs/1409.5403&#34; target=&#34;_blank&#34;&gt;Deformable Part Models are Convolutional Neural Networks&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;overfeat&#34;&gt;Overfeat&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1312.6229&#34; target=&#34;_blank&#34;&gt;Overfeat&lt;/a&gt; is a poineer to incorprate CNN models into object detection, localization and classification tasks.&lt;/p&gt;

&lt;h1 id=&#34;metrics-map&#34;&gt;Metrics: mAP&lt;/h1&gt;

&lt;p&gt;mAP is the metric to measure the accuracy of object detectors like Faster R-CNN, SSD, etc. AP is the average of the maximum precisions at different recall values. And mAP is the mean of AP in different classes. This sounds complicated, so let&amp;rsquo;s explain this step by step.&lt;/p&gt;

&lt;h3 id=&#34;precision-and-recall&#34;&gt;Precision and Recall&lt;/h3&gt;

&lt;p&gt;According to &lt;a href=&#34;https://en.wikipedia.org/wiki/Precision_and_recall#F-measure&#34; target=&#34;_blank&#34;&gt;wiki&lt;/a&gt;, &lt;strong&gt;precision&lt;/strong&gt; (also called positive predictive value) is the fraction of relevant instances among the retrieved instances, for example, percentage of your positive predictions are correct.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Recall&lt;/strong&gt; (also known as sensitivity) is the fraction of relevant instances that have been retrieved over the total amount of relevant instances. for example, percentage of positive instances in your positive predictions.&lt;/p&gt;

&lt;p&gt;Here is an image of precision and recall:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/precision-recall.jpg&#34; alt=&#34;precision and recall&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;average-precision-ap&#34;&gt;Average precision (AP)&lt;/h3&gt;

&lt;p&gt;First of all, a prediction is consider to be correct if IoU is greater than a threshold, usually 0.5. It is hard to explain AP by words, so let&amp;rsquo;s create an example to make it clear. (&lt;a href=&#34;https://medium.com/@jonathan_hui/map-mean-average-precision-for-object-detection-45c121a31173&#34; target=&#34;_blank&#34;&gt;example source&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;For example, if our image has 5 objects. First, we sort the predictions of our detector based on the confidence level. We get correct prediction in 1st, 2nd, 4th, 6th and 10th prediction. So, in the top 4 predictions, we get 3 of them correct. A prediction is consider to be correct if IoU is greater than 0.5. So the precision and the recall for our top 4 predictions is:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Precision&lt;/strong&gt; is the proportion of TP = &lt;sup&gt;3&lt;/sup&gt;&amp;frasl;&lt;sub&gt;4&lt;/sub&gt; = 0.75.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Recall&lt;/strong&gt; is the proportion of TP out of the possible positives = &lt;sup&gt;3&lt;/sup&gt;&amp;frasl;&lt;sub&gt;5&lt;/sub&gt; = 0.6.&lt;/p&gt;

&lt;p&gt;Here is a table of different corresponding precision and recall ranks:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/precisionrecallrank.png&#34; alt=&#34;prrank&#34; /&gt;&lt;/p&gt;

&lt;p&gt;AP (average precision) is computed as the average of &lt;strong&gt;maximum precision&lt;/strong&gt; at different recall levels. Here are two kinds of AP calculation:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Before 2010, people get the precision value at 11 recall levels (0.0, 0.1, 0.2 &amp;hellip; 1.0), and average them.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;$$AP = \frac{1}{11} (AP_r(0.0) + AP_r(0.1) + &amp;hellip; AP_r(1.0))$$&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;After 2010, people calculate the precision value at all different recall levels and average them. Here K is the number of total different recall levels.&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;$$AP = \sum_{i = 0}^K AP_r(i)$$&lt;/p&gt;

&lt;p&gt;The $AP_r$ represent the &lt;strong&gt;maximum precision&lt;/strong&gt; in certain recall level.&lt;/p&gt;

&lt;h3 id=&#34;mean-average-precision-map&#34;&gt;mean average precision (mAP)&lt;/h3&gt;

&lt;p&gt;Latest research papers tend to give results for the COCO dataset only. For COCO, AP is the average over multiple IoU. &lt;strong&gt;AP@[.5:.95]&lt;/strong&gt; corresponds to the average AP for IoU from 0.5 to 0.95 with a step size of 0.05. AP (which is also called mAP in COCO) averages AP over all class categories.&lt;/p&gt;

&lt;h1 id=&#34;r-cnn-families&#34;&gt;R-CNN Families&lt;/h1&gt;

&lt;h2 id=&#34;rcnn&#34;&gt;RCNN&lt;/h2&gt;

&lt;p&gt;The goal of &lt;a href=&#34;https://arxiv.org/abs/1311.2524&#34; target=&#34;_blank&#34;&gt;R-CNN&lt;/a&gt; is to take in an image, and correctly identify where are objects (via a bounding box) in the image. The intuition of RCNN is quite simple, just proposing some possible regions, and determine whether they are objects or not. Here is a diagram of RCNN:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/RCNN.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;selective-search&#34;&gt;Selective Search&lt;/h3&gt;

&lt;p&gt;R-CNN uses Selective Search method to extract region proposals.&lt;/p&gt;

&lt;p&gt;This method uses an over-segmentation method to divide the image into small regions (1k to 2k). Merge the two most probable adjacent areas according to the consolidation rules. This is is a hierarchical grouping algorithm by J.R.R. Uijlings.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/grouping.png&#34; alt=&#34;grouping&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Actually the similarity of regions is determined by their color histogram, texture(gradient histogram) and some other aspects. Note that this method tends to merge relatively small regions to avoid a large area gradually &amp;ldquo;eat&amp;rdquo; other small areas. Also, the method tend to merge the regions whose merged result takes large percentage in its bounding box.&lt;/p&gt;

&lt;p&gt;Repeat until the entire image merges into one area position. Output all areas that existed once, so-called candidate areas. And generate bounding box for them as region proposals.&lt;/p&gt;

&lt;h3 id=&#34;bounding-box-regression&#34;&gt;Bounding Box Regression&lt;/h3&gt;

&lt;p&gt;The Bounding Box Regressors are essential because the initial region proposals might not fully coincide with the region that is indicated by the learned features of the Convolutional Neural Network. It is a kind of a refinement step.&lt;/p&gt;

&lt;p&gt;So, based on the features obtained at the end of the final pooling layer, the region proposals are regressed. Note that only when region proposals have large IoU(&amp;gt;0.6) with ground truth, we can treat this transformation as linear model and do regression. The bounding box regression process is quite straight forward in RCNN paper.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/bbregression.png&#34; alt=&#34;bbregression&#34; /&gt;&lt;/p&gt;

&lt;p&gt;We directly regress the location $P_x$ and $P_y$, while regress the width $P_w$ and height $P_h$ in log-space which produce better results on the wide range of bounding-box shift.&lt;/p&gt;

&lt;h3 id=&#34;non-maximum-suppression&#34;&gt;Non Maximum Suppression&lt;/h3&gt;

&lt;p&gt;In the end, we can have a lot of bounding boxes on an object. We need to determine the best bounding box for this object. So, what NMS does is as follows:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;First calculate the area of each bounding box, and then sort according to the score&lt;/li&gt;
&lt;li&gt;Use the largest bounding box with the score as the selected box, calculate the rest of the bounding box and the current maximum score and the IoU of the box, remove the IoU greater than the set threshold Bounding box.&lt;/li&gt;
&lt;li&gt;Then repeat the above process until the candidate bounding box is empty&lt;/li&gt;
&lt;li&gt;Delete the selected box with the score less than a certain threshold to get this kind of result&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&#34;/img/NMS.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;workflow&#34;&gt;Workflow&lt;/h3&gt;

&lt;p&gt;R-CNN is formulated as follows:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Apply &lt;a href=&#34;#selective-search&#34;&gt;selective search&lt;/a&gt; to input image, and generate ~2k candidates per image containing target objects with different sizes. Region candidates are warped to have a fixed size as required by CNN.&lt;/li&gt;
&lt;li&gt;Start from a pre-train a CNN network on image classification tasks. Continue fine-tuning the CNN on warped proposal regions for K + 1 classes; The additional one class refers to the background (no object of interest). In the fine-tuning stage, we should use a much smaller learning rate and the mini-batch oversamples the positive cases because most proposed regions are just background.&lt;/li&gt;
&lt;li&gt;Given every image region, one forward propagation through the CNN generates a feature vector. This feature vector is then consumed by a binary SVM trained for each class independently.
The positive samples are proposed regions with IoU (intersection over union) overlap threshold &amp;gt;= 0.3, and negative samples are irrelevant others.&lt;/li&gt;
&lt;li&gt;Use &lt;a href=&#34;#bounding-box-regression&#34;&gt;bounding box regression&lt;/a&gt; to refine the region proposals with CNN feature vector. Also &lt;a href=&#34;#non-maximum-suppression&#34;&gt;NMS&lt;/a&gt; to delete redundant region proposals.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;speed-bottleneck&#34;&gt;Speed Bottleneck&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;Selective search for proposal is slow.&lt;/li&gt;
&lt;li&gt;Calculate feature vectors for every region proposals.&lt;/li&gt;
&lt;li&gt;Detection process consists of three steps without any computation sharing.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Later work mainly concentrate on the three aspects above to achieve real-time detection.&lt;/p&gt;

&lt;h2 id=&#34;fast-rcnn&#34;&gt;Fast-RCNN&lt;/h2&gt;

&lt;p&gt;Fast RCNN mainly deal with the second speed bottleneck of RCNN: Calculate feature vectors for every region proposals.&lt;/p&gt;

&lt;p&gt;Instead of extracting CNN feature vectors independently for each region proposal, this model aggregates them into one CNN forward pass over the entire image and the region proposals share this feature matrix. Then the same feature matrix is branched out to be used for learning the object classifier (No SVM but softmax) and the bounding-box regressor.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/fastrcnn.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;roi-pooling&#34;&gt;ROI pooling&lt;/h3&gt;

&lt;p&gt;Indeed, there is nothing magic about ROI pooling. It is used to generate fixed size feature map regardless of the input image(region proposal) size.&lt;/p&gt;

&lt;p&gt;For example, the feature size of a projected region is $h \times w$, and we want a fixed feature map with size $H \times W$. ROI pooling will divide original feature map into $H \times W$ grids, approximately every subwindow of size $h/H \times w/W$. Then apply max-pooling in each grid.&lt;/p&gt;

&lt;p&gt;Here is a diagram of ROI pooling from cs231n Lecture:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/roi-pooling.png&#34; width=&#34;600&#34;&gt;&lt;/p&gt;

&lt;h3 id=&#34;workflow-1&#34;&gt;Workflow&lt;/h3&gt;

&lt;p&gt;Fast R-CNN is formulated as follows:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;The same with R-CNN, Apply &lt;a href=&#34;#selective-search&#34;&gt;selective search&lt;/a&gt; to input image, and generate ~2k candidates per image containing target objects with different sizes.&lt;/li&gt;
&lt;li&gt;Start from a pretrained ImageNet model.

&lt;ul&gt;
&lt;li&gt;Replace the last max pooling layer of the pre-trained CNN with a RoI pooling layer to ensure fixed output size.&lt;/li&gt;
&lt;li&gt;Different from R-CNN, only do CNN forward pass once, and directly map region proposals to their corresponding feature map. Shared computation speed up the whole process.&lt;/li&gt;
&lt;li&gt;Replace the last fully connected layer and the last softmax layer (K classes) with a fully connected layer and softmax over K + 1 classes.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Finally the model branches into two output layers:

&lt;ul&gt;
&lt;li&gt;A softmax classifier of K + 1 classes (extra one is the background class), output a discrete probability distribution per RoI.&lt;/li&gt;
&lt;li&gt;A bounding-box regression model to refine original RoI for each of K classes.&lt;/li&gt;
&lt;li&gt;The overall loss function is a combination of classification loss and bounding box location loss.
&lt;br /&gt;
&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;faster-rcnn&#34;&gt;Faster-RCNN&lt;/h2&gt;

&lt;p&gt;The intuition of &lt;a href=&#34;https://arxiv.org/pdf/1506.01497.pdf&#34; target=&#34;_blank&#34;&gt;Faster-RCNN&lt;/a&gt; is simple: since CNN is so good and efficient, and using selective search is too slow, why not also using CNN to generate region proposals? So, the main difference between faster-RCNN and fast RCNN is the region proposal network (RPN) to generate region proposals with CNN.&lt;/p&gt;

&lt;p&gt;Here is an illustratioon of Faster RCNN:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/faster-RCNN.png&#34; width=&#34;800&#34;&gt;&lt;/p&gt;

&lt;h3 id=&#34;region-proposal-network-rpn&#34;&gt;Region Proposal Network (RPN)&lt;/h3&gt;

&lt;p&gt;Region proposal network consists of three parts:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Feed input image into CNN and get a set of convlutional feature maps on the last convolutional layer.&lt;/li&gt;
&lt;li&gt;Then a sliding window is running spatially on these feature maps. The size of sliding window is $3 \times 3$. For each sliding window, a set of 9 anchors are generated which all have the same center (the same with sliding window) but with 3 different aspect ratios (1:2, 1:1, 2:1) and 3 different scales. Note that all these coordinates are computed with respect to the original image. (image &lt;a href=&#34;https://www.quora.com/How-does-the-region-proposal-network-RPN-in-Faster-R-CNN-work&#34; target=&#34;_blank&#34;&gt;source&lt;/a&gt;)
&lt;img src=&#34;/img/anchor.png&#34; width=&#34;600&#34;&gt;&lt;/li&gt;
&lt;li&gt;Finally, the 3×3 spatial features extracted from convolution feature maps are fed to a smaller network which has two tasks: classification (cls) and regression (reg). The output of regressor determines a a predicted bounding box (x,y,w,h), The classification network produces a probability p indicating whether the the predicted box contains an object (1) or it is from background (0 for no object).
&lt;img src=&#34;/img/rpn-end.jpg&#34; width=&#34;600&#34;&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;So, the output of rpn is similar to selective search method, which is ~2k region proposals. But the speed of RPN is much faster than selective search method. In this way, the last speed bottleneck of RCNN is addressed. Note that if accuracy is not important, RPN can complete object recognition by its own by modify the classification into N classes.&lt;/p&gt;

&lt;h3 id=&#34;workflow-2&#34;&gt;Workflow&lt;/h3&gt;

&lt;p&gt;Except the RPN, the rest of Faster R-CNN is the same with Fast R-CNN. Faster R-CNN is formulated as follows:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Start from ImageNet pretrained model, fine-tune the RPN (region proposal network) end-to-end for the region proposal task. Positive samples have IoU (intersection-over-union) &amp;gt; 0.7, while negative samples have IoU &amp;lt; 0.3.&lt;/li&gt;
&lt;li&gt;Train a Fast R-CNN object detection model using the proposals generated by the current RPN&lt;/li&gt;
&lt;li&gt;Then use the Fast R-CNN network to initialize RPN training. While keeping the shared convolutional layers, only fine-tune the RPN-specific (background/object classification and bounding box regression) layers. At this stage, RPN and the detection network share convolutional layers.&lt;/li&gt;
&lt;li&gt;Finally fine-tune the unique layers of Fast R-CNN&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;mask-rcnn&#34;&gt;Mask-RCNN&lt;/h2&gt;

&lt;p&gt;So far, we’ve seen how CNN features are interesting and effective in object recognition with bounding boxes. Recently, Kaiming He, Ross Girshick and a team of researchers uses CNN features to another important aspect of object recognition: instance segmentation. They name their work &lt;a href=&#34;https://arxiv.org/abs/1703.06870&#34; target=&#34;_blank&#34;&gt;Mask RCNN&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Semantic segmentation need to segment and cluster pixels of different object classes in a single graph; while instance segmentation need to locate every single object and cluster pixels of this object. This image shows the difference of semantic and instance segmentation:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/instancesegmentation.png&#34; width=&#34;600&#34;&gt;&lt;/p&gt;

&lt;p&gt;So, overall, instance is more difficult than semantic segmentation. And it is highly related to object detection. For semantic segmentation, most modern frameworks are based on &lt;a href=&#34;https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf&#34; target=&#34;_blank&#34;&gt;Fully Convolutional Neural Networks (FCN)&lt;/a&gt;. FCN managed to extract the region of objects in an image but it cannot tell us the exact region of every single object. Here, Mask RCNN combined RCN and Faster RCNN, adding an extra FCN branch to faster RCNN, parallel to localization and classification, to make pixel level prediction and instance segmentation in a single bounding box.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/mask-rcnn.png&#34; width=&#34;600&#34;&gt;&lt;/p&gt;

&lt;h3 id=&#34;roi-align&#34;&gt;ROI Align&lt;/h3&gt;

&lt;p&gt;Different to object detection, pixel level prediction need more fine-grained alignment rather than simple ROI pooling. In ROI pooling, the location information is quantified twice:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Quantize region proposal coordinates as integer point coordinates&lt;/li&gt;
&lt;li&gt;The quantized region is evenly divided into $k \times k$ cells and the boundaries of each cell are quantized.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&#34;/img/ROIquantize.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;From the diagram above, the ground truth bounding box is $655 \times 655$. And after 5 max pooling operation, the corresponding size on feature map is &lt;sup&gt;655&lt;/sup&gt;&amp;frasl;&lt;sub&gt;32&lt;/sub&gt; = 20.78 , but ROI pooling quantizes it into $20$. After that, we need to divide the bounding box into $7 \times 7$ cells, the size of each cell is &lt;sup&gt;20&lt;/sup&gt;&amp;frasl;&lt;sub&gt;7&lt;/sub&gt; = 2.86 , but ROI pooling quantizes it into $2$. So, the largest quantization error of ROI pooling is about $1.5$ pixels on feature map, which is $50$ pixels on input image.&lt;/p&gt;

&lt;p&gt;What ROIalign does is removing the quantization in ROI pooling.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Do not quantize region proposal coordinates, keep it as float numbers.&lt;/li&gt;
&lt;li&gt;Do not quantize $k \times k$ cells boundaries when dividing, keep it as float numbers.&lt;/li&gt;
&lt;li&gt;Calculate the value of four fixed points in each cell using bilinear interpolation method, and then perform max pooling for these four points.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Here is a figure for ROI align:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/ROIAlignDiagram.png&#34; width=&#34;600&#34;&gt;&lt;/p&gt;

&lt;p&gt;And according to the paper, four fixed points in a cell works best, but only use one fixed point in a cell merely hurt the performance. Bilinear interpolation is totally differentiable, so back propogation is also straight forward.&lt;/p&gt;

&lt;p&gt;The training process is quite similar with faster RCNN, except for an extra predicting head for segmentation.&lt;/p&gt;

&lt;h3 id=&#34;key-point-detection&#34;&gt;Key Point Detection&lt;/h3&gt;

&lt;p&gt;In addition, mask RCNN can handle human pose key point detection. We just need to replace the segmentation mask with one-hot key point mask. In mask RCNN, human pose has 17 key points. The result of Mask RCNN is pretty good:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/maskrcnnresult.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;single-shot-detector&#34;&gt;Single Shot Detector&lt;/h1&gt;

&lt;p&gt;The previous methods of object detection all share one thing in common: they have one part of their network dedicated to providing region proposals followed by a high quality classifier to classify these proposals. These methods are very accurate but come at a big computational cost. In embedded systems, we have to control the computation cost. These method may not be suitable.&lt;/p&gt;

&lt;p&gt;Another way of doing object detection is by combining these two tasks into one network. Similar to region proposal network, we use different pre-defined boxes with various shape to capture objects, and predict class scores and bounding box offsets.&lt;/p&gt;

&lt;h2 id=&#34;single-shot-multibox-detector-ssd&#34;&gt;Single Shot MultiBox Detector(SSD)&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1512.02325&#34; target=&#34;_blank&#34;&gt;Single Shot MultiBox Detector(SSD)&lt;/a&gt; (by C. Szegedy et al.) was released at the end of 2016 and reaches the state-of-the-art performance of object recognition task in real time (58 FPS on a Nvidia Titan X).&lt;/p&gt;

&lt;p&gt;The name of SSD describes itself very well. Single Shot mean this method only need to run CNN forward pass once; Multibox means this method uses multiple boxes (different shape, on multiple CNN feature map scales) to detect object; detector means emm&amp;hellip;. Let&amp;rsquo;s dive into the details of SSD:&lt;/p&gt;

&lt;h3 id=&#34;architecture&#34;&gt;Architecture&lt;/h3&gt;

&lt;p&gt;Here is a diagram of SSD architecture:(image &lt;a href=&#34;https://www.slideshare.net/xavigiro/ssd-single-shot-multibox-detector&#34; target=&#34;_blank&#34;&gt;source&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/SSDarchitecture.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The main property of SSD is that it uses feature map from different CNN layers with different scale. SSD is built on VGG net and has six feature maps in total, each responsible for a different scale of objects, allowing it to identify objects across a large range of scales.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/ssdwork.png&#34; width=&#34;600&#34;&gt;&lt;/p&gt;

&lt;p&gt;The figure above shows how SSD works. In a convolutional fashion, we evaluate a small set
of default boxes (like &lt;a href=&#34;#region-proposal-network&#34;&gt;RPN anchor boxes&lt;/a&gt; of different aspect ratios at each location in several feature maps with
different scales (e.g. 8 $\times$ 8 and 4 $\times$ 4 in b and c). For each default box, we predict
both the shape offsets and the confidences for all object categories $((c_1, c_2, · · · , c_p))$.
At training time, we first match these default boxes to the ground truth boxes. For
example, we have matched two default boxes with the cat and one with the dog, which
are treated as positives and the rest as negatives. Any default box with an IOU of 0.5 or greater with a ground truth box is considered a match.&lt;/p&gt;

&lt;p&gt;For each default boxes of each cell, SSD predict two things:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Confident scores for all object categories $((c_1, c_2, · · · , c_p))$ (the same to RCNN, an extra background class indicating no object in this box)&lt;/li&gt;
&lt;li&gt;An offset vector with 4 entries $(C_x, C_y, C_w, C_h)$ to refine the matched default boxes to ground truth boxes.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;And the model loss is a weighted sum
between localization loss (e.g. Smooth L1) and confidence loss (e.g. Softmax). In the figure above, only three default get matched. All other default boxes need to predict a background class.&lt;/p&gt;

&lt;p&gt;SSD is quite similar to a [RPN]() with multi scale boxes. The default boxes are called prior here. Note that the prior can be customed according to our specific task. For example, if you want to detect pedestrian, you may need default boxes with large height and small width.&lt;/p&gt;

&lt;h3 id=&#34;implementation-details&#34;&gt;Implementation Details&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;Start from pretrained ImageNet model (VGG), get multiple-scale convolutional feature map.&lt;/li&gt;
&lt;li&gt;Hard negative Mining: during training, as most of the bounding boxes will have low IoU, interpreted as negative training examples. We may have disproportionate amount of negative examples. The paper suggests keeping a ratio of negative to positive examples of around 3:1.&lt;/li&gt;
&lt;li&gt;Data Augmentation: Sample patches with 0.1, 0.3, 0.5, 0.7, or 0.9 overlap to the ground truth; Random sample patches; 0.5 probability horizonal flip.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;you-only-look-once-yolo&#34;&gt;You Only Look Once(YOLO)&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://pjreddie.com/darknet/yolo/&#34; target=&#34;_blank&#34;&gt;You only look once (YOLO)&lt;/a&gt; is a state-of-the-art, real-time object detection system. On a Pascal Titan X it processes images at 30 FPS and has a mAP of 57.9% on COCO test-dev. More importantly, YOLO keeps evolving in recent years.&lt;/p&gt;

&lt;h3 id=&#34;yolov1&#34;&gt;YOLOv1&lt;/h3&gt;

&lt;p&gt;YOLOv1 is a little bit less precise than R-CNN families, but it is the first work to realize real time detection with CNN. Similar to other single shot detectors, the intuition of YOLOv1 is quite straight forward.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/YOLOv1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;YOLOv1 models detection as a regression problem. It divides the image into an S × S grid and for each grid cell predicts B bounding boxes, confidence for those boxes, and C class probabilities. These predictions are encoded as an S × S × (B × 5 + C) tensor. For example, on PASCAL VOC, S = 7,
B = 2. PASCAL VOC has 20 labelled classes so C = 20. Our final prediction is a 7 × 7 × 30 tensor.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/YOLOv1arch.png&#34; width=&#34;800&#34;&gt;&lt;/p&gt;

&lt;p&gt;The figure above is the architecture of YOLOv1. Based on pretrain the convolutional layers on the ImageNet classification
task at half the resolution (224 × 224 input image) and then double the resolution for detection.&lt;/p&gt;

&lt;p&gt;Here is the multi-part loss function that we want to optimize:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/YOLOv1lossfunction.png&#34; width=&#34;600&#34;&gt;&lt;/p&gt;

&lt;p&gt;where:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;B: Number of bounding boxes (2)&lt;/li&gt;
&lt;li&gt;$x_i,y_i,w_i,h_i$:  Bounding box parameters&lt;/li&gt;
&lt;li&gt;$C_i$: Some particular class i&lt;/li&gt;
&lt;li&gt;S: Grid size (7)&lt;/li&gt;
&lt;li&gt;$\bf{1}^{obj}_i$ : If object appear on the cell i, if does not appear it will be zero&lt;/li&gt;
&lt;li&gt;$\bf{1}^{obj}_{ij}$ : Bounding box j, from cell i responsible for prediction (i.e. has the highest
IOU of any predictor in that grid cell)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;And the weight in loss function $\lambda_{coord} = 5$ and $\lambda_{noobj} = 0.5$. Different to bounding box regression, YOLO uses square root instead of logrithm, but they should act in the same way. Note that the loss function &lt;strong&gt;only&lt;/strong&gt; penalizes classification
error if an object is present in that grid cell (hence the conditional
class probability discussed earlier). It also &lt;strong&gt;only&lt;/strong&gt; penalizes
bounding box coordinate error if that predictor is
“responsible” for the ground truth box (i.e. has the highest
IOU of any predictor in that grid cell). So, the first step of every training phase is &lt;strong&gt;bounding box matching&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;YOLOv1 also have a lot of limitations:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Each grid cell only predicts two boxes and can only have one class. This prevent YOLO from detecting nearby objects.&lt;/li&gt;
&lt;li&gt;Compared to SSD, YOLO only uses coarse features. Cannot detect relative small objects.&lt;/li&gt;
&lt;li&gt;YOLO has higher localization errors and the recall is lower compared to SSD.&lt;/li&gt;
&lt;li&gt;The final FC layers are slow and kind of redundant.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;yolov2&#34;&gt;YOLOv2&lt;/h3&gt;

&lt;p&gt;Inspired by faster RCNN and SSD, &lt;a href=&#34;https://arxiv.org/pdf/1612.08242.pdf&#34; target=&#34;_blank&#34;&gt;YOLOv2&lt;/a&gt; improves the accuracy significantly while making it faster.&lt;/p&gt;

&lt;p&gt;First, YOLOv2 is &lt;strong&gt;Better&lt;/strong&gt;:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Batch Normalization&lt;/strong&gt;. Batch normalization leads to significant
improvements in convergence while eliminating the
need for other forms of regularization. Improve mAP by 2%. See my another blog for &lt;a href=&#34;https://shen338.github.io/post/going-deeper-into-batch-normalization/&#34; target=&#34;_blank&#34;&gt;batch norm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;High Resolution Classifier&lt;/strong&gt;: we first fine tune the classification network
at the full 448 × 448 resolution for 10 epochs on ImageNet different to original use 224 × 224 network directly for 448 × 448 detection. Improve mAP by 4%.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Convolutional With Anchor Boxes&lt;/strong&gt;: We remove the fully connected layers from YOLO and
use anchor boxes like &lt;a href=&#34;#faster-rcnn&#34;&gt;faster RCNN&lt;/a&gt; to predict bounding boxes. No improvement on mAP but 7% increase on recall rate, which makes the model has more room to improve.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Dimension Clusters&lt;/strong&gt;: The author run K-means algorithm to determine the anchor box sizes. Shown in the following figure, the authors choose
k = 5 as a good tradeoff between model complexity and
high recall.
&lt;img src=&#34;/img/clusterbox.png&#34; width=&#34;600&#34;&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Direct location prediction&lt;/strong&gt;: Instead of predicting offsets we follow the approach of
YOLO and predict location coordinates relative to the location
of the grid cell. (Predict coordinates directly and predict scale using archor boxes). Improve YOLO by 5%. The network predicts 5 coordinates
for each bounding box, $t_x, t_y, t_w, t_h$, and $t_o.$ If the cell is
offset from the top left corner of the image by $(c_x, c_y)$ and
the bounding box prior has width and height $p_w, p_h$, then
the predictions correspond to:
&lt;img src=&#34;/img/YOLOv2direct.png&#34; width=&#34;600&#34;&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Fine-Grained Features&lt;/strong&gt;: Similar to faster RCNN and SSD, adding a passthrough layer that brings
features from an earlier layer at 26 × 26 resolution along with original 13 × 13 resolution. Improve mAP about 1%.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Multi-Scale Training&lt;/strong&gt;: During training, YOLO takes images of size {320, 352, &amp;hellip;, 608} (with a step of 32 = YOLO downsampling rate). For every 10 batches, YOLOv2 randomly selects another image size to train the model. This acts as data augmentation and forces the network to predict well for different input image dimension and scale.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Second, YOLOv2 is &lt;strong&gt;Faster&lt;/strong&gt;:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;GoogleNet. Reduce billions of floating points operations but YOLO’s
custom model gets 88.0% ImageNet compared to 90.0% for
VGG-16.&lt;/li&gt;
&lt;li&gt;DarkNet 19: Compare to VGG 19, double the number of channels after every
pooling step. Replace redundant FC layer with global average pooling; use 1 × 1 filters to compress the feature representation
between 3 × 3 convolutions (bottleneck). Requires 5.58 billion operations and 91.2% top-5 accuracy on ImageNet.&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;yolov3&#34;&gt;YOLOv3&lt;/h3&gt;

&lt;h1 id=&#34;other-backbone-network&#34;&gt;Other backbone network&lt;/h1&gt;

&lt;h2 id=&#34;fpn&#34;&gt;FPN&lt;/h2&gt;

&lt;h2 id=&#34;focal-loss&#34;&gt;Focal Loss&lt;/h2&gt;

&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;

&lt;h2 id=&#34;reference-materials&#34;&gt;Reference Materials&lt;/h2&gt;
</description>
    </item>
    
    <item>
      <title> Fine-Grained Visual Categorization on iNaturalist dataset</title>
      <link>/project/fine-grained-visual-categorization-inaturalist/</link>
      <pubDate>Mon, 27 Nov 2017 00:00:00 -0500</pubDate>
      
      <guid>/project/fine-grained-visual-categorization-inaturalist/</guid>
      <description>

&lt;h2 id=&#34;table-of-content&#34;&gt;Table of Content&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#overview&#34;&gt;Overview&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#main-difficulties&#34;&gt;Main Difficulties&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#data-imbalance&#34;&gt;Data Imbalance&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#weakly-supervised-label&#34;&gt;Weakly supervised label&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#data-insufficiency&#34;&gt;Data insufficiency&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#fine-tuned-model-on-imagenet-pretrained-model&#34;&gt;Fine-tuned model on ImageNet pretrained model.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#fine-grained-classification&#34;&gt;Fine-grained Classification&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#super-category-classification&#34;&gt;Super-category classification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#fine-grained-classification-in-single-super-category&#34;&gt;Fine-grained classification in single super-category&lt;/a&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#bilinear-model&#34;&gt;Bilinear model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#attention-model-class-activation-map-&#34;&gt;Attention model(Class Activation Map)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#reference-materials&#34;&gt;Reference materials&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;

&lt;p&gt;I run into this challenge at early April. This dataset contains about 0.4 million images spanning over 8142 categories. Without expert knowledge, many of these species are extremely difficult to accurately classify due to their visual similarity. This classification task require us to features a large number of fine-grained
categories over class imbalance.&lt;/p&gt;

&lt;h2 id=&#34;main-difficulties&#34;&gt;Main Difficulties&lt;/h2&gt;

&lt;h3 id=&#34;data-imbalance&#34;&gt;Data Imbalance&lt;/h3&gt;

&lt;p&gt;The dataset is quite imbalance, some classes have more than 1000 images, while other classes only have less than 10 images. Here is a table of class image
number distribution.&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;image number range&lt;/th&gt;
&lt;th&gt;class number&lt;/th&gt;
&lt;th&gt;percentage&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;0&amp;lt;n&amp;lt;=10&lt;/td&gt;
&lt;td&gt;472&lt;/td&gt;
&lt;td&gt;5.78%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;10&amp;lt;n&amp;lt;=25&lt;/td&gt;
&lt;td&gt;4675&lt;/td&gt;
&lt;td&gt;57.4%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;25&amp;lt;n&amp;lt;=50&lt;/td&gt;
&lt;td&gt;1599&lt;/td&gt;
&lt;td&gt;19.6%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;50&amp;lt;n&amp;lt;=100&lt;/td&gt;
&lt;td&gt;554&lt;/td&gt;
&lt;td&gt;6.80%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;100&amp;lt;n&amp;lt;=200&lt;/td&gt;
&lt;td&gt;399&lt;/td&gt;
&lt;td&gt;4.90%&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;200&amp;lt;n&amp;lt;=1000&lt;/td&gt;
&lt;td&gt;443&lt;/td&gt;
&lt;td&gt;5.44%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Over one half classes have image number in range $[10, 25]$, while the other classes are ranged in $[0, 1000]$. Some method or tricks to compensate classs imbalance is necessary here. Otherwise our model would easily get crashed. The majority class image number range is $[10, 25]$, while some other classes can easily get 1000, which can totally destory the training for the majority class.&lt;/p&gt;

&lt;p&gt;First, we just abandon the excessive images in class with more than 200 images. Second, oversample the minority class and put more aggressive image augmentation
to these classes. For example, affine transformation like random rotate, crop, shear, flap, blurring like gaussian blur, random dropout and so on. Image augmentation is implemented with Augmentor. Finally, we utilize weighted cross entropy loss function, further compensate the remaining
imbalance effect in our dataset. Here is the code for Multiprocessing Augmentor Pipeline:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
# Multiprocessing Augmentor Pipeline
def pipeline():

    P1 = Augmentor.Pipeline()
    P1.flip_left_right(probability=0.3)
    P1.flip_top_bottom(probability=0.05)
    P1.rotate90(probability=0.1)
    P1.rotate_without_crop(probability=0.1, max_left_rotation=15, max_right_rotation=15)
    P1.random_distortion(probability=0.1, grid_width=10, grid_height=10, magnitude=1)
    P1.skew_corner(probability=0.1)
    P1.shear(probability=0.1, max_shear_left=10, max_shear_right=10)
    P1.crop_centre(probability=1.0, percentage_area=0.9)
    P1.crop_random(probability=1.0, percentage_area=0.9)

    return P1


def aug_func(imgs, label, q):

    P = pipeline()
    imgs = np.uint8(imgs)
    g = P.keras_generator_from_array(imgs, label, batch_size=imgs.shape[0]
                                     , image_data_format=&#39;WTF&#39;)
    i, l = next(g)
    q.put([i, l])


def aug_parallel(imgs, labels):

    queue = Queue()
    cpus = multiprocessing.cpu_count()
    step = int(imgs.shape[0] / cpus)
    processes = []
    for ii in range(cpus):

        p = Process(target=aug_func,
                    args=(imgs[ii * step:(ii + 1) * step, :, :, :],
                          labels[ii * step:(ii + 1) * step],
                          queue))
        processes.append(p)
        p.start()

    rets = []
    labels = []

    for p in processes:
        ret, l = queue.get() 
        rets.append(ret)
        labels.append(l)
    for p in processes:
        p.join()

    if step == 1:
        rets = np.squeeze(np.array(rets))
        labels = np.squeeze(np.array(labels), axis=1)
    else:
        rets = np.concatenate(rets, axis=0)
        labels = np.concatenate(labels, axis=0)

    return rets, labels


&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;weakly-supervised-label&#34;&gt;Weakly supervised label&lt;/h3&gt;

&lt;p&gt;After looking into some image samples in the dataset, we find the the image labels are just weakly supervised labels. In other words, usually only
a small area in the image contains the target object. For example, one image is labeled as sunflowers, but the sunflower only appear at one corner,
and the background is grass and woods.&lt;/p&gt;

&lt;p&gt;This kind of labeling will apparently increase the difficulty of precise image classification because we have to do something to localize the object in the
image and prevent the background from affecting the classification result. That&amp;rsquo;s the main reason we incorporate attention model afterwards.&lt;/p&gt;

&lt;p&gt;Here are some cases of weakly supervised labels:&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Image&lt;/th&gt;
&lt;th&gt;Image&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;img src=&#34;/img/Weak1.jpg&#34; alt=&#34;&#34; /&gt;&lt;/td&gt;
&lt;td&gt;&lt;img src=&#34;/img/weak2.png&#34; alt=&#34;&#34; /&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&#34;data-insufficiency&#34;&gt;Data insufficiency&lt;/h3&gt;

&lt;p&gt;Although we have about 450,000 images in total, when averaged to every class, we only have 50 images left. And the majority class image number range is $[10, 25]$.
With so few images, it would be impossible to train a model from scratch. The most popular solution would be adopt a ImageNet pretrained model and fine-tuned on our dataset.&lt;/p&gt;

&lt;p&gt;Our first choice is the &lt;a href=&#34;https://arxiv.org/abs/1602.07261&#34; target=&#34;_blank&#34;&gt;Inception Resnet V2&lt;/a&gt; network from google. This network is well designed to save parameters and achieve
good performance at the same time. The same with previous Inception module, Inception resnet v2 also implements nx1 and 1xn convolution to replace nxn convolution, also bottleneck architecture to reduce computation. Besides, Inception Resnet V2 introduces residual connection from ResNet architecture to
eases the gradient vanishing problem when training deep neural nets. And the total layer number of the network goes to 467.&lt;/p&gt;

&lt;h2 id=&#34;fine-tuned-model-on-imagenet-pretrained-model&#34;&gt;Fine-tuned model on ImageNet pretrained model.&lt;/h2&gt;

&lt;p&gt;The first trail is directly classification over 8142 categories. Here is an architecture diagram of Inception Resnet V2:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/InceptionResnetv2.jpg&#34; alt=&#34;InceptionResnetv2.jpg&#34; /&gt;&lt;/p&gt;

&lt;p&gt;We first freeze the CNN feature extraction part, and train our own classifier - the final layers
of the network. Note that we have to training two classifier, one at the end of network, the auxiliary one at the middle. The middle one is used to provide additional
gradient to help the network to avoid gradient vanishing.
After 5 epoches of training, the accurarcy goes up to 40%.&lt;/p&gt;

&lt;p&gt;And after that, we use a relatively low learning rate to fine-tune the network to avoid too much modification on the original network. After about 15 epoches, the training accuracy goes up to about 80% and the validation accuracy is about 60%, the top3 accuracy on test set is about 80%. Apparently there are some kinds of overfit, but this is inevitable with so little data. We will further try to freeze the starting layers throughtout the traiing process to add some regulate the expression capacity of the network. Hope that works.&lt;/p&gt;

&lt;h2 id=&#34;fine-grained-classification&#34;&gt;Fine-grained Classification&lt;/h2&gt;

&lt;h3 id=&#34;super-category-classification&#34;&gt;Super-category classification&lt;/h3&gt;

&lt;p&gt;After finishing the previous model with InceptionResnetv2, we carefully examined our result. We found that our model often predicted three very similar classes, like three different sunflowers with very subtle difference. This means our model may have a hard time to classify images over similar classes. So, the next step is fine grained classification over these sub species. First, we divide our dataset into 11 super categories, like plants, insects, birds and so on. We consider this problem in a high dimensional space, and treat the image classes as clusters. The cluster distance of classes in the same super-category should be signaficantly smaller than cluster distance of classes in different super-categories. But the our image labelling cannot represent this property. After one-hot encoding, we treat every image class equivalently and assume the distance between them are the same.&lt;/p&gt;

&lt;p&gt;According to these analysis, we decided to classify images into their super-categories and do fine-grained classification in every single super-category. In this way, our model would concentrate more on the fine details of similar objects in the same super-category, without wasting resources on the easy cross super-category classification.&lt;/p&gt;

&lt;p&gt;Here is a diagram of image space:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/Figure_1.png&#34; alt=&#34;Diagram of image space&#34; /&gt;&lt;/p&gt;

&lt;p&gt;It would be more efficient to classify ABC and DE first. In this way, the class distance variance would be much less, we can use relatively less resource to achieve comparable performance to direct classification.&lt;/p&gt;

&lt;h3 id=&#34;fine-grained-classification-in-single-super-category&#34;&gt;Fine-grained classification in single super-category&lt;/h3&gt;

&lt;h4 id=&#34;bilinear-model&#34;&gt;Bilinear model&lt;/h4&gt;

&lt;p&gt;The first model is &lt;a href=&#34;http://vis-www.cs.umass.edu/bcnn/docs/bcnn_iccv15.pdf&#34; target=&#34;_blank&#34;&gt;bilinear model&lt;/a&gt;. This method considers the interaction between different feature map channels. If the feature map has the size $[H, W, C]$, treat every pixel in feature map as a feature vector, feature vectors has the size $[1,C]$. And the bilinear model takes the outer product of every feature vector and forms a $[H, W, C, C]$ feature map. After taking global pooling, we get a feature map [C, C]. In this process, bilinear model utilizes higher order information of the feature map. According to our test, its performance is pretty good. It achieves nearly 90% accuracy in single super-category classification. But when applying it to bigger super-category, our GPU memory is not enough. The reason is after bilinear operation, the feature map size becomes so big $(C^2)$, so the paremeters in following fully connected layers become overwhelming. For example, if the feature map has 512 channels, and our biggest super-category has more than 3000 classes, the parameters in FC layer is $512^2*3000 = 78 million$, which is more than all the CNN layers combined.&lt;/p&gt;

&lt;h4 id=&#34;attention-model-class-activation-map&#34;&gt;Attention model(Class Activation Map)&lt;/h4&gt;

&lt;p&gt;Due to the weakly supervised nature of the dataset (discussed &lt;a href=&#34;#weakly-supervised-label&#34;&gt;above&lt;/a&gt;), we want to first roughly localize the target object in the image before fine-grained classification.
Here attention model comes to the rescue. With attention model, we can determine which part of images matters for the network to make predictions, which should be the object we want to localize. Since we already have the network of super-category classification, its feature map can be utilized to generate class activation map &lt;a href=&#34;http://cnnlocalization.csail.mit.edu/Zhou_Learning_Deep_Features_CVPR_2016_paper.pdf&#34; target=&#34;_blank&#34;&gt;(CAM)&lt;/a&gt; to represent the attention level of input images.
Here is a diagram of CAM:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/CAM.png&#34; alt=&#34;CAM&#34; /&gt;&lt;/p&gt;

&lt;p&gt;As shown in the diagram, we can use the feature map and fc layer weight to generate the attention map for images with super-category network. Put an image into the network, we can get a feature map $[H, W, C]$ and a prediction. In order to map the global pooling layers
to predictions, the FC layer weight is $W \in R^{C \times N}$, where N is the output class number. If the prediction is $j$, CAM is calculated as follows:&lt;/p&gt;

&lt;p&gt;$$CAM = \sum_{k=0}^C F(k)*W_{k,j}$$&lt;/p&gt;

&lt;p&gt;where F(k) represent kth feature map channel. Alternatively, we can use top N prediction if top 1 prediction is not confident. Afterwards, we upsample CAM to fit input image size.&lt;/p&gt;

&lt;p&gt;After this, we used OSTU algorithm to make the CAM into binary image and extract the biggest contour and generate bounding box via OpenCV. Here are some results without any cherry picking:&lt;/p&gt;

&lt;p&gt;First column is original images and generated bounding box; second column is CAM heatmap; third column is the binary image after OSTU algorithm and the biggest contour.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/CAM_result.png&#34; alt=&#34;drawing&#34; style=&#34;width: 500px;&#34;/&gt;&lt;/p&gt;

&lt;p&gt;The result is pretty amazing. Regardless of the type of objects, like plants, birds, insects, we can accurately locate them.&lt;/p&gt;

&lt;h4 id=&#34;classification-after-localization&#34;&gt;Classification after Localization&lt;/h4&gt;

&lt;p&gt;Still in process.&lt;/p&gt;

&lt;h2 id=&#34;reference-materials&#34;&gt;Reference materials&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1602.07261&#34; target=&#34;_blank&#34;&gt;Inception ResNet V2 Paper&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://vis-www.cs.umass.edu/bcnn/docs/bcnn_iccv15.pdf&#34; target=&#34;_blank&#34;&gt;Bilinear Model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://cnnlocalization.csail.mit.edu/Zhou_Learning_Deep_Features_CVPR_2016_paper.pdf&#34; target=&#34;_blank&#34;&gt;Class Activation Map Paper&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Obfuscated/Blurred Human Face Reconstruction</title>
      <link>/project/obfuscatedblurred-human-face-reconstruction/</link>
      <pubDate>Mon, 27 Nov 2017 00:00:00 -0500</pubDate>
      
      <guid>/project/obfuscatedblurred-human-face-reconstruction/</guid>
      <description>

&lt;h1 id=&#34;table-of-contents&#34;&gt;Table of Contents&lt;/h1&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;#data-preparation&#34;&gt;Data preparation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#super-resolution-resnet&#34;&gt;Super Resolution ResNet&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#generative-adversarial-network-model&#34;&gt;Generative Adversarial Network Model&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Nowadays, a lot of images on the Internet are intentionally blurred or mosaiced due to various reasons. The main objective of this project is to
reconstruct these images, especially heavily blurred ones, to their original high-resolution counterpart. This problem is an ill-posed problems, we need to
predict fine-grained details only based on little information on degenerated images. Exploring and enforcing strong prior information
about the high-resolution image are necessary to guarantee the stability of
this reconstruction process. Many traditional example-based methods have been devoted to resolving this problem via probabilistic
graphical model, neighbor embedding, sparse coding, linear or nonlinear regression, and random forest.&lt;br /&gt;
Our approach is utilizing deep networks for image reconstruction to learn the mapping between low and high-resolution image pairs, automatically take the prior
information into account. Check out the code &lt;a href=&#34;https://github.com/shen338/Obfuscated-Face-Reconstruction&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Our experiment consists of three steps.&lt;/p&gt;

&lt;h2 id=&#34;data-preparation&#34;&gt;Data preparation&lt;/h2&gt;

&lt;p&gt;In 2017, the most popular dataset about human face is the &lt;a href=&#34;http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html&#34; target=&#34;_blank&#34;&gt;CelebA dataset&lt;/a&gt; from CUHK, which has
 10,177 number of identities, 202,599 number of face images.&lt;br /&gt;
 What we do first is detecting and cropping human face from the image and resize it into 128x128 in convenience. We use OpenCV&amp;rsquo;s module named &amp;ldquo;haarcascade_frontalface&amp;rdquo;
 to detect faces and use bicubic interpolation to resize images. The code are as below:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;facedata = &amp;quot;haarcascade_frontalface_default.xml&amp;quot;
cascade = cv2.CascadeClassifier(facedata)
img = cv2.imread(image_directory)
minisize = (img.shape[1], img.shape[0])
miniframe = cv2.resize(img, minisize)

faces = cascade.detectMultiScale(miniframe)
if(len(faces) == 0): continue
x, y, w, h = [v for v in faces[0]]  # only need the first detected face image

img_raw = img[y:y + h, x:x + w]
img_raw = cv2.resize(img_raw, (128, 128), interpolation=cv2.INTER_LANCZOS4)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then we manually downsample these high-resolution images using gaussian and average pooling. After write both low and high-resolution image pairs into binary files
using TFRecord for multi-threading read in the future. Code:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import tensorflow as tf
import numpy as np
import cv2
import glob
import random
import sys


def load_image(addr):
    # read an image and resize to (224, 224)
    # cv2 load images as BGR, convert it to RGB
    img = cv2.imread(addr)
    #img = cv2.resize(img, (128, 128), interpolation=cv2.INTER_CUBIC)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    img = img.astype(np.uint8)
    return img

def _int64_feature(value):
  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))

def _bytes_feature(value):
  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))


shuffle_data = True
image_path_small = &#39;./origin/small/*.png&#39;
address_small = glob.glob(image_path_small)
print(len(address_small))
image_path_origin = &#39;./origin/origin/*.png&#39;
address_origin = glob.glob(image_path_origin)

if shuffle_data:
    c = list(zip(address_small, address_origin))
    random.shuffle(c)
    address_small,  address_origin= zip(*c)

train_filename = &#39;train_espcn.tfrecords&#39;

# create new TFrecord file
writer = tf.python_io.TFRecordWriter(train_filename)

for i in range(len(address_small)):

    if not i % 1000:
        print(&#39;Train data: {}/{}&#39;.format(i, len(address_small)))
        sys.stdout.flush()

    img_small = load_image(address_small[i])
    img_origin = load_image(address_origin[i])

    feature = {&#39;train/image_small&#39;: _bytes_feature(tf.compat.as_bytes(img_small.tostring())),
               &#39;train/image_origin&#39;: _bytes_feature(tf.compat.as_bytes(img_origin.tostring()))}

    # Create an example protocol buffer
    example = tf.train.Example(features=tf.train.Features(feature=feature))

    # Serialize to string and write on the file
    writer.write(example.SerializeToString())

writer.close()
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;super-resolution-resnet&#34;&gt;Super Resolution ResNet&lt;/h2&gt;

&lt;p&gt;My first trail was using convolution neural network to do end-to-end super-resolution. The intuition is quite simple,
just feed the low resolution images to the network, do downsampling and upsampling, in the end, use
high resolution images as ground truth to train this network. In this way, the neural network will learn how to map from
low-resolution image directly to high-resolution images. One thing worth to mention, In that time, I didn&amp;rsquo;t have a good GPU,
so I have to so every I can to save computational resources.
We replaced the upsampling layers with Pixel Shuffle layers, which directly map the feature map into
high resolution output instead of doing transposed convolution to upsample the feature map.&lt;/p&gt;

&lt;p&gt;Next, we use skip connection from &lt;a href=&#34;https://arxiv.org/abs/1512.03385&#34; target=&#34;_blank&#34;&gt;ResNet paper&lt;/a&gt; to enhance our model&amp;rsquo;s  expression capacity. We put 15 residual
block module in our network before Pixel Shuffle layers. The enhancing is quite obvious in the result comparasion.
We also use pretrained VGG net to build the preceptual loss. Instead of using
vanilla MSE loss to compare the network output and ground truth, we feed the network output and ground truth
into ImageNet pretrained VGG net and compare their feature map difference using MSE. Using direct MSE loss function also works here, but it may
not agree with human observers, because human visual system is not sentitive to color and edgee. While perceptual loss simulates human visual system, making the
loss value more related to human percetion. The architecture of our network is shown as follows:
&lt;img src=&#34;https://raw.githubusercontent.com/shen338/Obfuscated-Face-Reconstruction/master/SRResNet_model.PNG&#34; alt=&#34;SRResnet&#34; /&gt;&lt;/p&gt;

&lt;p&gt;This model works well for 4 times mosaiced/blurred images, even though there is still some smoothing effect in human face detailes. I think our model even makes
these celebrities more beautiful/handsome 😄. After smoothing out their wrinkles and little flaws, they all look younger than before! The result is shown as follows:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/shen338/Obfuscated-Face-Reconstruction/master/result/SRResNet_result.PNG&#34; alt=&#34;SRResnet_result&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;generative-adversarial-network-model&#34;&gt;Generative Adversarial Network Model&lt;/h2&gt;

&lt;p&gt;Although our ResNet model works well in 4 times mosaiced/blurred images, it terribly failed in more than 8 times mosaiced/blurred images. Maybe there is not enough
prior information for our network to reconstruct high-resolution images. To deal with this problem, we incorporate GAN model into our project, and expect GANs can
generate the fine details to the reconstruction process and improve our result. We brought this idea from &lt;a href=&#34;https://arxiv.org/abs/1609.04802&#34; target=&#34;_blank&#34;&gt;SRGAN paper&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Different from original GAN model to generate unique images based on image dataset, instead of feeding random noise to generator, we feed low-resolution
image to the generator and expect it to produce high-resolution images. And the discriminator is still responsible to determine whether the image is from
generator or from dataset. And the training loss also consist of two parts, the super-resolution(SR) loss, and the GAN loss. The SR loss is the perceptual loss (mentioned above)
between ground truth high-resolution image and the generator output, while GAN loss is the loss function for the generator and discriminator combined.&lt;/p&gt;

&lt;p&gt;First, we tried vanilla deep convolution GAN(DCGAN) on our GAN model. After a few days of hyperparamete tunning, we find this model is not that stable.
The whole model can easily crash due to a small shift of a single hyperparameter. But there are better alternatives for DCGAN: the &lt;a href=&#34;https://arxiv.org/abs/1701.07875&#34; target=&#34;_blank&#34;&gt;Wasserstein GAN&lt;/a&gt;.
Although still have imperfections, WGAN almost totally solve issues like training instability, failure to converge or model collapse.&lt;/p&gt;

&lt;p&gt;Here is the algorithm of WGAN:
&lt;img src=&#34;/img/WGAN.jpg&#34; alt=&#34;WGAN&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Just minor modification from original GAN algorithm, how amazing is that! In Wasserstein GAN paper, the author thoroughly analyzed the weakness and holes on
original GANs. For details of the mathematics and implementation about WGAN, see my &lt;a href=&#34;https://shen338.github.io/post/amazing-gan---wasserstein-gan/&#34; target=&#34;_blank&#34;&gt;another blog&lt;/a&gt;. Here I will only list the modification from original GAN to WGAN:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Remove the sigmoid function at the end of Discriminator&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Remove the log function in generator and discriminator loss function&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;clip the gradient norm into an interval $[-c, c]$&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Use optimizers without momentum term, like RMSprop, not Adam.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The core code are as follows:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def train():

    image_lr = tf.placeholder(dtype=tf.float32, shape=(None, 16, 16, 3), name=&#39;lr&#39;)
    image_hr = tf.placeholder(dtype=tf.float32, shape=(None, 128, 128, 3), name=&#39;hr&#39;)

    net = WGAN(gamma)

    gen = net.generator(image_lr, bottleneck_num=2)

    real_score = net.discrimintor(gen)
    fake_score = net.discrimintor(image_hr, reuse=True)

    with tf.name_scope(&#39;SR_loss&#39;):

        residual = image_hr - gen
        square = tf.abs(residual)
        SR_loss = tf.reduce_mean(square)

        tf.summary.scalar(&#39;SR_loss&#39;, SR_loss)

    with tf.name_scope(&#39;gan_loss&#39;):

        D_loss = tf.reduce_mean(fake_score) - tf.reduce_mean(real_score)

        G_loss = -tf.reduce_mean(fake_score)

        tf.summary.scalar(&#39;G_loss&#39;, G_loss)
        tf.summary.scalar(&#39;D_loss&#39;, D_loss)

        G_overall_loss = gan_ratio*G_loss + SR_loss 

    # get variable from G and D
    var_g = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, &#39;generator&#39;)
    var_d = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, &#39;discriminator&#39;)

    with tf.name_scope(&#39;optim&#39;):

        optim_g = tf.train.RMSPropOptimizer(learning_rate=LEARNING_RATE)\
            .minimize(G_overall_loss, var_list=var_g)
        optim_d = tf.train.RMSPropOptimizer(learning_rate=LEARNING_RATE) \
            .minimize(-D_loss, var_list=var_d)

    clip_D = [p.assign(tf.clip_by_value(p, -0.01, 0.01)) for p in var_d]

    # set up logging for tensorboard
    writer = tf.summary.FileWriter(filewriter_path)
    writer.add_graph(tf.get_default_graph())
    summaries = tf.summary.merge_all()
	
	config = tf.ConfigProto()
    config.gpu_options.allow_growth = True
    with tf.Session() as sess:

        init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())
        sess.run(init_op)

        steps, start_average, end_average = 0, 0, 0
        start_time = time.clock()

        coord = tf.train.Coordinator()
        threads = tf.train.start_queue_runners(coord=coord)

        for ii in range(NUM_EPOCHS):

            batch_average = 0
            batch_num = int(np.floor(192794 / BATCH_SIZE / 6.0))

            for jj in range(batch_num):

                g_ops = [optim_g, G_loss, summaries]
                d_ops = [optim_d, D_loss]

                for kk in range(critic):

                    steps += 1
                    img_lr, img_hr = load_batch_date()
                    img_lr = (img_lr.astype(np.float32) - 127.5) / 127.5
                    img_hr = (img_hr.astype(np.float32) - 127.5) / 127.5

                    _, loss_d = sess.run(d_ops, feed_dict=
                                  {image_lr: img_lr, image_hr: img_hr})

                    sess.run(clip_D)

                steps += 1
                img_lr, img_hr = sess.run([images, labels])
                img_lr = (img_lr.astype(np.float32) - 127.5) / 127.5
                img_hr = (img_hr.astype(np.float32) - 127.5) / 127.5

                _, loss_g, summary = sess.run(g_ops,
                                feed_dict={image_lr: img_lr, image_hr: img_hr})

                # update W_loss and Kt

                writer.add_summary(summary, steps)
                batch_average += loss_g

                if (steps % 100 == 0):
                    print(&#39;step: {:d}, G_loss: {:.9f}, D_loss: {:.9f}&#39;.format(steps, loss_g, loss_d))
                    print(&#39;time:&#39;, time.clock())

            batch_average = float(batch_average) / batch_num

            duration = time.time() - start_time
            print(&#39;Epoch: {}, step: {:d}, loss: {:.9f}, &#39;
                  &#39;({:.3f} sec/epoch)&#39;.format(ii, steps, batch_average, duration))

            start_time = time.time()
            net.save(sess, saver, checkpoint_path, steps)
        coord.request_stop()

        # Wait for threads to stop
        coord.join(threads)
        sess.close()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Also, we implement WGAN-GP method, instead of directly clipping gradient, it put penalty on gradient&amp;rsquo;s L2 norm. This method is even better
compared to original Wasserstein GAN.&lt;/p&gt;

&lt;p&gt;And the result is pretty good compared to SRResNet model:
&lt;img src=&#34;https://raw.githubusercontent.com/shen338/Obfuscated-Face-Reconstruction/master/result/SRGAN_result.PNG&#34; alt=&#34;SRGAN_result&#34; /&gt;&lt;br /&gt;
The smoothing effect is quite significant in this case. But the SRGAN result is totally acceptable considering our model is reconstruct the
high-resolution image only using &lt;sup&gt;1&lt;/sup&gt;&amp;frasl;&lt;sub&gt;64&lt;/sub&gt; of original information. I believe our result should be better with a faster GPU and deep network. I finished this
project on a Nivida Quadro K1000 GPU, which takes 6 hours to run one epoch.&lt;/p&gt;

&lt;p&gt;There are always some failure case using GAN model:
&lt;img src=&#34;https://raw.githubusercontent.com/shen338/Obfuscated-Face-Reconstruction/master/result/failure_case.PNG&#34; alt=&#34;failure case&#34; /&gt;
Some of them are funny and some of them are just scary&amp;hellip;&lt;/p&gt;

&lt;p&gt;Reference Materials:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html&#34; target=&#34;_blank&#34;&gt;CelebA Dataset from CUHK&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1501.00092.pdf&#34; target=&#34;_blank&#34;&gt;Image Super-Resolution Using Deep Convolutional Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1701.07875&#34; target=&#34;_blank&#34;&gt;Wasserstein GAN&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://lilianweng.github.io/lil-log/2017/08/20/from-GAN-to-WGAN.html&#34; target=&#34;_blank&#34;&gt;A great blog on GAN and WGAN&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Spatial Transform Network (STN)</title>
      <link>/post/spatial-transform-network/</link>
      <pubDate>Sun, 19 Nov 2017 21:00:00 -0500</pubDate>
      
      <guid>/post/spatial-transform-network/</guid>
      <description>

&lt;h2 id=&#34;spatial-transformer&#34;&gt;Spatial Transformer&lt;/h2&gt;

&lt;p&gt;Before diving into STNs, we need to concrete our knowledge about image transformer.&lt;/p&gt;

&lt;h3 id=&#34;image-transformation&#34;&gt;Image Transformation&lt;/h3&gt;

&lt;h3 id=&#34;bilinear-interpolation&#34;&gt;Bilinear Interpolation&lt;/h3&gt;

&lt;h3 id=&#34;tensorflow-implement&#34;&gt;Tensorflow Implement&lt;/h3&gt;

&lt;h3 id=&#34;result&#34;&gt;Result&lt;/h3&gt;

&lt;h2 id=&#34;spatial-transform-network&#34;&gt;Spatial Transform Network&lt;/h2&gt;

&lt;h3 id=&#34;motivation&#34;&gt;Motivation&lt;/h3&gt;

&lt;h3 id=&#34;fun-work-with-stns&#34;&gt;Fun work with STNs&lt;/h3&gt;
</description>
    </item>
    
    <item>
      <title>Subwavelength focusing by binary multi-annular plates: design theory and experiment</title>
      <link>/publication/map/</link>
      <pubDate>Sun, 15 Feb 2015 00:00:00 -0500</pubDate>
      
      <guid>/publication/map/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
