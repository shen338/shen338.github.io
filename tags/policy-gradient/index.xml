<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Policy gradient on Tong Shen</title>
    <link>/tags/policy-gradient/</link>
    <description>Recent content in Policy gradient on Tong Shen</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Sat, 01 Dec 2018 21:00:00 -0600</lastBuildDate>
    
	<atom:link href="/tags/policy-gradient/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Reinforcement Learning: Markov Decision Process and Model-free Algorithms</title>
      <link>/post/model-free-reinforcement-learning/</link>
      <pubDate>Sat, 01 Dec 2018 21:00:00 -0600</pubDate>
      
      <guid>/post/model-free-reinforcement-learning/</guid>
      <description>What is reinforcement learning? In supervised learning, we saw algorithms that tried to make their outputs mimic the labels y given in the training set. In that setting, the labels gave an unambiguous “right answer” for each of the inputs x. In contrast, for many sequential decision making and control problems, it is very difficult to provide this type of explicit supervision to a learning algorithm. For example, if we have just built a four-legged robot and are trying to program it to walk, then initially we have no idea what the “correct” actions to take are to make it walk, and so do not know how to provide explicit supervision for a learning algorithm to try to mimic.</description>
    </item>
    
  </channel>
</rss>